\documentclass[11pt, compress, t, notes = noshow, xcolor = table, 
aspectratio = 1610]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}

\usepackage{framed}

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{amsmath, nccmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{caption}
\captionsetup[figure]{font=tiny}
\usepackage[labelformat=empty]{caption}
\usepackage{subcaption}
\usepackage[round, comma]{natbib}

% Defines macros and environments
% \input{style/common.tex}
\usepackage{style/report}

% ------------------------------------------------------------------------------

\colorlet{highlightcol}{gray!90}
\newcommand{\maketag}[1]{\colorbox{highlightcol}{\textcolor{white}
{\MakeUppercase{#1}}}}
\newcommand{\highlight}[1]{\textcolor{highlightcol}{\textbf{#1}}}
\newcommand{\arritem}{\item[\highlight{$\rightarrow$}]}
\newcommand{\positem}{\item[$\highlight{+}$]}
\newcommand{\negitem}{\item[$\highlight{-}$]}
\newcommand{\flexitem}[1]{\item[$\highlight{#1}$]}
\newcommand{\conclbox}[1]{\fbox{\parbox{\textwidth}{\centering\textbf{#1}}}}
\colorlet{GRAY}{gray}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

% ------------------------------------------------------------------------------

\newcommand{\topo}{\mathcal{T}}
\newcommand{\mani}{\mathcal{M}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\RD}{\mathbb{R}^D}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\setN}{\{1, 2, ..., N\}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\x}{\bm{x}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\y}{\bm{y}}
\newcommand{\pv}{\bm{p}}
\newcommand{\qv}{\bm{q}}
\newcommand{\W}{\bm{W}}
\newcommand{\M}{\bm{M}}
\newcommand{\Lap}{\bm{L}}
\newcommand{\D}{\bm{D}}
\newcommand{\K}{\bm{K}}
\newcommand{\G}{\bm{G}}
\newcommand{\I}{\bm{I}}
\newcommand{\E}{\bm{E}}
\newcommand{\twonorm}[1]{\left\lVert #1 \right\rVert^2}

% ------------------------------------------------------------------------------

\title{Semi-Supervised Locally Linear Embedding (SSLLE)}
\author{Lisa Wimmer}
\date{2021/26/02}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter
\insertframetitle}

\begin{document}

\lecturechapter{Application \& Sensitivity Analysis of Critical 
Hyperparameters}
\lecture{Semi-Supervised Locally Linear Embedding (SSLLE)}

% ------------------------------------------------------------------------------
% AGENDA
% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{0 agenda}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\begin{itemize}
\large
\flexitem{1} Problem
\flexitem{2} Local graph-based manifold learning (LGML)
\flexitem{3} Techniques
\begin{itemize}
  \large
  \flexitem{1} Unsupervised
  \flexitem{2} Semi-supervised ~ \maketag{SSLLE}
  \flexitem{3} Challenges
\end{itemize}
\flexitem{4} Sensitivity analysis
\begin{itemize}
  \large
  \flexitem{1} Setup
  \flexitem{2} Results
\end{itemize}
\flexitem{5} Discussion
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------
% MANIFOLD LEARNING PROBLEM
% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{1 problem} ~~ manifold learning}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Situation}. Rapidly increasing amount of data thanks to novel 
applications and data sources

\vspace{0.5cm}

\begin{minipage}[b]{0.7\textwidth}
  \raggedright
  \textbf{Problem.} High data dimensionality detrimental to
  \begin{itemize}
    \arritem Model functionality
    \arritem Interpretability
    \arritem Generalization ability
  \end{itemize}
  \medskip
  \textbf{Manifold assumption.} Data in high-dimensional observation
  space truly sampled from low-dimensional manifold
\end{minipage}%
\begin{minipage}[b]{0.3\textwidth}
  \begin{figure}[H]
    \includegraphics[trim = 70 30 70 30, clip, % left bottom right top
      width = 0.6\textwidth]{figures/s-curve}
  \end{figure}
\end{minipage}

\vfill

\conclbox{How to find a meaningful, structure-preserving embedding?}

\end{frame}

\LARGE
\begin{frame}{\textcolor{gray!90}{1 problem} ~~ manifold learning}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Formal goal of manifold learning.}
\medskip
\begin{itemize}
  \arritem \textbf{Given.} Data $\X = (\x_1, \x_2, ..., \x_N)$, with $\x_i \in 
  \RD$ $\forall i \in \setN$ and $N, D \in \N$, supposedly lying on 
  $d$-dimensional manifold $\mani$ \\
  $\Rightarrow$ $\psi: \mani \rightarrow \Rd$ with $d \ll D, d \in \N$ \\
  $\Rightarrow$ $\X \sim \mani \subset \RD$ 
  \medskip
  \arritem \textbf{Goal.} Find $d$-dimensional Euclidean representation \\
  $\Rightarrow$ $\Y = (\y_1, \y_2, ..., \y_N)$, with 
  $\y_i = \psi(\x_i) \in \R^d$ $\forall i \in \setN$.
\end{itemize}

\vfill

\begin{figure}[H]
 \begin{subfigure}[c]{0.2\textwidth}
  \centering
   \includegraphics[trim = 70 30 70 30, clip, % left bottom right top
      width = 0.63\textwidth]{figures/s-curve}
 \end{subfigure}
 \hfill
 \begin{subfigure}[c]{0.7\textwidth}
   \includegraphics[trim = 80 20 0 0, clip, % left bottom right top
      width = 0.6\textwidth]{figures/s-curve-undone}
 \end{subfigure}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------
% LGML
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{{}}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{white}{\rule{\textwidth}{1pt}}
% \smallskip
% 
% \begin{itemize}
% \large
% \flexitem{1} Problem
% \flexitem{2} \textbf{Local graph-based manifold learning (LGML)}
% \flexitem{3} Techniques
% \begin{itemize}
%   \large
%   \flexitem{1} Unsupervised
%   \flexitem{2} Semi-supervised
%   \flexitem{3} Challenges
% \end{itemize}
% \flexitem{4} Sensitivity analysis
% \begin{itemize}
%   \large
%   \flexitem{1} Setup
%   \flexitem{2} Results
% \end{itemize}
% \flexitem{5} Discussion
% \end{itemize}
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ 2 ~~ LGML}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{2 lgml} ~~ taxonomy}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Landscape}. Various approaches, many of which may be translated into one
another

\vspace{0.5cm}

\begin{minipage}[b]{0.65\textwidth}
  \begin{figure}[H]
    \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
      width = \textwidth]{figures/models_overview}
  \end{figure}
\end{minipage}%
\begin{minipage}[b]{0.35\textwidth}
  \highlight{LEM} ~ Laplacian eigenmaps \\
  \highlight{LLE} ~ Locally linear embedding \\
  \highlight{HLLE} ~ Hessian LLE \\
  \highlight{SSLLE} ~ Semi-supervised LLE 
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{2 lgml} ~~ concept}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Idea}. Capture intrinsic geometry, find principal axes of variability, 
retain most salient ones

\medskip

\begin{figure}[H]
  \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
    width = \textwidth]{figures/kpca_lgml_algo}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{2 lgml} ~~ concept}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Graph representation}. Constructing a skeletal model of the manifold in 
$\RD$

\vspace{0.15cm}

\begin{minipage}[b]{0.67\textwidth}
  \medskip
  \textbf{Vertices.} Given by observations \\
  \textbf{Edges.} Present between neighboring points
    \begin{itemize} 
      \arritem Typically, $k$-neighborhoods
      \arritem Edge weights determined by nearness
    \end{itemize}
  \medskip
  \textbf{Graph functional}. Belief about intrinsic manifold properties at the 
  heart of each method
  \begin{itemize}
    \arritem Smoothness ~ \maketag{LEM}
    \arritem Local linearity ~ \maketag{LLE} \maketag{SSLLE}
    \arritem Curvature ~ \maketag{HLLE}
    \arritem ...
  \end{itemize}
\end{minipage}%
\begin{minipage}[b]{0.33\textwidth}
  \begin{figure}[H]
    \includegraphics[trim = 250 190 220 160, clip, % left bottom right top
      width = \textwidth]{figures/s-curve-connected}
  \end{figure}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{2 lgml} ~~ concept}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Eigenanalysis}. Finding axes of variability in intrinsic 
manifold structure

\begin{itemize}
  \arritem Matrix representation of manifold properties
  \arritem Assessment through eigenanalysis
  \begin{itemize}
    \arritem Directions of variability $\Rightarrow$ eigenvectors
    \arritem Respective degrees of variability $\Rightarrow$ eigenvalues
  \end{itemize}
\end{itemize}

\vfill

\textbf{Dimensionality reduction}. Projection into subspace 
spanned by $d$ principal eigenvectors

\vfill

\begin{figure}[H]
  \raggedright
  \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
    width = 0.7\textwidth]{figures/eigenanalysis}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------
% LGML TECHNIQUES
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{{}}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{white}{\rule{\textwidth}{1pt}}
% \smallskip
% 
% \begin{itemize}
% \large
% \flexitem{1} Problem
% \flexitem{2} Local graph-based manifold learning (LGML)
% \flexitem{3} \textbf{Techniques}
% \begin{itemize}
%   \large
%   \flexitem{1} Unsupervised
%   \flexitem{2} Semi-supervised
%   \flexitem{3} Challenges
% \end{itemize}
% \flexitem{4} Sensitivity analysis
% \begin{itemize}
%   \large
%   \flexitem{1} Setup
%   \flexitem{2} Results
% \end{itemize}
% \flexitem{5} Discussion
% \end{itemize}
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ 3 ~~ TECHNIQUES}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.1 unsupervised} ~~ lem}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Proposal}. \citet{belkinniyogi2001}

\vspace{0.3cm}

\textbf{Idea}. Forcing nearby inputs to be mapped to nearby outputs

\begin{itemize}
  \arritem Notion of smoothness in mapping function
  \arritem Second-order penalty on large gradients
\end{itemize}

\vspace{0.3cm}

\textbf{Graph Laplacian}. Coercing neighborhood graph information into a matrix

\begin{itemize}
  \arritem Weight matrix. $\W = (w)_{ij} \in \R^{N \times N}$, where 
  $w_{ij} = w_{ij}(\twonorm{\x_i - \x_j})$
  \arritem Diagonal matrix of row sums. $\D = 
  \text{\textit{diag}}(\sum_j w_{ij}) \in \R^{N \times N}$
  \arritem Graph Laplacian. $\Lap = \D - \W \in \R^{N \times N}$
\end{itemize}

\vspace{0.3cm}

\textbf{Generalized eigenvalue problem}. 

\begin{fleqn}
  \begin{equation}
    \min_{\Y} \text{\textit{trace}}(\Y^T \Lap \Y), \quad \text{s.t. } 
    \Y^T \D \Y = \I
  \end{equation}
\end{fleqn}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.1 unsupervised} ~~ lle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Proposal}. \citet{roweissaul2000}

\vspace{0.3cm}

\textbf{Idea}. Preserving locally linear reconstructions 

\begin{itemize}
  \arritem Linear reconstruction of points in $\RD$ by their neighbors
  \arritem Reconstruction weights = topological properties
  \arritem Neighborhood patches invariant to dimensionality reduction
\end{itemize}

\vspace{0.3cm}

\begin{figure}[H]
 \begin{subfigure}[b]{0.48\textwidth}
   \includegraphics[trim = 50 50 20 60, clip, % left bottom right top
   width = 0.7\textwidth]{figures/reconstruction-3d}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.48\textwidth}
   \includegraphics[trim = 40 30 0 20, clip, % left bottom right top
   width = 0.5\textwidth]{figures/reconstruction-2d}
 \end{subfigure}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.1 unsupervised} ~~ lle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Procedure}. Solving $N + 1$ optimization problems  

\vspace{0.3cm}

\textbf{Reconstruction loss minimization}. Finding optimal reconstruction 
weights

\begin{fleqn}
  \begin{equation}
    \small
    \min_{\W} \varepsilon(\W) = \min_{\W} \sum_i
    \twonorm{\x_i - \sum_j w_{ij} \x_j}, 
    \quad \text{s.t. } \bm{1}^T \bm{w}_i = 1 \quad \forall i \in \setN
  \end{equation}
\end{fleqn}

\textbf{Embedding loss minimization}. Finding optimal embedding coordinates

\begin{fleqn}
  \begin{equation}
    \small
    \min_{\Y} \Phi(\Y) = \min_{\Y} \sum_i \twonorm{\y_i - \sum_j w_{ij} 
    \y_j},
    \quad  \text{s.t. } \frac{1}{N} \sum_i \y_i \y_i^T = \I,
    \quad \sum_i \y_i = \bm{0} \quad \forall i \in \setN
  \end{equation}
\end{fleqn}

\textbf{Eigenvalue problem}. Define $\E = (\I - \W)^T(\I - \W)$ and set 
$\tilde{\Y} = \Y^T$, such that

\begin{fleqn}
  \begin{equation}
    \small
    \min_{\tilde{\Y}} \text{\textit{trace}}(\tilde{\Y}^T \E \tilde{\Y}), \quad
    \text{s.t. } \frac{1}{N} \tilde{\Y}^T \tilde{\Y} = \I, \quad
    \tilde{\Y}^T\bm{1} = \bm{0}.
  \end{equation}
\end{fleqn}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.1 unsupervised} ~~ hlle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Proposal}. \citet{donohogrimes2003}

\vspace{0.3cm}

\textbf{Idea}. Capturing manifold curvature

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.2 semi-supervised} ~~ sslle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Proposal}. \citet{yangetal2006}

\vspace{0.3cm}

\textbf{Idea}. Improving LLE by use of prior knowledge

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.3 challenges} ~~ critical parameters}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Intrinsic dimensionality}. True sources of variability

\begin{itemize}
  \arritem Considered known with availability of prior information
\end{itemize}

% \vspace{0.3cm}

\textbf{Neighborhood size}. Global vs local structure

\begin{itemize}
  \arritem Tunable (expensive)
\end{itemize}

% \vspace{0.3cm}

\textbf{Regularization constant}. Singularity for $d \ll k$

\begin{itemize}
  \arritem Heuristics
\end{itemize}

% \vspace{0.3cm}

\textbf{Number \& location of prior points}. Utility of prior knowledge ~ 
\maketag{analysis}

\begin{itemize}
  \arritem Exploration vs labeling cost
\end{itemize}

\textbf{Noise level}. Quality of prior knowledge ~ 
\maketag{analysis}

\begin{itemize}
  \arritem How exact must prior information be?
\end{itemize}

\textbf{Confidence parameter}. Belief in prior knowledge

\begin{itemize}
  \arritem Rather robust
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------
% SENSITIVITY ANALYSIS
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{{}}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{white}{\rule{\textwidth}{1pt}}
% \smallskip
% 
% \begin{itemize}
% \large
% \flexitem{1} Problem
% \flexitem{2} Local graph-based manifold learning (LGML)
% \flexitem{3} Techniques
% \begin{itemize}
%   \large
%   \flexitem{1} Unsupervised
%   \flexitem{2} Semi-supervised
%   \flexitem{3} Challenges
% \end{itemize}
% \flexitem{4} \textbf{Sensitivity analysis}
% \begin{itemize}
%   \large
%   \flexitem{1} Setup
%   \flexitem{2} Results
% \end{itemize}
% \flexitem{5} Discussion
% \end{itemize}
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ 4 ~~ SENSITIVITY ANALYSIS}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{4.1 setup} ~~ scenarios}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

% \footnotesize

\medskip

woteva

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{4.1 setup} ~~ evaluation}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

% \footnotesize

\medskip

woteva

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{4.2 results} ~~ foo}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

% \footnotesize

\medskip

woteva

\end{frame}

% ------------------------------------------------------------------------------
% DISCUSSION
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{{}}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{white}{\rule{\textwidth}{1pt}}
% \smallskip
% 
% \begin{itemize}
% \large
% \flexitem{1} Problem
% \flexitem{2} Local graph-based manifold learning (LGML)
% \flexitem{3} Techniques
% \begin{itemize}
%   \large
%   \flexitem{1} Unsupervised
%   \flexitem{2} Semi-supervised
%   \flexitem{3} Challenges
% \end{itemize}
% \flexitem{4} Sensitivity analysis
% \begin{itemize}
%   \large
%   \flexitem{1} Setup
%   \flexitem{2} Results
% \end{itemize}
% \flexitem{5} \textbf{Discussion}
% \end{itemize}
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ 5 ~~ DISCUSSION}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{5 discussion} ~~ foo}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

% \footnotesize

\medskip

woteva

\end{frame}

% ------------------------------------------------------------------------------
% BIBLIOGRAPHY
% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ REFERENCES}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{}
\normalsize

\bibliography{bibliography}
\bibliographystyle{dcu}

\end{frame}

% ------------------------------------------------------------------------------

\endlecture

\end{document}