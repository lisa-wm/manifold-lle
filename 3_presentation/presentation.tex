\documentclass[11pt, compress, t, notes = noshow, xcolor = table, 
aspectratio = 1610]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}

\usepackage{framed}

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{amsmath, nccmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{caption}
\captionsetup[figure]{font=tiny}
\usepackage[labelformat=empty]{caption}
\usepackage{subcaption}
\usepackage[round, comma]{natbib}

% Defines macros and environments
% \input{style/common.tex}
\usepackage{style/report}

% ------------------------------------------------------------------------------

\colorlet{highlightcol}{gray!90}
\newcommand{\maketag}[1]{\colorbox{highlightcol}{\textcolor{white}
{\MakeUppercase{#1}}}}
\newcommand{\highlight}[1]{\textcolor{highlightcol}{\textbf{#1}}}
\newcommand{\arritem}{\item[\highlight{$\rightarrow$}]}
\newcommand{\positem}{\item[$\highlight{+}$]}
\newcommand{\negitem}{\item[$\highlight{-}$]}
\newcommand{\flexitem}[1]{\item[$\highlight{#1}$]}
\newcommand{\conclbox}[1]{\fbox{\parbox{\textwidth}{\textbf{#1}}}}
\colorlet{GRAY}{gray}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

% ------------------------------------------------------------------------------

\newcommand{\topo}{\mathcal{T}}
\newcommand{\mani}{\mathcal{M}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\RD}{\mathbb{R}^D}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\setN}{\{1, 2, ..., N\}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\x}{\bm{x}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\y}{\bm{y}}
\newcommand{\pv}{\bm{p}}
\newcommand{\qv}{\bm{q}}
\newcommand{\D}{\bm{D}}
\newcommand{\E}{\bm{E}}
\newcommand{\G}{\bm{G}}
\newcommand{\Hes}{\bm{H}}
\newcommand{\I}{\bm{I}}
\newcommand{\K}{\bm{K}}
\newcommand{\Lap}{\bm{L}}
\newcommand{\M}{\bm{M}}
\newcommand{\W}{\bm{W}}
\newcommand{\twonorm}[1]{\left\lVert #1 \right\rVert^2}
\newcommand{\frobnorm}[1]{\left\lVert #1 \right\rVert^2_F}

% ------------------------------------------------------------------------------

\title{Semi-Supervised Locally Linear Embedding (SSLLE)}
\author{Lisa Wimmer}
\date{2021/26/02}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter
\insertframetitle}

\begin{document}

\lecturechapter{Application \& Sensitivity Analysis of Critical 
Hyperparameters}
\lecture{Semi-Supervised Locally Linear Embedding (SSLLE)}

% ------------------------------------------------------------------------------
% AGENDA
% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{0 agenda}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\begin{itemize}
\large
\flexitem{1} Problem
\flexitem{2} Local graph-based manifold learning (LGML)
\flexitem{3} Techniques
\begin{itemize}
  \large
  \flexitem{1} Unsupervised
  \flexitem{2} Semi-supervised ~ \maketag{SSLLE}
  \flexitem{3} Challenges
\end{itemize}
\flexitem{4} Sensitivity analysis
\begin{itemize}
  \large
  \flexitem{1} Setup
  \flexitem{2} Results
\end{itemize}
\flexitem{5} Discussion
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------
% MANIFOLD LEARNING PROBLEM
% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{1 problem} ~~ manifold learning}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Situation}. Rapidly increasing amount of data thanks to novel 
applications and data sources

\vspace{0.5cm}

\begin{minipage}[b]{0.7\textwidth}
  \raggedright
  \textbf{Problem.} High data dimensionality detrimental to
  \begin{itemize}
    \arritem Model functionality
    \arritem Interpretability
    \arritem Generalization ability
  \end{itemize}
  \medskip
  \textbf{Manifold assumption.} Data in high-dimensional observation
  space truly sampled from low-dimensional manifold
\end{minipage}%
\begin{minipage}[b]{0.3\textwidth}
  \begin{figure}[H]
    \includegraphics[trim = 70 30 70 30, clip, % left bottom right top
      width = 0.6\textwidth]{figures/s-curve}
  \end{figure}
\end{minipage}

\vfill

\conclbox{How to find a meaningful, structure-preserving embedding?}

\end{frame}

\LARGE
\begin{frame}{\textcolor{gray!90}{1 problem} ~~ manifold learning}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Formal goal of manifold learning.}
\medskip
\begin{itemize}
  \arritem \textbf{Given.} Data $\X = (\x_1, \x_2, ..., \x_N)$, with $\x_i \in 
  \RD$ $\forall i \in \setN$ and $N, D \in \N$, supposedly lying on 
  $d$-dimensional manifold $\mani$ \\
  $\Rightarrow$ $\psi: \mani \rightarrow \Rd$ with $d \ll D, d \in \N$ \\
  $\Rightarrow$ $\X \sim \mani \subset \RD$ 
  \medskip
  \arritem \textbf{Goal.} Find $d$-dimensional Euclidean representation \\
  $\Rightarrow$ $\Y = (\y_1, \y_2, ..., \y_N)$, with 
  $\y_i = \psi(\x_i) \in \R^d$ $\forall i \in \setN$.
\end{itemize}

\vfill

\begin{figure}[H]
 \begin{subfigure}[c]{0.2\textwidth}
  \centering
   \includegraphics[trim = 70 30 70 30, clip, % left bottom right top
      width = 0.63\textwidth]{figures/s-curve}
 \end{subfigure}
 \hfill
 \begin{subfigure}[c]{0.7\textwidth}
   \includegraphics[trim = 80 20 0 0, clip, % left bottom right top
      width = 0.6\textwidth]{figures/s-curve-undone}
 \end{subfigure}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------
% LGML
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{{}}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{white}{\rule{\textwidth}{1pt}}
% \smallskip
% 
% \begin{itemize}
% \large
% \flexitem{1} Problem
% \flexitem{2} \textbf{Local graph-based manifold learning (LGML)}
% \flexitem{3} Techniques
% \begin{itemize}
%   \large
%   \flexitem{1} Unsupervised
%   \flexitem{2} Semi-supervised
%   \flexitem{3} Challenges
% \end{itemize}
% \flexitem{4} Sensitivity analysis
% \begin{itemize}
%   \large
%   \flexitem{1} Setup
%   \flexitem{2} Results
% \end{itemize}
% \flexitem{5} Discussion
% \end{itemize}
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ 2 ~~ LGML}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{2 lgml} ~~ taxonomy}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Landscape}. Various approaches, many of which may be translated into one
another

\vspace{0.5cm}

\begin{minipage}[b]{0.65\textwidth}
  \begin{figure}[H]
    \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
      width = \textwidth]{figures/models_overview}
  \end{figure}
\end{minipage}%
\begin{minipage}[b]{0.35\textwidth}
  \highlight{LEM} ~ Laplacian eigenmaps \\
  \highlight{LLE} ~ Locally linear embedding \\
  \highlight{HLLE} ~ Hessian LLE \\
  \highlight{SSLLE} ~ Semi-supervised LLE 
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{2 lgml} ~~ concept}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Idea}. Capture intrinsic geometry, find principal axes of variability, 
retain most salient ones

\medskip

\begin{figure}[H]
  \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
    width = \textwidth]{figures/kpca_lgml_algo}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{2 lgml} ~~ concept}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Graph representation}. Constructing a skeletal model of the manifold in 
$\RD$

\vspace{0.15cm}

\begin{minipage}[b]{0.67\textwidth}
  \medskip
  \textbf{Vertices.} Given by observations \\
  \textbf{Edges.} Present between neighboring points
    \begin{itemize} 
      \arritem Typically, $k$-neighborhoods
      \arritem Edge weights determined by nearness
    \end{itemize}
  \medskip
  \textbf{Graph functional}. Belief about intrinsic manifold properties at the 
  heart of each method
  \begin{itemize}
    \arritem Smoothness ~ \maketag{LEM}
    \arritem Local linearity ~ \maketag{LLE} \maketag{SSLLE}
    \arritem Curviness ~ \maketag{HLLE}
    \arritem ...
  \end{itemize}
\end{minipage}%
\begin{minipage}[b]{0.33\textwidth}
  \begin{figure}[H]
    \includegraphics[trim = 250 190 220 160, clip, % left bottom right top
      width = \textwidth]{figures/s-curve-connected}
  \end{figure}
\end{minipage}

\vspace{0.3cm}

\conclbox{Achievements: non-linearity \& locality}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{2 lgml} ~~ concept}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Eigenanalysis}. Finding axes of variability in intrinsic 
manifold structure

\begin{itemize}
  \arritem Matrix representation of manifold properties
  \arritem Assessment through eigenanalysis
  \begin{itemize}
    \arritem Directions of variability $\Rightarrow$ eigenvectors
    \arritem Respective degrees of variability $\Rightarrow$ eigenvalues
  \end{itemize}
\end{itemize}

\vspace{0.3cm}

\textbf{Dimensionality reduction}. Projection into subspace 
spanned by $d$ principal eigenvectors

\vspace{0.3cm}

\begin{figure}[H]
  \raggedright
  \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
    width = 0.7\textwidth]{figures/eigenanalysis}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------
% LGML TECHNIQUES
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{{}}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{white}{\rule{\textwidth}{1pt}}
% \smallskip
% 
% \begin{itemize}
% \large
% \flexitem{1} Problem
% \flexitem{2} Local graph-based manifold learning (LGML)
% \flexitem{3} \textbf{Techniques}
% \begin{itemize}
%   \large
%   \flexitem{1} Unsupervised
%   \flexitem{2} Semi-supervised
%   \flexitem{3} Challenges
% \end{itemize}
% \flexitem{4} Sensitivity analysis
% \begin{itemize}
%   \large
%   \flexitem{1} Setup
%   \flexitem{2} Results
% \end{itemize}
% \flexitem{5} Discussion
% \end{itemize}
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ 3 ~~ TECHNIQUES}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.1 unsupervised} ~~ lem}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Proposal}. \citet{belkinniyogi2001}

\vspace{0.3cm}

\textbf{Idea}. Forcing nearby inputs to be mapped to nearby outputs

\begin{itemize}
  \arritem Notion of smoothness in mapping function
  \arritem Second-order penalty on gradient
\end{itemize}

\vspace{0.3cm}

\textbf{Graph Laplacian}. Discrete approximation of Laplace-Beltrami operator

\begin{itemize}
  \arritem Weight matrix. $\W = (w)_{ij} \in \R^{N \times N}$, where 
  $w_{ij} = w_{ij}(\twonorm{\x_i - \x_j})$
  % \arritem Diagonal matrix of row sums. $\D = 
  % \text{\textit{diag}}(\sum_j w_{ij}) \in \R^{N \times N}$
  \arritem Graph Laplacian. $\Lap = \D - \W \in \R^{N \times N}$, 
  $\D = \text{\textit{diag}}(\sum_j w_{ij}) \in \R^{N \times N}$
\end{itemize}

\vspace{0.3cm}

\textbf{Generalized eigenvalue problem}. 

\begin{fleqn}
  \begin{equation}
    \small
    \min_{\Y} \text{\textit{trace}}(\Y^T \Lap \Y), \quad \text{s.t. } 
    \Y^T \D \Y = \I
  \end{equation}
\end{fleqn}

\conclbox{Solution: bottom $d + 1$ eigenvectors}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.1 unsupervised} ~~ lle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Proposal}. \citet{roweissaul2000}

\vspace{0.3cm}

\textbf{Idea}. Preserving locally linear reconstructions 

\begin{itemize}
  \arritem Linear reconstruction of points in $\RD$ by their neighbors
  \arritem Reconstruction weights = topological properties
  \arritem Neighborhood patches invariant to dimensionality reduction
\end{itemize}

\vspace{0.3cm}

\begin{figure}[H]
 \begin{subfigure}[b]{0.48\textwidth}
   \includegraphics[trim = 50 50 20 60, clip, % left bottom right top
   width = 0.7\textwidth]{figures/reconstruction-3d}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.48\textwidth}
   \includegraphics[trim = 40 30 0 20, clip, % left bottom right top
   width = 0.5\textwidth]{figures/reconstruction-2d}
 \end{subfigure}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.1 unsupervised} ~~ lle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Reconstruction loss minimization}. Finding optimal reconstruction 
weights

\begin{fleqn}
  \begin{equation}
    \small
    \min_{\W} \varepsilon(\W) = \min_{\W} \sum_i
    \twonorm{\x_i - \sum_j w_{ij} \x_j}, 
    \quad \text{s.t. } \bm{1}^T \bm{w}_i = 1 \quad \forall i \in \setN
  \end{equation}
\end{fleqn}

\textbf{Embedding loss minimization}. Finding optimal embedding coordinates

\begin{fleqn}
  \begin{equation}
    \small
    \min_{\Y} \Phi(\Y) = \min_{\Y} \sum_i \twonorm{\y_i - \sum_j w_{ij} 
    \y_j},
    \quad  \text{s.t. } \frac{1}{N} \sum_i \y_i \y_i^T = \I,
    \quad \sum_i \y_i = \bm{0} \quad \forall i \in \setN
  \end{equation}
\end{fleqn}

\textbf{Eigenvalue problem}. Define $\E = (\I - \W)^T(\I - \W)$ 
% and set $\tilde{\Y} = \Y^T$
, such that

\begin{fleqn}
  \begin{equation}
    \small
    \min_{\Y} \text{\textit{trace}}(\Y^T \E \Y), \quad
    \text{s.t. } \frac{1}{N} \Y^T \Y = \I, \quad
    \Y^T\bm{1} = \bm{0}.
  \end{equation}
\end{fleqn}

\conclbox{Solution: bottom $d + 1$ eigenvectors}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.1 unsupervised} ~~ hlle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Proposal}. \citet{donohogrimes2003}

\vspace{0.3cm}

\textbf{Idea}. Finding a truly locally linear mapping while preserving local 
isometry

\begin{itemize}
  \arritem Notion of curviness in mapping function
  \arritem Second-order penalty on Hessian
  \arritem Strong convergence guarantees but rather complex computations
\end{itemize}

\vspace{0.3cm}

\textbf{Hessian functional}. Measuring average curviness over $\mani$

\begin{itemize}
  \arritem Continuous functional. 
  $\mathscr{H}(f) = \int_{\mani} \frobnorm{\Hes_f^{\text{loc}}(\pv)} d\pv$
  \arritem Hessian estimators $\Hes_{\ell}$ derived from locally linear 
  neighborhood patches
  \arritem Empirical approximator.
  $\mathcal{H}_{ij} = \sum_{\ell} \sum_m (\Hes_{\ell})_{m,i}
  (\Hes_{\ell})_{m,j}$
  \arritem Finding null space of $\mathcal{H}$
\end{itemize}

\vspace{0.3cm}

\conclbox{Solution: bottom $d + 1$ eigenvectors + scaling}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.2 semi-supervised} ~~ sslle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Proposal}. \citet{yangetal2006}

\vspace{0.3cm}

\textbf{Problem}. Embedding found by unsupervised methods not always meaningful

\vspace{0.3cm}

\textbf{Idea}. Improving LLE by use of prior knowledge

\vspace{0.3cm}

\textbf{Semi-supervision}. Anchoring embedding at some prior points with known 
coordinates

\begin{itemize}
  \arritem More active than semi-supervised learning?
  \arritem Setting. Information available or to be obtained by querying the 
  oracle
  \arritem Goal. Maximum information at little expense $\Rightarrow$ careful 
  choice of prior points
\end{itemize}

\vspace{0.3cm}

\begin{figure}[H]
 \begin{subfigure}[c]{0.2\textwidth}
  \centering
   \includegraphics[trim = 70 30 70 30, clip, % left bottom right top
      width = 0.5\textwidth]{figures/s-curve-pp-maxmin}
 \end{subfigure}
 \hfill
 \begin{subfigure}[c]{0.7\textwidth}
   \includegraphics[trim = 80 20 0 0, clip, % left bottom right top
      width = 0.5\textwidth]{figures/s-curve-undone-pp-maxmin}
 \end{subfigure}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.2 semi-supervised} ~~ sslle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

% \textbf{Choice of prior points}. Basically, three options
% 
% \begin{itemize}
%   \arritem Pre-existing prior information
%   \arritem Random choice
%   \arritem Maximum exploration
% \end{itemize}
% 
% \vspace{0.3cm}

% \textbf{Types of prior information}. Exact vs inexact
% 
% \begin{itemize}
%   \arritem Level of confidence encoded in parameter
% \end{itemize}
% 
% \vspace{0.3cm}
% 
% \textbf{Algorithmic impact}. Recall LLE eigenvalue problem
% 
% \begin{fleqn}
%   \begin{equation*}
%     \small
%     \min_{\Y} \text{\textit{trace}}(\Y^T \E \Y), \quad
%     \text{s.t. } \frac{1}{N} \Y^T \Y = \I, \quad
%     \Y^T\bm{1} = \bm{0}.
%   \end{equation*}
% \end{fleqn}
% 
% $\Rightarrow$ Partitioning of $\E$ and $\Y$ 

% \begin{figure}[H]
%   \raggedright
%   \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
%   width = 0.4\textwidth]{figures/matrix_partition}
% \end{figure}

\textbf{Types of prior information}. Exact vs inexact
\begin{itemize}
  \arritem Level of confidence encoded in parameter $\beta$
\end{itemize}
% \vspace{0.1cm}

\begin{minipage}[t]{0.6\textwidth}
\phantom{foo} \\
\textbf{Algorithmic impact}. Recall LLE eigenvalue problem
\begin{fleqn}
  \begin{equation*}
    \min_{\Y} \text{\textit{trace}}(\Y^T \E \Y), \quad
    \text{s.t. } \frac{1}{N} \Y^T \Y = \I, \quad
    \Y^T\bm{1} = \bm{0}.
  \end{equation*}
\end{fleqn}
$\Rightarrow$ Partitioning of $\E$ and $\Y$ \\
\end{minipage}%
\begin{minipage}[t]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.35\textwidth}
  \begin{figure}[H]
    \includegraphics[trim = 0 0 0 -15, clip, % left bottom right top
      width = 1.1\textwidth]{figures/matrix_partition}
  \end{figure}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.2 semi-supervised} ~~ sslle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Modified optimization problem}. Exact information 

\begin{fleqn}
  \begin{equation}
    \min_{\Y_2}
    \begin{bmatrix} \textcolor{gray!90}{\Y_1} & \Y_2 \end{bmatrix}
    \begin{bmatrix} \textcolor{gray!90}{M_{11}} & M_{12} \\ M_{21} & M_{22} 
    \end{bmatrix}
    \begin{bmatrix} \textcolor{gray!90}{\Y_1^T} \\ \Y_2^T \end{bmatrix}
  \end{equation}
\end{fleqn}

\begin{fleqn}
  \begin{equation}
    \Leftrightarrow \Y_2^T = M_{22}^{-1} M_{12} \textcolor{gray!90}{\Y_1^T}
  \end{equation}
\end{fleqn}

\textbf{Modified optimization problem}. Inexact information 

\begin{fleqn}
  \begin{equation}
    \min_{\Y}
    \begin{bmatrix} \Y_1 & \Y_2 \end{bmatrix}
    \begin{bmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{bmatrix}
    \begin{bmatrix} \Y_1^T \\ \Y_2^T \end{bmatrix} +
    \beta \frobnorm{\Y_1^T - \textcolor{gray!90}{\hat{\Y}_1^T}}
  \end{equation}
\end{fleqn}

\begin{fleqn}
  \begin{equation}
    \Leftrightarrow \begin{bmatrix} \textcolor{gray!90}{M_{11}} +
    \beta \I & M_{12} \\ M_{21} & M_{22} \end{bmatrix}
    \begin{bmatrix} \Y_1^T \\ \Y_2^T \end{bmatrix} =
    \begin{bmatrix} \textcolor{gray!90}{\hat{\Y}_1^T} \\ \bm{0} \end{bmatrix}
  \end{equation}
\end{fleqn}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.2 semi-supervised} ~~ sslle}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Choice of landmark points.} Basically, three options

\begin{itemize}
  \arritem Pre-existing prior information $\Rightarrow$ worst case: poor 
  coverage
  \arritem Random sampling
  \arritem Maximum coverage
\end{itemize}

\begin{figure}[H]
 \begin{subfigure}[c]{0.3\textwidth}
  \centering
   \includegraphics[trim = 70 30 70 30, clip, % left bottom right top
      width = \textwidth]{figures/s-curve-undone-pp-poor}
 \end{subfigure}
 \hfill
 \begin{subfigure}[c]{0.3\textwidth}
  \centering
   \includegraphics[trim = 70 30 70 30, clip, % left bottom right top
      width = \textwidth]{figures/s-curve-undone-pp-random}
 \end{subfigure}
 \hfill
 \begin{subfigure}[c]{0.3\textwidth}
   \includegraphics[trim = 70 30 70 30, clip, % left bottom right top
      width = \textwidth]{figures/s-curve-undone-pp-maxmin}
 \end{subfigure}
\end{figure}

\vspace{0.3cm}

\textbf{Maximum coverage.} Points scattered across manifold surface

\begin{itemize}
  \arritem Goodness of solution depending on condition number $\kappa(M_{22})$
  \arritem $\kappa(M_{22})$ minimal at maximization of minimum pairwise 
  distances between prior points
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{3.3 challenges} ~~ critical parameters}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Intrinsic dimensionality}. True sources of variability

\begin{itemize}
  \arritem Considered known with availability of prior information
\end{itemize}

% \vspace{0.3cm}

\textbf{Neighborhood size}. Global vs local structure

\begin{itemize}
  \arritem Tunable (expensive)
\end{itemize}

% \vspace{0.3cm}

\textbf{Regularization constant}. Singularity for $D < k$

\begin{itemize}
  \arritem Heuristics
\end{itemize}

% \vspace{0.3cm}

\textbf{Number \& location of prior points}. Utility of prior knowledge ~ 
\maketag{analysis}

\begin{itemize}
  \arritem Exploration vs labeling cost
\end{itemize}

\textbf{Noise level}. Quality of prior knowledge ~ 
\maketag{analysis}

\begin{itemize}
  \arritem How exact must prior information be?
\end{itemize}

\textbf{Confidence parameter}. Belief in prior knowledge

\begin{itemize}
  \arritem Rather robust
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------
% SENSITIVITY ANALYSIS
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{{}}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{white}{\rule{\textwidth}{1pt}}
% \smallskip
% 
% \begin{itemize}
% \large
% \flexitem{1} Problem
% \flexitem{2} Local graph-based manifold learning (LGML)
% \flexitem{3} Techniques
% \begin{itemize}
%   \large
%   \flexitem{1} Unsupervised
%   \flexitem{2} Semi-supervised
%   \flexitem{3} Challenges
% \end{itemize}
% \flexitem{4} \textbf{Sensitivity analysis}
% \begin{itemize}
%   \large
%   \flexitem{1} Setup
%   \flexitem{2} Results
% \end{itemize}
% \flexitem{5} Discussion
% \end{itemize}
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ 4 ~~ SENSITIVITY ANALYSIS}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{4.1 setup} ~~ data}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Swiss roll}. 

\vspace{0.3cm}

\textbf{Incomplete tire}. 

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{4.1 setup} ~~ scenarios}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\textbf{Sensitivity analysis I}. Landmark coverage $\times$ number of landmark 
points

\begin{itemize}
  \arritem Landmark coverage $\in \{ \text{poor}, \text{ random}, 
  \text{ maximum}\}$
  \arritem Number of landmark points $\in \{2, 4, 6, 8, 10, 12\}$
  \arritem 18 scenarios
\end{itemize}

\vspace{0.3cm}

\conclbox{Best case: maximum coverage, 12 landmarks}

\vspace{0.3cm}

\textbf{Sensitivity analysis II}. Noise level $\times$ number of landmark 
points

\begin{itemize}
  \arritem Simulation of inexact prior information through perturbation with 
  Gaussian noise
  \arritem Noise level $\in \{0.1, 0.5, 1, 3, 5\}$ 
  $\Rightarrow$ standard deviation
  \arritem Number of landmark points $\in \{2, 4, 6, 8, 10, 12\}$
  \arritem 30 scenarios
\end{itemize}

\vspace{0.3cm}

\conclbox{Best case: noise level 0.1, 12 landmarks}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{4.1 setup} ~~ evaluation}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

% \footnotesize

\medskip

woteva

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{4.2 results} ~~ foo}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

% \footnotesize

\medskip

woteva

\end{frame}

% ------------------------------------------------------------------------------
% DISCUSSION
% ------------------------------------------------------------------------------

% \LARGE
% \begin{frame}{{}}
% \normalsize
% \vspace{-0.5cm}
% \noindent \textcolor{white}{\rule{\textwidth}{1pt}}
% \smallskip
% 
% \begin{itemize}
% \large
% \flexitem{1} Problem
% \flexitem{2} Local graph-based manifold learning (LGML)
% \flexitem{3} Techniques
% \begin{itemize}
%   \large
%   \flexitem{1} Unsupervised
%   \flexitem{2} Semi-supervised
%   \flexitem{3} Challenges
% \end{itemize}
% \flexitem{4} Sensitivity analysis
% \begin{itemize}
%   \large
%   \flexitem{1} Setup
%   \flexitem{2} Results
% \end{itemize}
% \flexitem{5} \textbf{Discussion}
% \end{itemize}
% 
% \end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ 5 ~~ DISCUSSION}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\textcolor{gray!90}{5 discussion} ~~ foo}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

% \footnotesize

\medskip

woteva

\end{frame}

% ------------------------------------------------------------------------------
% BIBLIOGRAPHY
% ------------------------------------------------------------------------------

\LARGE
\begin{frame}{\phantom{foo}}
\normalsize
\vspace{-0.5cm}
\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}
\smallskip

\Huge
\hspace{0pt}
\vfill
\textbf{\highlight{~~ REFERENCES}}
\vfill
\hspace{0pt}

\noindent \textcolor{gray!90}{\rule{\textwidth}{1pt}}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{}
\footnotesize

\bibliography{bibliography}
\bibliographystyle{dcu}

\end{frame}

% ------------------------------------------------------------------------------

\endlecture

\end{document}