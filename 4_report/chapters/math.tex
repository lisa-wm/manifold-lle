\subsection{Basic Concepts in Manifold Learning}
\label{basic-conc}

\begin{itemize}
  \item Topologies
  \item Manifolds
  \item Tangent spaces
  \item Eigenvalues/eigenvectors, spectral decomposition
\end{itemize}

\textcolor{red}{AS BRIEF AS POSSIBLE, but concepts and corresponding notation
should be introduced properly}

\subsection{Evaluation of Manifold Learning}
\label{eval-mani}

\begin{itemize}
  \item How can we evaluate manifold learning?
\end{itemize}

How can we evaluate manifold learning?

Which measure will be used in experiments?

\subsection{Neighborhoods \& Spectral Graph Theory}
\label{neigh-graph}

\begin{itemize}
  \item Neighborhoods and neighborhood graphs
  \item Linear reconstruction and reconstruction error
  \item Degree and adjacency matrix
\end{itemize}

\subsection{Laplacian Eigenmaps}
\label{laplace}

\begin{itemize}
  \item Laplacian-Beltrami operator
  \item Laplacian eigenmaps
\end{itemize}

\textcolor{red}{Can be interpreted as framework for both LLE and HLLE; only to
the extent where it helps to frame LLE}

% Explanation found: "In practical terms, the computations required by Hessian LLE 
% have more in common with LLE, but Laplacian eigenmaps provide a
% clearer framework for understanding how the method works. In the same way that 
% Laplacian eigenmaps uses a graph-based approximation to the Laplace-Beltrami operator on the
% data manifold, Hessian LLE relies on a numerical approximation to the Hessian matrix of a
% function defined on the data manifold."
