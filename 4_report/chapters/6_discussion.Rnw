The experiments have shown that SSLLE performs well for synthesized examples 
and under favorable conditions.
Real-world applications, where data become truly high-dimensional and cannot be 
expected to lie on simple and well-behaved manifolds, will also face some 
substantial drawbacks.
The following synopsis discusses the major strengths and drawbacks, many of 
which hold equally true for the other LGML algorithms.
\\

\textbf{Strengths}
\begin{itemize}
  \item[] \textbf{Theoretical simplicity.} foo
  \item[] \textbf{Hyperparameter sparsity.} foo
  \item[] \textbf{Computational efficiency.} foo
  \item[] \textbf{Positive impact of semi-supervision.} foo
\end{itemize}

\textbf{Drawbacks}
\begin{itemize}
  \item[] \textbf{Dependence on graph approximation.} foo
  \item[] \textbf{No indication of intrinsic dimensionality.} agnostic
  \item[] \textbf{Fundamental weakness in optimization problem.} foo
  \item[] \textbf{Weak preservation of geometric properties.} foo
  \item[] \textbf{Dependence on prior information.} foo
\end{itemize}

Theoretical convergence? (e.g., ISOMAP has this)

Potential shortcoming: what if manifold is not well-sampled? Not a problem with
synthetic data, but IRL. But probably problematic with all manifold approaches

This is directly related to the COD -- local methods require dense sampling 
\citep{vandermaatenetal2009}

Also: generalization to new points (w/o recomputing everything)
neighborhood-preserving propositions --> fundamental problem: except for prior
points, it is deterministic (as opposed to generative approaches, such as 
autoencoders)