\subsection{Unsupervised Techniques}
\label{unsupervised}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps (LEM)}
\label{lem}

The reason for LEM to appear in this report alongside the LLE family is its 
underlying theory both providing a foundation for LLE \citep{belkinniyogi2003} 
and closely relating to the theoretical concepts in HLLE 
\citep{donohogrimes2003}.
LEM is centered around the preservation of locality, i.e., mapping nearby
inputs to nearby outputs.
Locality is enforced via the \textit{Laplace-Beltrami operator} defined on
smooth, compact manifolds, and operationalized by means of the 
\textit{graph Laplacian} acting as a discrete approximator 
\citep{belkinniyogi2003}.
This idea is best understood recalling that the similarity of outputs for
similar inputs is essentially a notion of smoothness and can thus be controlled
by a size constraint on the gradient of the mapping function.
% \\

\begin{minipage}[b]{0.7\textwidth}
  \textbf{Continuous justification.}
  Consider the twice differentiable function $f: \mani \rightarrow \R$ mapping
  $\pv, \qv$ to $f(\pv)$ and $f(\qv)$, respectively.
  On $\mani$ these points are connected by a length-parametrized curve $c(t)$.
  Denote the geodesic distance between $\pv$ and $\qv$ by $\ell$, such that
  $\pv = c(0)$ and $\qv = c(\ell)$.
  Gradients of $f$ with respect to $\pv$ are defined in the local tangent space
  $T_{\pv}(\mani)$.
  Local tangent spaces of $\mani$ are $d$-dimensional hyperplanes 
  \citep{sudderth2002}, as shown exemplarily by figure \ref{fig_sphere_tangent}.
  If $\pv$ is identified with the origin of $T_{\pv}(\mani)$, the tangent space
  inherits an orthonormal coordinate system from endowing
  $T_{\pv}(\mani)$ with the inner product of $\Rd$ \citep{donohogrimes2003}.
  With this, the distance $|f(\pv) - f(\qv)|$ of mappings can be expressed as
  the length of
  $\int_0^{\ell} \langle \nabla f(c(t)), c^{\prime}(t) \rangle dt$.
  In other words, the geodesic connecting $\pv$ and $\qv$ is projected
  onto $T_{\pv}(\mani)$, and the length of this projection depends on the
  gradient of $f$ and the curve velocity.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.25\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 70 60 30, clip, % left bottom right top
      width = 0.8\textwidth]{figures/sphere_tangent}
    \caption[Tangent hyperplane for two-dimensional unit sphere]{
    \raggedright
    Tangent hyperplane for a point on the two-dimensional unit sphere. 
    \textit{Source:} own representation.}
    \label{fig_sphere_tangent}
  \end{figure}
\end{minipage}

\vspace{0.5cm}

It can be shown that
$|f(\pv) - f(\qv)| \leq \| \nabla f(\pv) \| \cdot \| \pv - \qv\| + o$, where
$o$ marks a term of vanishing size \citep{belkinniyogi2008}.
$\| \nabla f \|$ thus controls how far apart points are mapped on the real line.
Consequently, the goal is to find a mapping that, on average, preserves
locality by minimizing $\int_{\mani}\| \nabla f \|^2$.
This is just equivalent to minimizing $\int_{\mani} \mathcal{L}(f)f$ with the
Laplace-Beltrami operator $\mathcal{L}$ \citep{belkinniyogi2003}.
For $\mathcal{L} f = \lambda f$, $f$ is an eigenfunction of $\mathcal{L}$ with 
$\lambda \in \R$ as its associated eigenvalue.
These eigenfunctions are orthogonal and have real eigenvalues, making them 
natural candidates for a functional basis \citep{levy2006}.
The optimal embedding map is then given by the $d$ principal eigenfunctions of
$\mathcal{L}$ after removing the bottom one which would map $\mani$ to a
single point \citep{belkinniyogi2003}.
\\

\textbf{Finite approximation.}
Now the same reasoning can be applied to the neighborhood graph approximation of
$\mani$.
Mapping nearby inputs to nearby is achieved by assigning edge weights\footnote{
These weights stem from the heat kernel intimately related to the 
Laplace-Beltrami operator and ensure positive semi-definiteness of the resulting
graph Laplacian. 
As an alternative, \citet{belkinniyogi2003} propose a simpler kernel that is 
equal to 1 for connected nodes and 0 otherwise.
} $w_{ij} = \exp(\frac{1}{t} \twonorm{\x_i - \x_j}), t \in \R$, if 
$\x_i, \x_j$ are are connected, and zero otherwise.
Clearly, edges between closer points receive larger weights.
The \textit{adjacency matrix} $\D = (d)_{ij} \in \R^{N \times N}$ takes the row
sums of the \textit{weight matrix} $\W = (w)_{ij} \in \R^{N \times N}$ on its 
diagonals.
Penalizing output disparities more severely for pairs of nearby points, i.e.,
pairs with a large weight coefficient, the smoothness requirement may be stated
as follows:

\begin{equation*}
  \begin{split}
    \min_{\Y} \sum_{i, j} \| \y_i - \y_j \|^2 w_{ij}
    &= \min_{\Y} \sum_{i, j} \y_i^T \y_i w_{ij} + \y_j^T \y_j
    w_{ij} -
    2 \y_i^T \y_j w_{ij} \\
    &= \min_{\Y} \sum_i \y_i^T \y_i d_{ii} + \sum_j \y_j^T \y_j
    d_{jj} - 2 \sum_{i, j} \y_i^T \y_j w_{ij}.
  \end{split}
\end{equation*}

Now, define the \textit{graph Laplacian} as
$\Lap = \D - \W \in \R^{N \times N}$, thereby coercing
all information about the graph structure into a single matrix
representation.
With $\Lap$ the above can be rewritten as generalized eigenvalue problem, 
adhering to the LGML algorithmic concept:

\begin{equation}
  \min_{\Y} \text{\textit{trace}}(\Y^T \Lap \Y), \quad \text{s.t. } 
  \Y^T \D \Y = \I,
  \label{eq-obj-lem}
\end{equation}

which is solved by eigendecomposition of $\Lap$ \citep{belkinniyogi2003}.
Analogous to the continuous case, the bottom eigenvector with zero eigenvalue is
constant and must be discarded\footnote{
As a consequence of its definition, $\Lap$ always has at least one eigenpair 
consisting of a zero eigenvalue and a constant eigenvector.
In fact, the multiplicity of the zero eigenvalue corresponds to the number of 
connected graph components \citep{marsden2013}.
}.
The subsequent $d$ eigenvectors hold the desired low-dimensional
embedding coordinates \citep{levy2006}.

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

In proposing LEM, \citet{belkinniyogi2003} also demonstrated how the somewhat 
earlier LLE algorithm may be reinterpreted within the LEM framework: it can be 
shown to approximate the graph Laplacian under certain conditions and thus 
asymptotically approach the Laplace-Beltrami operator.
More recent research, however, suggests that these conditions might be more 
restrictive than previously assumed. 
In particular, convergence appears to depend on the choice of a regularization 
parameter required in the case of $D < k$ \citep{wuwu2018}.
\\

\begin{minipage}[b]{0.6\textwidth}
  \textbf{Idea.}
  The initial proposal by \citet{roweissaul2000}, ignorant to these findings, 
  was made with a different, and rather heuristically motivated, intuition.
  LLE relies on a simple yet powerful idea.
  Each point $\x_i$ in the $D$-dimensional input space is expressed as a convex 
  combination of its neighbors, such that the weighting coefficients of this 
  reconstruction essentially represent the edge weights of the neighborhood 
  graph around $\x_i$. 
  These (generalized) barycentric coordinates now bear a crucial property: they 
  are invariant to rotation, rescaling and translation of the neighborhood, and 
  thus topological properties that equally hold in the low-dimensional embedding 
  space. 
  In other words, the same weights that serve to reconstruct $\x_i$ in $\RD$ 
  should do so in $\Rd$ \citep{roweissaul2000}.
  Obviously, this belief is only justified if $\mani$ is indeed locally linear 
  and the graph edges run along the manifold surface rather than 
  short-circuiting it, again hinting at the important role of neighborhood size.
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}%
\begin{minipage}[b]{0.35\textwidth}
  \begin{figure}[H]
   \centering
   \begin{subfigure}[b]{\textwidth}
     \centering
     \includegraphics[trim = 40 50 20 45, clip, % left bottom right top
     width = 0.95\textwidth]{figures/reconstruction_3d}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{\textwidth}
     \centering
     \includegraphics[trim = 20 30 0 20, clip, % left bottom right top
     width = 0.75\textwidth]{figures/reconstruction_2d}
   \end{subfigure}
    \caption[Linear reconstruction in LLE]{
    \raggedright
    Reconstruction in three (\textit{top}) and two (\textit{bottom}) 
    dimensions. \textit{Source:} own representation.}
    \label{fig_reconstruction}
  \end{figure}
\end{minipage}

\vspace{0.3cm}

Algorithmically, LLE performs two subsequent steps \citep{roweissaul2000}:
\begin{tight_enumerate}
  \item Compute the reconstruction weights in $\RD$ minimizing reconstruction
  loss.
  \item Compute the embedding coordinates in $\Rd$ minimizing embedding loss.
\end{tight_enumerate}

\textbf{Reconstruction loss minimization.}
Reconstruction errors are measured by a quadratic loss function.
Optimization of the objective is subject to a sum-one constraint for the weights
of each point.
A second constraint, zero weights for non-neighboring points, is implicitly
enforced during construction of the neighborhood graph, where edges are only
drawn to vertices belonging to $\x_i$'s neighborhood \citep{ghojoghetal2020}.
The resulting optimization problem is convex and has a unique
closed-form solution\footnote{
Note that the weight matrix $\W$ is different from the one computed in LEM.
} \citep{roweissaul2000}:

\begin{equation}
  \begin{split}
    \min_{\W} \varepsilon(\W) = & \min_{\W} \sum_i
    \twonorm{\x_i - \sum_j w_{ij} \x_j}
    = \min_{\W} \sum_i \twonorm{\x_i - \bm{N}_i \bm{w}_i}, \\
    & \text{s.t. } \bm{1}^T \bm{w}_i = 1 \quad \forall i \in \setN.
  \end{split}
  \label{eq_obj_lle_recon}
\end{equation}

Here, $\bm{N}_i \in \R^{D \times k}$ denotes the matrix of feature vectors of
$\x_i$'s neighbors and $\bm{w}_i = \sum_j w_{ij} \in \R^k$.

Equation \ref{eq_obj_lle_recon} can be re-arranged by use of the sum-one
constraint and simplified by introduction of the gram, or local covariance,
matrix $\G_i$ \citep{saulroweis2001}:

\begin{equation}
  \begin{split}
    \min_{\W} \varepsilon(\W) &= \min_{\W} \sum_i \twonorm{\x_i \bm{1}^T
    \bm{w}_i - \bm{N}_i \bm{w}_i}
    = \min_{\W} \sum_i \bm{w}_i^T (\x_i \bm{1}^T - \bm{N}_i)^T
    (\x_i \bm{1}^T - \bm{N}_i) \bm{w}_i \\
    &= \min_{\W} \sum_i \bm{w}_i^T \G_i \bm{w}_i, \quad 
    \text{s.t. } \bm{1}^T \bm{w}_i = 1 \quad \forall i \in \setN.
  \end{split}
  \label{eq_obj_lle_recon_2}
\end{equation}

By standard use of a Lagrange multiplier, the solution for the above constrained optimization problem collapses to 
$\bm{w}_i = \frac{\G_i^{-1}\bm{1}}{\bm{1}^T \G_i^{-1}\bm{1}}.$
Solving the reconstruction problem therefore requires $N$ matrix inversions, 
which may prove problematic if the gram matrices do not achieve full rank.
In the case of $D < k$, $\G_i$ is indeed singular and must be robustified by 
adding a small numerical constant to its diagonal \citep{ghojoghetal2020}.
\\

\textbf{Embedding loss minimization.}
The second optimization problem minimizes the embedding cost arising from 
mapping neighborhood geometries into the $d$-dimensional subspace.
Keeping the weight coefficients fixed, the aim is to find the embedding 
coordinates that best preserve the vicinity structures and adhere to the 
constraints of summing to zero (i.e., being centered around the origin) as well 
as having unit covariance \citep{roweissaul2000}:

\begin{equation}
  \begin{split}
    \min_{\Y} \Phi(\Y) = &\min_{\Y} \sum_i \twonorm{\y_i - \sum_j w_{ij} \y_j}, 
    \\
    & \text{s.t. } \frac{1}{N} \sum_i \y_i \y_i^T = \I \quad \text{and}
    \quad \sum_i \y_i = \bm{0} \quad \forall i \in \setN.
    % & \phantom{\text{s.t. }} \sum_i \y_i = \bm{0} \quad \forall i \in \setN. 
  \end{split}
  \label{eq-obj-lle-emb}
\end{equation}

The objective can again be stated as an eigenvalue problem.
For this purpose, define \\ $\E = (\I - \W)^T(\I - \W)$ and set 
$\tilde{\Y} = \Y^T$ \citep{cayton2005}, yielding:

\begin{equation}
  \min_{\tilde{\Y}} \text{\textit{trace}}(\tilde{\Y}^T \E \tilde{\Y}), \quad
  \text{s.t. } \frac{1}{N} \tilde{\Y}^T \tilde{\Y} = \I \quad \text{and}
  \quad \tilde{\Y}^T\bm{1} = \bm{0}.
  \label{eq-obj-lle-emb-2}
\end{equation}

Again, the solution is found by eigenanalysis of the matrix encoding the 
intrinsic manifold structures.
Note that the first constraint carries a factor $1/N$ as originally 
proposed.
In fact, any such quadratic form, provided its right hand side is of full rank, 
would suffice to ensure the embedding vectors actually span a $d$-dimensional 
space \citep{burges2010}.
The additional sum-zero condition is implicitly met by discarding the constant 
eigenvector \citep{ghojoghetal2020}.
As mentioned before, the close resemblance to the optimization problem in LEM 
(equation \ref{eq-obj-lem}) is not coincidental.
\citet{belkinniyogi2003} show that LLE approximates the eigenfunctions of the 
iterated form $\frac{1}{2} \mathcal{L}^2$, which are identical to those of 
$\mathcal{L}$.

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (HLLE)}
\label{hlle}

% ------------------------------------------------------------------------------

\subsection{Semi-Supervised Locally Linear Embedding (SSLLE)}
\label{sslle}

% ------------------------------------------------------------------------------

\subsection{Particular Challenges}
\label{challenges}