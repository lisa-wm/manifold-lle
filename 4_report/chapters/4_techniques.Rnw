\subsection{Unsupervised Techniques}
\label{unsupervised}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps (LEM)}
\label{lem}

The reason for LEM to appear in this report alongside the LLE family is its 
underlying theory both providing a foundation for LLE \citep{belkinniyogi2003} 
and closely relating to the theoretical concepts in HLLE 
\citep{donohogrimes2003}.
LEM is centered around the preservation of locality, i.e., mapping nearby
inputs to nearby outputs.
Locality is enforced via the \textit{Laplace-Beltrami operator} defined on
smooth, compact manifolds, and operationalized by means of the 
\textit{graph Laplacian} acting as a discrete approximator 
\citep{belkinniyogi2003}.
This idea is best understood recalling that the similarity of outputs for
similar inputs is essentially a notion of smoothness and can thus be controlled
by a size constraint on the gradient of the mapping function.
% \\

\begin{minipage}[b]{0.7\textwidth}
  \textbf{Continuous justification.}
  Consider the twice differentiable function $f: \mani \rightarrow \R$ mapping
  $\pv, \qv$ to $f(\pv)$ and $f(\qv)$, respectively.
  On $\mani$ these points are connected by a length-parametrized curve $c(t)$.
  Denote the geodesic distance between $\pv$ and $\qv$ by $\ell$, such that
  $\pv = c(0)$ and $\qv = c(\ell)$.
  Gradients of $f$ with respect to $\pv$ are defined in the local tangent space
  $T_{\pv}(\mani)$.
  Local tangent spaces of $\mani$ are $d$-dimensional hyperplanes 
  \citep{sudderth2002}, as shown exemplarily by figure \ref{fig_sphere_tangent}.
  If $\pv$ is identified with the origin of $T_{\pv}(\mani)$, the tangent space
  inherits an orthonormal coordinate system from endowing
  $T_{\pv}(\mani)$ with the inner product of $\Rd$ \citep{donohogrimes2003}.
  With this, the distance $|f(\pv) - f(\qv)|$ of mappings can be expressed as
  the length of
  $\int_0^{\ell} \langle \nabla f(c(t)), c^{\prime}(t) \rangle dt$.
  In other words, the geodesic connecting $\pv$ and $\qv$ is projected
  onto $T_{\pv}(\mani)$, and the length of this projection depends on the
  gradient of $f$ and the curve velocity.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.25\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 70 60 30, clip, % left bottom right top
      width = 0.8\textwidth]{figures/sphere_tangent}
    \caption[Tangent hyperplane for two-dimensional unit sphere]{
    \raggedright
    Tangent hyperplane for a point on the two-dimensional unit sphere. 
    \textit{Source:} own representation.}
    \label{fig_sphere_tangent}
  \end{figure}
\end{minipage}

\vspace{0.5cm}

It can be shown that
$|f(\pv) - f(\qv)| \leq \| \nabla f(\pv) \| \cdot \| \pv - \qv\| + o$, where
$o$ marks a term of vanishing size \citep{belkinniyogi2008}.
$\| \nabla f \|$ thus controls how far apart points are mapped on the real line.
Consequently, the goal is to find a mapping that, on average, preserves
locality by minimizing $\int_{\mani}\| \nabla f \|^2$.
This is just equivalent to minimizing $\int_{\mani} \mathcal{L}(f)f$ with the
Laplace-Beltrami operator $\mathcal{L}$ \citep{belkinniyogi2003}.
For $\mathcal{L} f = \lambda f$, $f$ is an eigenfunction of $\mathcal{L}$ with 
$\lambda \in \R$ as its associated eigenvalue.
These eigenfunctions are orthogonal and have real eigenvalues, making them 
natural candidates for a functional basis \citep{levy2006}.
The optimal embedding map is then given by the $d$ principal eigenfunctions of
$\mathcal{L}$ after removing the bottom one which would map $\mani$ to a
single point \citep{belkinniyogi2003}.
\\

\textbf{Finite approximation.}
Now the same reasoning can be applied to the neighborhood graph approximation of
$\mani$.
Mapping nearby inputs to nearby is achieved by assigning edge weights\footnote{
These weights stem from the heat kernel intimately related to the 
Laplace-Beltrami operator and ensure positive semi-definiteness of the resulting
graph Laplacian. 
As an alternative, \citet{belkinniyogi2003} propose a simpler kernel that is 
equal to 1 for connected nodes and 0 otherwise.
} $w_{ij} = \exp(\frac{1}{t} \twonorm{\x_i - \x_j}), t \in \R$, if 
$\x_i, \x_j$ are are connected, and zero otherwise.
Clearly, edges between closer points receive larger weights.
The \textit{adjacency matrix} $\D = (d)_{ij} \in \R^{N \times N}$ takes the row
sums of the \textit{weight matrix} $\W = (w)_{ij} \in \R^{N \times N}$ on its 
diagonals.
Penalizing output disparities more severely for pairs of nearby points, i.e.,
pairs with a large weight coefficient, the smoothness requirement may be stated
as follows:

\begin{equation*}
  \begin{split}
    \min_{\Y} \sum_{i, j} \| \y_i - \y_j \|^2 w_{ij}
    &= \min_{\Y} \sum_{i, j} \y_i^T \y_i w_{ij} + \y_j^T \y_j
    w_{ij} -
    2 \y_i^T \y_j w_{ij} \\
    &= \min_{\Y} \sum_i \y_i^T \y_i d_{ii} + \sum_j \y_j^T \y_j
    d_{jj} - 2 \sum_{i, j} \y_i^T \y_j w_{ij}.
  \end{split}
\end{equation*}

Now, define the \textit{graph Laplacian} as
$\Lap = \D - \W \in \R^{N \times N}$, thereby coercing
all information about the graph structure into a single matrix
representation.
With $\Lap$ the above can be rewritten as generalized eigenvalue problem, 
adhering to the LGML algorithmic concept:

\begin{equation}
  \min_{\Y} \text{\textit{trace}}(\Y^T \Lap \Y), \quad \text{s.t. } 
  \Y^T \D \Y = \I,
  \label{eq-obj-lem}
\end{equation}

which is solved by eigendecomposition of $\Lap$ \citep{belkinniyogi2003}.
Analogous to the continuous case, the bottom eigenvector with zero eigenvalue is
constant and must be discarded\footnote{
As a consequence of its definition, $\Lap$ always has at least one eigenpair 
consisting of a zero eigenvalue and a constant eigenvector.
In fact, the multiplicity of the zero eigenvalue corresponds to the number of 
connected graph components \citep{marsden2013}.
}.
The subsequent $d$ eigenvectors hold the desired low-dimensional
embedding coordinates \citep{levy2006}.

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (HLLE)}
\label{hlle}

% ------------------------------------------------------------------------------

\subsection{Semi-Supervised Locally Linear Embedding (SSLLE)}
\label{sslle}

% ------------------------------------------------------------------------------

\subsection{Particular Challenges}
\label{challenges}