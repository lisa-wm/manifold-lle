\subsection{Unsupervised Techniques}
\label{unsupervised}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps (LEM)}
\label{lem}

The reason for LEM to appear in this report alongside the LLE family is its 
underlying theory both providing a foundation for LLE \citep{belkinniyogi2003} 
and closely relating to the theoretical concepts in HLLE 
\citep{donohogrimes2003}.
LEM is centered around the preservation of locality, i.e., mapping nearby
inputs to nearby outputs.
Locality is enforced via the \textit{Laplace-Beltrami operator} defined on
smooth, compact manifolds, and operationalized by means of the 
\textit{graph Laplacian} acting as a discrete approximator 
\citep{belkinniyogi2003}.
This idea is best understood recalling that the similarity of outputs for
similar inputs is essentially a notion of smoothness and can thus be controlled
by a size constraint on the gradient of the mapping function.
\\

\begin{minipage}[b]{0.7\textwidth}
  \textbf{Continuous justification.}
  Consider the twice differentiable function $f: \mani \rightarrow \R$ mapping
  $\pv, \qv$ to $f(\pv)$ and $f(\qv)$, respectively.
  On $\mani$ these points are connected by a length-parametrized curve $c(t)$.
  Denote the geodesic distance between $\pv$ and $\qv$ by $\ell$, such that
  $\pv = c(0)$ and $\qv = c(\ell)$.
  Gradients of $f$ with respect to $\pv$ are defined in the local tangent space
  $T_{\pv}(\mani)$.
  Local tangent spaces of $\mani$ are $d$-dimensional hyperplanes 
  \citep{sudderth2002}, as shown exemplarily by figure \ref{fig_sphere_tangent}.
  If $\pv$ is identified with the origin of $T_{\pv}(\mani)$, the tangent space
  inherits an orthonormal coordinate system from endowing
  $T_{\pv}(\mani)$ with the inner product of $\Rd$ \citep{donohogrimes2003}.
  With this, the distance $|f(\pv) - f(\qv)|$ of mappings can be expressed as
  the length of
  $\int_0^{\ell} \langle \nabla f(c(t)), c^{\prime}(t) \rangle dt$.
  In other words, the geodesic connecting $\pv$ and $\qv$ is projected
  onto $T_{\pv}(\mani)$, and the length of this projection depends on the
  gradient of $f$ and the curve velocity.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.25\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 70 60 30, clip, % left bottom right top
      width = 0.8\textwidth]{figures/sphere_tangent}
    \caption[Tangent hyperplane for two-dimensional unit sphere]{
    \raggedright
    Tangent hyperplane for a point on the two-dimensional unit sphere. 
    \textit{Source:} own representation.}
    \label{fig_sphere_tangent}
  \end{figure}
\end{minipage}

\vspace{0.5cm}

It can be shown that
$|f(\pv) - f(\qv)| \leq \| \nabla f(\pv) \| \cdot \| \pv - \qv\| + o$, where
$o$ marks a term of vanishing size \citep{belkinniyogi2008}.
$\| \nabla f \|$ thus controls how far apart points are mapped on the real line.
Consequently, the goal is to find a mapping that, on average, preserves
locality by minimizing $\int_{\mani}\| \nabla f \|^2$.
This is just equivalent to minimizing $\int_{\mani} \mathcal{L}(f)f$ with the
Laplace-Beltrami operator $\mathcal{L}$ \citep{belkinniyogi2003}.
For $\mathcal{L} f = \lambda f$, $f$ is an eigenfunction of $\mathcal{L}$ with 
$\lambda \in \R$ as its associated eigenvalue.
These eigenfunctions are orthogonal and have real eigenvalues, making them 
natural candidates for a functional basis \citep{levy2006}.
The optimal embedding map is then given by the $d$ principal eigenfunctions of
$\mathcal{L}$ after removing the bottom one which would map $\mani$ to a
single point \citep{belkinniyogi2003}.
% \\

\newpage
\textbf{Finite approximation.}
Now the same reasoning can be applied to the neighborhood graph approximation of
$\mani$.
Mapping nearby inputs to nearby is achieved by assigning edge weights\footnote{
These weights stem from the heat kernel intimately related to the 
Laplace-Beltrami operator and ensure positive semi-definiteness of the resulting
graph Laplacian \citep{belkinniyogi2003}. 
% As an alternative, \citet{belkinniyogi2003} propose a simpler kernel that is 
% equal to 1 for connected nodes and 0 otherwise.
} $w_{ij} = \exp(\frac{1}{t} \twonorm{\x_i - \x_j}), t \in \R$, if 
$\x_i, \x_j$ are are connected, and zero otherwise.
Clearly, edges between closer points receive larger weights.
The \textit{adjacency matrix} $\D = (d)_{ij} \in \R^{N \times N}$ takes the row
sums of the \textit{weight matrix} $\W = (w)_{ij} \in \R^{N \times N}$ on its 
diagonal.
Penalizing output disparities more severely for pairs of nearby points, the 
smoothness requirement may be stated as follows:

\begin{equation*}
  \begin{split}
    \min_{\Y} \sum_{i, j} \| \y_i - \y_j \|^2 w_{ij}
    &= \min_{\Y} \sum_{i, j} \y_i^T \y_i w_{ij} + \y_j^T \y_j
    w_{ij} -
    2 \y_i^T \y_j w_{ij} \\
    &= \min_{\Y} \sum_i \y_i^T \y_i d_{ii} + \sum_j \y_j^T \y_j
    d_{jj} - 2 \sum_{i, j} \y_i^T \y_j w_{ij}.
  \end{split}
\end{equation*}

Now, define the \textit{graph Laplacian} as
$\Lap = \D - \W \in \R^{N \times N}$, thereby coercing
all information about the graph structure into a single matrix
representation.
With $\Lap$ the above can be rewritten as generalized eigenvalue problem, 
adhering to the LGML algorithmic concept:

\begin{equation}
  \min_{\Y} \text{\textit{trace}}(\Y^T \Lap \Y), \quad \text{s.t. } 
  \Y^T \D \Y = \I,
  \label{eq-obj-lem}
\end{equation}

which is solved by eigendecomposition of $\Lap$ \citep{belkinniyogi2003}.
Analogous to the continuous case, the bottom eigenvector with zero eigenvalue is
constant and must be discarded\footnote{
As a consequence of its definition, $\Lap$ always has at least one eigenpair 
consisting of a zero eigenvalue and a constant eigenvector.
In fact, the multiplicity of the zero eigenvalue corresponds to the number of 
connected graph components \citep{marsden2013}.
}.
The subsequent $d$ eigenvectors hold the desired low-dimensional
embedding coordinates \citep{levy2006}.

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

In proposing LEM, \citet{belkinniyogi2003} also demonstrated how the somewhat 
earlier LLE algorithm may be reinterpreted within the LEM framework: it can be 
shown to approximate the graph Laplacian under certain conditions and thus 
asymptotically approach the Laplace-Beltrami operator.
More recent research, however, suggests that these conditions might be more 
restrictive than previously assumed. 
In particular, convergence appears to depend on the choice of a regularization 
parameter required in the case of $D < k$ \citep{wuwu2018}.
\\

\begin{minipage}[b]{0.6\textwidth}
  \textbf{Idea.}
  The initial proposal by \citet{roweissaul2000}, ignorant to these findings, 
  was made with a different, and rather heuristically motivated, intuition.
  LLE relies on a simple yet powerful idea.
  Each point $\x_i$ in the $D$-dimensional input space is expressed as a convex 
  combination of its neighbors, such that the weighting coefficients of this 
  reconstruction essentially represent the edge weights of the neighborhood 
  graph around $\x_i$. 
  These (generalized) barycentric coordinates now bear a crucial property: they 
  are invariant to rotation, rescaling and translation of the neighborhood, and 
  thus topological properties that equally hold in the low-dimensional embedding 
  space. 
  In other words, the same weights that serve to reconstruct $\x_i$ in $\RD$ 
  should do so in $\Rd$ \citep{roweissaul2000}.
  Obviously, this belief is only justified if $\mani$ is indeed locally linear 
  and the graph edges run along the manifold surface rather than 
  short-circuiting it, again hinting at the important role of neighborhood size.
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}%
\begin{minipage}[b]{0.35\textwidth}
  \begin{figure}[H]
   \centering
   \begin{subfigure}[b]{\textwidth}
     \centering
     \includegraphics[trim = 40 50 20 45, clip, % left bottom right top
     width = 0.95\textwidth]{figures/reconstruction_3d}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{\textwidth}
     \centering
     \includegraphics[trim = 20 30 0 20, clip, % left bottom right top
     width = 0.75\textwidth]{figures/reconstruction_2d}
   \end{subfigure}
    \caption[Linear reconstruction in LLE]{
    \raggedright
    Reconstruction in three (\textit{top}) and two (\textit{bottom}) 
    dimensions. \textit{Source:} own representation.}
    \label{fig_reconstruction}
  \end{figure}
\end{minipage}

\vspace{0.3cm}

Algorithmically, LLE performs two subsequent steps \citep{roweissaul2000}:
\begin{tight_enumerate}
  \item Compute the reconstruction weights in $\RD$ minimizing reconstruction
  loss.
  \item Compute the embedding coordinates in $\Rd$ minimizing embedding loss.
\end{tight_enumerate}

\textbf{Reconstruction loss minimization.}
Reconstruction errors are measured by a quadratic loss function.
Optimization of the objective is subject to a sum-one constraint for the weights
of each point.
A second constraint, zero weights for non-neighboring points, is implicitly
enforced during construction of the neighborhood graph, where edges are only
drawn to vertices belonging to $\x_i$'s neighborhood \citep{ghojoghetal2020}.
The resulting optimization problem is convex and has a unique
closed-form solution\footnote{
Note that the weight matrix $\W$ is different from the one computed in LEM.
} \citep{roweissaul2000}:

\begin{equation}
  \begin{split}
    \min_{\W} \varepsilon(\W) = & \min_{\W} \sum_i
    \twonorm{\x_i - \sum_j w_{ij} \x_j}
    = \min_{\W} \sum_i \twonorm{\x_i - \bm{N}_i \bm{w}_i}, \\
    & \text{s.t. } \bm{1}^T \bm{w}_i = 1 \quad \forall i \in \setN.
  \end{split}
  \label{eq_obj_lle_recon}
\end{equation}

Here, $\bm{N}_i \in \R^{D \times k}$ denotes the matrix of feature vectors of
$\x_i$'s neighbors and $\bm{w}_i = \sum_j w_{ij} \in \R^k$.

Equation \ref{eq_obj_lle_recon} can be re-arranged by use of the sum-one
constraint and simplified by introduction of the Gram, or local covariance,
matrix $\G_i$ \citep{saulroweis2001}:

\begin{equation}
  \begin{split}
    \min_{\W} \varepsilon(\W) &= \min_{\W} \sum_i \twonorm{\x_i \bm{1}^T
    \bm{w}_i - \bm{N}_i \bm{w}_i}
    = \min_{\W} \sum_i \bm{w}_i^T (\x_i \bm{1}^T - \bm{N}_i)^T
    (\x_i \bm{1}^T - \bm{N}_i) \bm{w}_i \\
    &= \min_{\W} \sum_i \bm{w}_i^T \G_i \bm{w}_i, \quad 
    \text{s.t. } \bm{1}^T \bm{w}_i = 1 \quad \forall i \in \setN.
  \end{split}
  \label{eq_obj_lle_recon_2}
\end{equation}

By standard use of a Lagrange multiplier, the solution for the above constrained optimization problem collapses to 
$\bm{w}_i = \frac{\G_i^{-1}\bm{1}}{\bm{1}^T \G_i^{-1}\bm{1}}.$
Solving the reconstruction problem therefore requires $N$ matrix inversions, 
which may prove problematic if the gram matrices do not achieve full rank.
In the case of $D < k$, $\G_i$ is indeed singular and must be robustified by 
adding a small numerical constant to its diagonal \citep{ghojoghetal2020}.
\\

\textbf{Embedding loss minimization.}
The second optimization problem minimizes the embedding cost arising from 
mapping neighborhood geometries into the $d$-dimensional subspace.
Keeping the weight coefficients fixed, the aim is to find the embedding 
coordinates that best preserve the vicinity structures and adhere to the 
constraints of summing to zero (i.e., being centered around the origin) as well 
as having unit covariance \citep{roweissaul2000}:

\begin{equation}
  \begin{split}
    \min_{\Y} \Phi(\Y) = &\min_{\Y} \sum_i \twonorm{\y_i - \sum_j w_{ij} \y_j}, 
    \\
    & \text{s.t. } \frac{1}{N} \sum_i \y_i \y_i^T = \I \quad \text{and}
    \quad \sum_i \y_i = \bm{0} \quad \forall i \in \setN.
    % & \phantom{\text{s.t. }} \sum_i \y_i = \bm{0} \quad \forall i \in \setN. 
  \end{split}
  \label{eq_obj_lle_emb}
\end{equation}

The objective can again be stated as an eigenvalue problem.
For this purpose, define \\ $\E = (\I - \W)^T(\I - \W)$ and set 
$\tilde{\Y} = \Y^T$ \citep{cayton2005}, yielding:

\begin{equation}
  \min_{\tilde{\Y}} \text{\textit{trace}}(\tilde{\Y}^T \E \tilde{\Y}), \quad
  \text{s.t. } \frac{1}{N} \tilde{\Y}^T \tilde{\Y} = \I \quad \text{and}
  \quad \tilde{\Y}^T\bm{1} = \bm{0}.
  \label{eq_obj_lle_emb_2}
\end{equation}

Again, the solution is found by eigenanalysis.
Note that the first constraint carries a factor $1/N$ as originally 
proposed.
In fact, any such quadratic form, provided its right hand side is of full rank, 
would suffice to ensure the embedding vectors actually span a $d$-dimensional 
space \citep{burges2010}.
The additional sum-zero condition is implicitly met by discarding the constant 
eigenvector \citep{ghojoghetal2020}.
As mentioned before, the close resemblance to the optimization problem in LEM 
(equation \ref{eq-obj-lem}) is not coincidental.
\citet{belkinniyogi2003} show that LLE approximates the eigenfunctions of the 
iterated form $\frac{1}{2} \mathcal{L}^2$, which are identical to those of 
$\mathcal{L}$.

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (HLLE)}
\label{hlle}

Lastly, HLLE \citep{donohogrimes2003} pursues an approach toward LGML that 
straddles the two former techniques: it borrows heavily from the idea behind 
LEM but is more akin to LLE in an algorithmic sense\footnote{
HLLE is also closely related to another technique beyond the scope of this 
report, namely \textit{local tangent space alignment (LTSA)} (see, for example, 
\citet{tingjordan2018}).
} \citep{dissross2008}.
As opposed to LLE, HLLE is built upon a rigid theoretical foundation.
It makes the relatively weak assumptions of local isometry and homeomorphicity 
to an open, connected subset of $\Rd$, providing veritable convergence 
guarantees, albeit only for the continuous limit \citep{donohogrimes2003}.
\\

\textbf{Idea.}
HLLE considers the same twice-differentiable mapping functions 
$f: \mani \rightarrow \R$ as LEM.
Recall that LEM defines the gradient of $f$ with respect to local tangent 
spaces $T_{\pv}(\mani)$ at $\pv \in \mani$ as a notion of smoothness.
Similarly, HLLE computes the Hessian to measure curviness of $f$ 
\citep{donohogrimes2003}.
One advantage of this modification is that, while the Laplacian equals zero 
for any harmonic\footnote{
An example is indeed given by the coordinate functions; however, other functions 
that are clearly non-linear have the harmonic property (see, for example,
\citet{axleretal2001}).
} function on $\mani$, the Hessian vanishes if and only if $f$ is 
linear \citep{dissross2008}.
\\

\textbf{Continuous justification.}
Consider $\pv \in \mani$ and its $k$-neighborhood $\mathcal{N}_k(\pv)$, each of 
whose members has a unique closest point on $T_{\pv}(\mani)$ via the smooth 
mapping $f$.
Identifying $f(\pv)$ with $\bm{0} \in \Rd$ yields a system of local 
coordinates on $T_{\pv}(\mani)$ that depends on this particular choice of the 
origin.
For a neighbor $\pv^{\prime}$ of $\pv$, let these local coordinates be denoted 
by $\x^{\text{loc, }\pv^{\prime}}$.
Then, the Hessian $\Hes_f^{\text{loc}}(\pv)$ of $f$ at $\pv$ in tangent 
coordinates may be expressed as the ordinary Hessian of a function 
$g: U \rightarrow \R$ with $f(\pv^{\prime}) = g(\x^{\text{loc, }\pv^{\prime}})$, 
$U$ being a 
neighborhood of zero in $\Rd$ \citep{donohogrimes2003}:

\begin{equation}
  (\Hes_f^{\text{loc}}(\pv))_{i,j} = \frac{\partial^2 
  g(\x^{\text{loc, }\pv^{\prime}})}
  {\partial x_i^{\text{loc, }\pv^{\prime}} 
  \partial x_j^{\text{loc, }\pv^{\prime}}} 
  \bigg\rvert_{\x^{\text{loc, }\pv^{\prime}} = 0}.
  \label{eq-hessian-local}
\end{equation}

From these point-wise tangent Hessians it is now possible to construct a 
quadratic functional $\mathscr{H}(f)$ on the entire manifold, analogous to the 
Laplace-Beltrami operator in LEM.
The crucial property of $\mathscr{H}(f)$ is given by the fact that, if $\mani$ 
is truly locally homeomorphic to an open, connected subset of $\Rd$, 
$\mathscr{H}(f)$ has a $(d + 1)$-dimensional null space of linear functions.
After discarding the bottom constant function corresponding to a zero 
eigenvalue, the subsequent $d$ eigenfunctions span the desired 
low-dimensional embedding space \citep{donohogrimes2003}.
First, however, the dependency on the respective local coordinate systems must 
be removed by taking the Frobenius norm of the tangent Hessians\footnote{
For any alternative coordinate system, $\Hes^{\prime}$ as obtained by orthogonal 
transformation of $\Hes$ with a suitable matrix $\bm{B}$, it must hold that 
$\frobnorm{\Hes^{\prime}} = \frobnorm{ \bm{B} \Hes \bm{B}^T} = 
\text{\textit{trace}} (\bm{B} \Hes^T \bm{B}^T \bm{B} \Hes \bm{B}^T) = 
\text{\textit{trace}} (\Hes^T \Hes) = \frobnorm{\Hes}$, due to the permutation 
invariance of the trace operator \citep{dissross2008}.
}.
Then, $\mathscr{H}(f)$ as a measure for overall curviness of the mapping is 
given by \citep{donohogrimes2003}:

\begin{equation}
  \mathscr{H}(f) = \int_{\mani} \frobnorm{\Hes_f^{\text{loc}}(\pv)} d\pv.
  \label{eq-hfunctional}
\end{equation}

\textbf{Finite approximation.}
In analogy to LEM, the functional defined on $\mani$ is approximated in an 
empirical manner; yet, the computations are somewhat more involved.
LEM incorporates neighborhood information during weight computation.
LLE and HLLE take a more explicit look at locally linear patches on the manifold 
surface and attempt to map these to the low-dimensional space 
\citep{cayton2005}.
As before, the first step is neighborhood construction.
Let $\bm{N}_i \in \R^{D \times k}$ again denote the matrix of feature vectors of
$\x_i$'s neighbors, this time centered with respect to the mean over all 
members.
From these neighborhood matrices the local tangent coordinates are 
estimated by means of $N$ singular value decompositions 
$\bm{N}_i = \bm{U_i}\bm{D_i}\bm{V_i}^T$ \citep{dissross2008}.
In effect, this amounts to finding the basis of $T_{\x_i}(\mani)$ by performing 
PCA on the local covariance matrix at $\x_i$ and retaining the $d$ principal 
eigenvectors.
Now a matrix $\bm{Z_i}$, whose columns contain all cross products of $\bm{U}_i$ 
up to order $d$, is constructed and coerced into orthonormal form.
Extracting the transpose of the last $\frac{d(d + 1)}{2}$ columns of $\bm{Z_i}$ 
yields the local Hessian approximator $\Hes_i$ as the least-squares estimate of 
a local quadratic polynomial regression in the neighborhood of $\x_i$ (\citet{vandermaatenetal2009}, \citet{tingjordan2018}).
The empirical Hessian functional $\mathcal{H}$ is obtained as a 
quadratic form of the local Hessian approximators \citep{donohogrimes2003}:

\begin{equation}
  \mathcal{H} = \sum_i \sum_j \mathcal{H}_{ij}, \quad 
  \mathcal{H}_{ij} = \sum_{\ell} \sum_m (\Hes_{\ell})_{m,i}
  (\Hes_{\ell})_{m,j}.
  \label{eq_hfunctional_emp}
\end{equation}

Eventually, eigenanalysis of $\mathcal{H}$ yields the approximate null space 
spanned by the $d$ bottom eigenvectors after discarding the constant one.
The final step consists of finding a basis for the null space.
For this, take $\bm{Q} \in \R^{N \times d}$ containing the $d$
non-constant eigenvectors and find a second matrix $\bm{R}$ such that the
columns of $\bm{Q}\bm{R}$ restricted to a fixed local neighborhood are
orthonormal.
The embedding coordinates are then given by $\bm{Q}^T\bm{R}^T$
\citep{yezhi2015}.
As an alternative, \citet{dissross2008} proposes to replace the last step
by simply scaling $\bm{Q}$ with $\sqrt{N}$.
\\

Tracing the steps only roughly sketched above, it becomes clear that the 
theoretical guarantees of HLLE come at the expense of rather complex 
computations\footnote{
For a more in-depth analysis see, for example, \citet{tingjordan2018}.
}.
At the same time, its implementation employs numerous approximations 
calling the merit of theoretical convergence into question.
It is perhaps this approximate yet computationally challenging design, along 
with the fact that the other methods are more easily accessible by intuition, 
that has acted as a limiting factor on the practical application of HLLE 
(\citet{cayton2005}, \citet{yezhi2015}).

% ------------------------------------------------------------------------------

\subsection{Semi-Supervised Locally Linear Embedding (SSLLE)}
\label{sslle}

\begin{minipage}[b]{0.6\textwidth}
  \textbf{Idea.}
  All of the above methods operate in an unsupervised manner, relying 
  solely on the $D$-dimensional coordinates of the observation space.
  The endeavor of dimensionality reduction will thus sometimes fail to produce 
  a meaningful embedding.
  \citet{yangetal2006} propose to anchor the low-dimensional representation in 
  LLE at a number $m \in \N$ of prior points whose coordinates in $\Rd$ are 
  already known.
  Figure \ref{fig_scurve_undone_pp} hints at how such prior knowledge, 
  illustrated by points in black, might help to improve the embedding.
  What \citet{yangetal2006} dub semi-supervised LLE is actually somewhat 
  different from the idea typically employed in semi-supervised learning.
  Rather than supporting a supervised learning task by information extracted 
  from the pool of unlabeled data, an inherently unsupervised problem is 
  alleviated by specifying part of the solution upfront.
  % Arguably, SSLLE might therefore be viewed in the context of active 
  % learning, where the model is allowed to query labels in a sequential 
  % manner that seeks to maximize utility at minimal expense \footnote{
  % For an extensive overview on active learning see, for example, 
  % \citet{settles2009}.
  % }.
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}%
\begin{minipage}[b]{0.35\textwidth}
  \begin{figure}[H]
   \centering
   \begin{subfigure}[b]{0.5\textwidth}
     \centering
     \includegraphics[trim = 80 20 70 30, clip, % left bottom right top
     width = \textwidth]{figures/s_curve_pp_random}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.4\textwidth}
     \centering
     \includegraphics[trim = 0 20 0 0, clip, % left bottom right top
     width = \textwidth]{figures/s_curve_undone_pp_random_long}
   \end{subfigure}
    \caption[S-curve with prior points]{
    \raggedright
    S-curve with 12 randomly sampled prior points.
    \textit{Left:} prior point locations in three-dimensional observation space.
    \textit{Right:} locations in true $\R^2$ embedding.
    \textit{Source:} own representation.}
    \label{fig_scurve_undone_pp}
  \end{figure}
\end{minipage}

\vspace{0.3cm}

\begin{minipage}[b]{0.6\textwidth}
  \textbf{Prior point location.}
  Obviously, semi-supervision commands the availability of prior 
  information.
  The practical experiments in chapter \ref{experiment} will assume a
  setting where prior information can be inquired from the pool of initially
  unlabeled observations.
  A straightforward approach would be to select the prior points in a way that is 
  most informative to the learning algorithm.
  As this is, of course, not always possible, the sensitivity analysis will 
  examine the impact of such optimal selection versus scenarios of poor landmark 
  choice and sampling uniformly at random:
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}%
\begin{minipage}[b]{0.35\textwidth}
  \begin{figure}[H]
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[trim = 80 20 60 0, clip, % left bottom right top
        width = \textwidth]{figures/s_curve_pp_poor}
    \end{subfigure}%
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[trim = 80 20 60 0, clip, % left bottom right top
        width = \textwidth]{figures/s_curve_pp_random}
  \end{subfigure}%
    \begin{subfigure}[b]{0.32\textwidth}
      \includegraphics[trim = 80 20 60 0, clip, % left bottom right top
        width = \textwidth]{figures/s_curve_pp_maxmin}
    \end{subfigure}
    \caption[S-curve with poor, random and optimal landmark coverage]{
      \raggedright
      S-curve with poor (\textit{left}), random (\textit{middle}), and optimal 
      (\textit{right}) coverage by 12 prior points.
      \textit{Source:} own representation.}
      \label{fig_scurve_coverage}
  \end{figure}
\end{minipage}

\vspace{0.3cm}

\textbf{Types of prior information.}
Depending on the application, the prior information may be exact or inexact.
While the first case greatly simplifies the manifold learning problem, the 
latter must be treated with some more care, or the semi-supervision will 
actually be harmful.
Besides their location, the reliability of prior points therefore seems crucial 
to the embedding, which is why the second part of the sensitivity analysis in 
chapter \ref{experiment} will assess the robustness of SSLLE to varying levels 
of label noise.
\\

\textbf{Algorithmic impact.}
Prior knowledge enters only in the second phase of the LLE algorithm.
First, the reconstruction weights are computed as usual (see equation 
\ref{eq_obj_lle_recon}).
The eigenvalue problem of minimizing embedding cost, by contrast, has a 
different nature: the matrices $\E$ and $\tilde{\Y}$ are partitioned into 
parts corresponding to known and unknown points, respectively\footnote{
Without loss of generality, the prior points are assumed to be the first $m$ 
ones.
}.
This leads to a system of linear equations whose solution depends on the purity 
of prior information.
If it is exact, the minimization problem collapses to optimizing over the 
unknown coordinates, denoted by $\Ytil_2 \in \R^{m \times d}$ 
\citep{yangetal2006}:

\begin{equation}
  \min_{\Ytil_2}
  \begin{bmatrix} \Ytil_1 & \Ytil_2 \end{bmatrix}
  \begin{bmatrix} \E_{11} & \E_{12} \\ \E_{21} & \E_{22} \end{bmatrix}
  \begin{bmatrix} \Ytil_1^T \\ \Ytil_2^T \end{bmatrix} \quad
  \Leftrightarrow \quad \Ytil_2^T = \E_{22}^{-1} \E_{12} \Ytil_1^T.
  \label{eq_sslle_exact}
\end{equation}

Equation \ref{eq_sslle_exact} reflects the importance of prior points noted 
before: as errors exist only in $\E_{12} = \E_{21}$ and $\E_{22}$, the condition 
number $\kappa(\E_{22})$ of $\E_{22}$ multiplied by the relative errors in the 
off-diagonal blocks acts as an upper bound on the relative embedding error.
For a sufficiently large number of observations, it can be shown that 
$\kappa(\E_{22})$ is minimal if the prior points are maximally scattered 
across the manifold surface, which is intuitively plausible 
\citep{yangetal2006}.
\\

For inexact prior information, the problem is slightly more complicated.
The label noise commands the introduction of a regularizing term that penalizes 
deviations of the assumed coordinates (denoted by the hat symbol) from the true 
ones.
The associated regularization parameter $\beta > 0$ encodes the level of 
confidence in the prior points \citep{yangetal2006}:

\begin{equation}
  \begin{split}
    \min_{\Ytil_1, \Ytil_2}
    \begin{bmatrix} \Ytil_1 & \Ytil_2 \end{bmatrix}
    &\begin{bmatrix} \E_{11} & \E_{12} \\ \E_{21} & \E_{22} \end{bmatrix}
    \begin{bmatrix} \Ytil_1^T \\ \Ytil_2^T \end{bmatrix} +
    \beta \frobnorm{\Ytil_1^T - \hat{\Ytil}_1^T} \quad \Leftrightarrow \\
    &\begin{bmatrix} \E_{11} + \beta \I & \E_{12} \\ 
    \E_{21} & \E_{22} \end{bmatrix}
    \begin{bmatrix} \Ytil_1^T \\ \Ytil_2^T \end{bmatrix} =
    \begin{bmatrix} \hat{\Ytil}_1^T \\ \bm{0} \end{bmatrix}.
  \end{split}
  \label{eq_sslle_inexact}
\end{equation}

Clearly, for the exact case, $\hat{\Ytil}_1^T = \Ytil_1^T$ and equation 
\ref{eq_sslle_inexact} reduces to equation \ref{eq_sslle_exact}.

% ------------------------------------------------------------------------------

\subsection{Particular Challenges}
\label{challenges}

% Before subjecting SSLLE to varying scenarios of critical parameters in the 
% sensitivity analysis, this chapter concludes with a summary of the main 
% challenges in practical application.
% \\

\textbf{Amount of and quality of prior knowledge.}
As hinted above, the location of prior points must be assumed to have vital 
impact on the quality of embedding coordinates, and so should their number.
Likewise, the reliability of prior information ought to be a critical factor: 
while the case of exact knowledge is purely beneficial, inexact knowledge will 
provide less reliable guidance and might even prove harmful.
The confidence parameter $\beta$ might also be expected to determine embedding 
quality.
However, it emerges in the practical implementation that results are 
surprisingly stable for varying levels of $\beta$, which is why the effort has 
instead been dedicated to address the amount and quality of prior knowledge.
\\

\textbf{Choice of intrinsic dimensionality.} 
Until now, it has been assumed that the intrinsic dimension $d$ of the data is 
a known parameter.
This is obviously not always the case.
Some methods offer the advantage of estimating $d$ in a built-in fashion. 
PCA, for instance, typically shows an indicative gap in its eigenvalue spectrum 
\citep{sauletal2006}.
For the methods covered here, no such tell-tale gap exists\footnote{
While \citet{shasaul2005} have drawn a mathematical relation between the 
eigenspectra in LLE and LEM and intrinsic dimensionality, they 
immediately discarded this finding for practical applications due to large 
computational overhead and lack of reliability in finite-sample situations.
There have been various other proposals to tackle the problem of dimensionality 
estimation (for an extensive discussion, see for example 
\citet{disswissel2017}).
}.
However, as the focus of this report lies on a semi-supervised method of 
manifold learning, it is mainly concerned with situations where prior knowledge 
of coordinates, and of $d$ in particular, is actually available.
\\

\textbf{Choice of neighborhood size.} 
Choosing the size of neighborhoods for the graph approximation, on the other 
hand, does pose a challenge.
It is a standard hyperparameter optimization problem in which a trade-off 
between locality and overall approximation must be balanced.
If neighborhoods are too small, the model will not be able to learn the global 
manifold structure; with overly large neighborhoods, it will forgo all 
advantages of locality and non-linearity \citep{deridderduin2002}.
A tuning step for $k$ has been directly integrated in the SSLLE implementation.
For a detailed description, please refer to section \ref{choice_k}.
\\

\textbf{Robustness of eigendecomposition.}
Lastly, the inversion of the Gram matrix required for reconstruction error 
minimization in (SS)LLE frequently suffers from singularity.
This problem has been noted by \citet{roweissaul2000} without offering a 
specific remedy.
The implementation uses a small additive constant $\gamma > 0$ to strengthen the 
main diagonal, as is standard practice in numerical optimization 
\citep{ghojoghetal2020}.
$\gamma$ is computed as the sum over the eigenvalues of the local Gram matrix, 
multiplied by a tolerance parameter set to 0.01, following a proposal by
\citet{dissgrilli2007}.
In fact, the embedding is rather sensitive to regularization 
\citep{deridderduin2002}; so, even though it is beyond the scope of this report 
to specifically address the issue in detail, it would be certainly worthwhile.