\subsection{Eigenanalysis}
\label{eigenanalysis}

Eventually, all spectral manifold learning methods boil down to an eigenanalysis 
of a matrix $K$ believed to capture information about the intrinsic manifold 
structure.
As explained in chapter \ref{lgb-principles}, PCA finds the principal 
eigenvectors of empirical covariance, thereby defining a low-dimensional 
subspace containing most of the data-inherent variability.
The very same idea applies when diagonalizing the more general matrix 
corresponding to the non-linear feature map: the top (or bottom\footnote{
This differs across methods and shall be made clear later.
}) $d$ eigenvectors of $K$ span a subspace into which the data may be projected 
under minimal loss of information.
More precisely, the representation of $\X$ by the $d$ principal eigenvectors of 
$K$ incurs is loss-optimal with respect to the least-squares error.
Eigenanalysis is thus a very powerful concept with ubiquitous application 
\citep{schoelkopfetal1998}.
\\

\textbf{Eigenvectors and eigenvalues.} Formally, eigenanalysis is the 
decomposition of a square matrix into pairs of \textit{eigenvectors} and 
\textit{eigenvalues}.
Let $A \in \R^{N \times N}$ be a square matrix and $\lambda \in \R$ a scalar 
value. 
$\lambda$ is said to be an eigenvalue to $A$ if there exists 
$\bm{v} \in \R^N \setminus \{0\}$ such that $A \bm{v} = \lambda \bm{v}$.
Then, $v$ is the eigenvector corresponding to the eigenvalue $\lambda$, and 
their tuple is also called an \textit{eigenpair}.
\\

\textbf{Null spaces.} A closely related notion is that of the 
\textit{null space}, consisting of the vectors that map $A$ to 0 upon 
multiplication from the right: $\{\bm{v} \in \R^N: A \bm{v} = 0\}$.
It can be easily seen that the null space consists precisely of those 
eigenvectors of $A$ that correspond to an eigenvalue of zero and the zero 
vector itself.
For a specific eigenvalue $\lambda$ of $A$, the null space of $\lambda I - A$ 
(with $I$ the $N$-dimensional identity matrix) constitutes the 
\textit{eigenspace} of $A$.
\citep{boermmehl2012}.
\\

\textbf{Generalized eigenvalue problems.} Eigendecomposition of a matrix $A$ can 
be framed as the solution of a generalized eigenvalue problem.
Generalized eigenvalue problems are posed subject to a constraint on a second, 
also symmetric matrix $B \in \R^{N \times N}$.
As the standard eigenvalue problem results immediately from $B = I$, the 
generalized form subsumes both cases.
It is given by $$A \bm{V} = B \bm{V} \bm{\Lambda},$$ where 
$\bm{V} = ([\bm{v}_1, \bm{v}_2, ..., \bm{v}_N]) \in \R^{N \times N}$ 
is the matrix of eigenvectors of $A$ and 
$\bm{\Lambda} = \text{\textit{diag}}([\lambda_1, \lambda_2, ..., \lambda_N]^T) 
\in \R^{N \times N}$ is the diagonal matrix of the associated eigenvalues.
The generalized eigenvalue problem may be stated equivalently as 
$$\max_{\bm{V}} \text{\textit{trace}}(\bm{V}^T A \bm{V}), \quad \text{s.t.} 
\quad \bm{V}^T B \bm{V} = I,$$ and translated to the first form with help of the 
Lagrangian multiplier \citep{ghojoghetal2019}.
It must be noted that solving eigenvalue problems becomes computationally 
challenging rather quickly.
Therefore, eigendecomposition is performed approximately in virtually all 
practical applications \citep{boermmehl2012}.

% ------------------------------------------------------------------------------
