Machine learning problems increasingly employ data of high dimensionality. 
While a large amount of samples is beneficial to learning, high-dimensional 
feature spaces, such as in speech recognition or gene processing, pose serious 
obstacles to the performance and convergence of most algorithms 
\citep{cayton2005}. 
Three aspects strike as particularly problematic: computational complexity, 
interpretability of results, and geometric idiosyncrasies of high-dimensional 
spaces.
Computational cost must be considered but is becoming less of an issue with 
technological evolution \citep{leistetal2009}.
By contrast, explainable results are increasingly in demand, but 
virtually inaccessible in more than a few dimensions \citep{doshivelezkim2017}. 
The geometric aspect entails, among others, a sharp incline in the number of 
points required to sample spaces and a loss in meaningfulness of distances 
\citep{verleysenfrancois2005}.
\\

\textbf{Manifold assumption.}
These challenges make the case for \textit{dimensionality reduction}. 
Far from undue simplification, the endeavor is justified by the 
belief that the data-generating process is indeed of much lower dimension 
than is observed\footnote{
Consider, for example, image data of objects in different poses.
Such data are typically stored in large pixel representations, yet it 
is reasonable to suppose the true sources of variability are few.
}.
More formally, the data are assumed to lie on a $d$-dimensional 
\textit{manifold}, i.e., the $d$-dimensional generalization of
a curved surface, embedded in the $D$-dimensional observation space with 
$d \ll D$ \citep{cayton2005}.
A crucial property of $d$-manifolds is their local topological equivalence to 
$\Rd$ \citep{mafu2011}.
It is precisely this locally Euclidean behavior that allows manifold coordinates 
to be mapped to $\Rd$ in a structure-preserving manner \citep{cayton2005}.
Finding this mapping constitutes an unsupervised task where models must learn 
the intrinsic manifold structure \citep{mafu2011}.
\\

\textbf{Local graph-based manifold learning (LGML).}
Various approaches have been proposed to retrieve points' intrinsic coordinates.
A taxonomy may be found in \citet{vandermaatenetal2009}. 
Many can be subsumed under the framework of \textit{kernel PCA}, characterizing 
the data by a specific matrix representation whose principal eigenvectors are 
used to span a $d$-dimensional embedding space \citep{hametal2003}.
As manifolds may exhibit complicated surfaces, methods that find non-linear 
representations are often more successful \citep{vandermaatenetal2009}.
LGML techniques achieve this by approximating the manifold with weighted 
neighborhood graphs.
They pay particular heed to local environments and are thus able retrieve 
highly non-linear structures \citep{belkinniyogi2003}.
\textit{Locally linear embedding (LLE)} is one of the earliest such techniques 
\citep{roweissaul2000}.
It is based on a rather heuristical notion of preserving local neighborhood 
relations.
\textit{Laplacian eigenmaps (LEM)} was proposed somewhat later on a more rigid 
theoretical foundation that is also extendable to LLE 
\citep{belkinniyogi2003}.
Both ideas are straddled by \textit{Hessian LLE (HLLE)}, a conceptual variant of 
LEM algorithmically akin to LLE \citep{donohogrimes2003}.
The fully unsupervised functionality of these methods offers a drawback: they 
may fail to find an embedding that has an actual reflection in the 
real-life setting.
Therefore, \citet{yangetal2006} incorporate prior information in 
\textit{semi-supervised LLE (SSLLE)} to produce more meaningful 
embeddings\footnote{
Note that this is rather different from general semi-supervised 
learning: SSLLE supports an inherently unsupervised task by some labeled data 
points.
Alternative proposals for a semi-supervised LLE have been made, 
e.g., by \citet{zhangchau2009}, that build upon a fully supervised LLE 
\citep{deridderduin2002}.
}.
\\

\textbf{Outline.}
Indeed, their results indicate considerable success of SSLLE.
It is the aim of this work to (1) reproduce these results, creating
an open-source implementation, and (2) to assess its performance under different 
parameter settings. 
The remainder of the report is organized as follows: first, the problem of 
manifold learning is formalized. 
The subsequent chapters sketch the idea of LGML 
and lay out the above named unsupervised techniques and SSLLE in more detail.
Afterwards, the results of the conducted experiments are presented, before the 
report concludes with a brief discussion.

% The remainder of the report is organized as follows: after formalizing the 
% problem of manifold learning (chapter \ref{math}), chapter \ref{lgml} explains 
% the idea of LGML; then, the above named unsupervised techniques and SSLLE are 
% considered in more detail (chapter \ref{techniques}).
% Chapter \ref{experiment} presents the results of the conducted experiments; 
% and finally, chapter \ref{discussion} concludes with a brief discussion.