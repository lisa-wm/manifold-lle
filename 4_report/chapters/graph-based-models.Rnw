\subsection{Principles of Local Graph-Based Manifold Learning}
\label{lgb-principles}

% ------------------------------------------------------------------------------

\subsubsection{General Concept}
\label{lgb-concept}

% As stated above, many methods of manifold learning rely on spectral techniques:
% they construct a special matrix from the observed data and use its eigenvectors 
% to compute the sought-after low-dimensional coordinates $Y$ (spectral 
% decomposition will be covered in somewhat more detail in chapter 
% \ref{eigenwert}.
% The nature of this matrix depends on the type of intrinsic structure the
% mapping shall preserve.
% \textit{Principal component analysis (PCA)} and \textit{multi-dimensional 
% scaling (MDS)} both retain global covariance structure
% 
% Linear methods, such as \textit{principal component analysis (PCA)} and 
% \textit{multi-dimensional scaling (MDS)}, implicitly assume the data to lie on a
% linear subspace of $\RD$.
% Both PCA and MDS achieve dimensionality reduction through representation of the 
% data along the axes of maximum variance.

(Local) graph-based techniques of manifold learning arise from varying geometric 
intuitions and computational approaches.
Interestingly, they still share a common structure that allows for 
interpretation in a general framework.
The first main block of this framework produces a matrix representation of the
$D$-dimensional data which is then mapped to $d$ dimensions in a subsequent step 
by means of spectral decomposition, such that the data are expressed via 
coordinates on the principal eigenvectors of the matrix \citep{sauletal2006}.
In fact, this procedure is not unique to graph-based methods but characterizes
all spectral methods \citep{cayton2005}.
Precisely how the matrix is constructed, however, determines the kind of 
intrinsic structure that can be learned and preserved.
As argued before, non-linearity is a desirable property that techniques based on
linear data structures\footnote{
For instance, principal component analysis (PCA) decomposes the covariance 
matrix, whereas multi-dimensional scaling (MDS) is based on the Gram matrix.
Both approaches essentially perform a rotation of the data and a linear 
projection to the subspace of maximum variance, yielding the same results 
\citep{cayton2005}.
} fail to achieve.
Graph-based methods derive their matrix from weighted neighborhood graphs.
In effect, these graphs are a discretized approximation of the underlying 
manifold $\mani$ (still assuming $\mani$ is sampled well by $\bm{X}$), and as 
such able to capture highly non-linear structures.
To this end, vicinities are expressed via graph representations 
\citep{sauletal2006}.
Local graph-based methods additionally restrict neighborhoods to local ones and
thereby avoid shortcuts: for manifolds with strongly non-convex surfaces, global 
solutions often prove too coarse.
The consequence of such locality is sparsity of graph matrices 
\citep{belkinniyogi2003}.

Summing up the above, local graph-based manifold learning can be schematized as
follows:

\begin{tight_enumerate}
  \item Compute neighborhoods of input data.
  \item Construct a sparse weighted graph from these neighborhood relations.
  \item Condense the graph information in a matrix.
  \item Learn an embedding from the eigenvectors of this matrix.
\end{tight_enumerate}

Crucially, the complex manifold learning problem can be addressed by a sequence 
of tractable optimization steps \citep{sauletal2006}.
The following chapters explain these steps in some more detail. 

% ------------------------------------------------------------------------------

\subsubsection{Neighborhood Graphs}
\label{neighbor-graph}

Approximating the intrinsic structure of $\mani$ by a neighborhood graph 
requires the determination of a \textit{neighborhood} around each 
$\bm{x} \in \bm{X}$.
A neighborhood of $\bm{x}$ is but a subset of $\bm{X}$ containing
another, open subset of $\bm{X}$ of which $\bm{x}$ is an element.
Members of the neighborhood are called neighbors of $\bm{x}$.
In metric spaces neighborhoods are defined via distances and therefore 
translate to open balls around each point \citep{waldmann2014}.
This distance-based construction now locally applies to manifolds as a direct 
consequence of their locally Euclidean behavior \citep{mafu2011}.
Neighborhood relations in local graph-based manifold learning may therefore 
established in the Euclidean observation space and are as such of linear nature.

There are two principal ways to build a neighborhood around $\bm{x} \in \bm{X}$, 
both of which usually employ squared Euclidean distances\footnote{
In principle, alternative distance measures are equally applicable.
}, denoted by $\| \cdot \|^2$.
Let $\mathcal{N}: \bm{X} \rightarrow \bm{X}^{\ell}, \bm{x} \mapsto \mathcal{N}
(\bm{x})$ be a constructor that assigns a set of neighbors to $\bm{x}$.
The first possibility is to restrict the size of the neighborhood to the $k$ 
points with the smallest distance to $\bm{x}$, such that
$\ell = k$ and $\mathcal{N}_k(\bm{x}) = \{\bm{x}_j \in \bm{X}: 
\| \bm{x} - \bm{x}_j \|^2 \leq
\gamma\}$, with $\gamma \in \R$ being the $k$-percentile of ordered pairwise 
distances.
Alternatively, the neighborhood may be constructed by collecting all points that
have a maximum distance of $\epsilon \in \R$ to $\bm{x}$, no matter how many 
neighbors this will result in, yielding $\mathcal{N}_{\epsilon} (\bm{x}) = 
\{\bm{x}_j \in \bm{X}: \| \bm{x} - \bm{x}_j \|^2 \leq \epsilon\}$ and 
$\ell = |\mathcal{N}_{\epsilon} (\bm{x})|$ \citep{heetal2005}.

\begin{minipage}[b]{0.5\textwidth}
  The thus defined neighborhoods can now be described by a \textit{neighborhood 
  graph} $\mathcal{G}$, where input points form vertices and edges indicate 
  neighborhood relations.
  Each vertex is connected to its $k$ nearest neighbors or all points 
  within $\epsilon$-radius, depending on the neighborhood definition.
  It is easy to see that $k$-neighborhoods are an asymmetric notion; for one 
  point to be among another's $k$ nearest neighbors the reverse need not be 
  true.
  Building upon $k$-neighborhoods therefore leads to directed graphs 
  \citep{heetal2005}.
  An example for such a directed graph is given by figure 
  \ref{fig:neighbor-graph}, showing 2-neighborhoods for seven fictional data 
  points.
  Conversely, the $\epsilon$-distance boundary holds in both directions and 
  produces undirected graphs \citep{heetal2005}.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 50 60 30, clip, % left bottom right top
      width = 0.9\textwidth]{figures/neighborhood-graph}
    \caption[Exemplary neighborhood graph]{Exemplary neighborhood graph for 
    seven fictional data points where outgoing arrows point to members of the 
    vertex's respective $k$-neighborhood, $k = 2$. \textit{Source:} own 
    representation.}
    \label{fig:neighbor-graph}
  \end{figure}
\end{minipage}

% ------------------------------------------------------------------------------

\subsubsection{Matrix Representation of Neighborhood Graphs}
\label{matrix-repr}

\begin{itemize}
  \item Degree and adjacency matrices
  \item Laplacian operators
\end{itemize}

% ------------------------------------------------------------------------------

\subsubsection{Solving Eigenwertproblems}
\label{eigenwert}

\begin{itemize}
  \item Eigenvectors, eigenvalues
  \item Spectral decomposition
\end{itemize}

% ------------------------------------------------------------------------------

\subsection{Techniques of Local Graph-Based Manifold Learning}
\label{lgb-tech}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps}
\label{laplace}

\begin{itemize}
  \item Notion of locality
  \item Laplacian eigenmaps
\end{itemize}

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

\begin{itemize}
  \item Notion of local linearity
  \item Approximation of graph Laplacian
\end{itemize}

% TODO Regularized version???

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (H-LLE)}
\label{hlle}

\begin{itemize}
  \item Hessian instead of Laplacian (eigenmaps)
  \item Hessian instead of LS fit (LLE)
\end{itemize}