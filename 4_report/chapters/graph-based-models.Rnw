\subsection{Principles of Local Graph-Based Manifold Learning}
\label{lgb-principles}

After the goal of manifold learning has been formalized, it shall now be laid 
out how the problem is approached by LLE as the conceptual parent of SS-LLE 
The incorporation of prior information is a rather different matter that will be 
addressed in chapter \ref{sslle}. 
Much of the theoretical foundation for LLE has been discussed only in later 
work.
In order to provide a more integrated background, explanations will therefore be 
given in a broader context of local graph-based manifold learning which also 
comprises Laplacian eigenmaps and H-LLE.
The particular relationship of the three methods shall be made clear along the 
way.

Local graph-based manifold learning generally arises from a variety 
of geometric intuitions and computational implementations.
Nonetheless, methods share common structures that allow for interpretation in a 
more abstract framework (\citet{bengioetal2003}, \citet{bengioetal2004}).
It should be noted that such a framework might be established from several 
perspectives; after all, the different methods attempt to solve the same problem 
and can thus often be translated into one another.
This report will sketch the idea behind local graph-based manifold learning in 
the light of \textit{kernel principal component analysis (kernel PCA)}.
The reasoning behind this choice is that kernel PCA provides a useful 
intuition to manifold learning and subsumes the other methods in a way that 
proves beneficial for out-of-sample extension \citep{bengioetal2004}.
Kernel PCA was actually proposed first and later shown to link the others by a 
unified idea (\citet{hametal2003}, \citet{bengioetal2004}). 
\\

\textbf{Conceptual framework.} Kernel PCA builds upon two fundamental concepts 
in machine learning: it performs \textit{principal component analysis (PCA)} on 
data transformed by the \textit{kernel trick} \citep{schoelkopfetal1998}.
PCA is a quite powerful technique by itself already.
It finds the directions of maximum variance along with their respective 
degree of importance through eigenanalysis of the empirical covariance 
matrix (chapter \ref{eigenanalysis} will treat eigenanalysis in more detail).
The data are projected into the linear subspace spanned by the $d$ 
principal eigenvectors as determined by ordering of their associated 
eigenvalues.
In this, PCA is an orthogonal transformation of the input coordinates that 
allows for representation by a smaller number of axes carrying most of the 
inherent variability \citep{cayton2005}.

Its central drawback is the confinement to linear subspaces.
In general applications, data must be expected to lie on non-linear manifolds 
and PCA will fail to retrieve their latent structure.
Kernel PCA now applies a trick otherwise known from support vector machines.
By kernelization, i.e., mapping the data to a space $\mathcal{F}$ of arbitrarily 
high dimension, it obtains features that relate to the input in a possibly 
complicated non-linear way\footnote{
Support vector machines use the kernel trick to achieve linear separability. 
An intuitive example may be given by data observed in two classes that form 
concentric circles in $\R^2$. 
While such data are not linearly separable in two dimensions, they are in three: 
mapping the classes to different heights enables separation by a horizontal 
hyperplane.
This example also hints at the idea of (spectral) clustering to which kernel 
PCA is indeed intimately related \citep{bengioetal2004}.
}.
Crucially, the feature map $\phi: \RD \rightarrow \mathcal{F}$ need never be 
explicitly computed (this might prove prohibitively expensive).
Kernel PCA instead solely relies on inner products $\langle \phi(\x_i), 
\phi(\x_j) \rangle$ of the transformed inputs.
Employing Mercer's theorem of functional analysis, these inner products may 
be interpreted as performed by a continuous kernel 
$\kappa: \RD \rightarrow \mathcal{F}, (\x_i, \x_j) \mapsto \kappa(\x_i, \x_j)$, 
in some corresponding Hilbert space.
Appropriate choice of $\kappa$ then allows for the data to be represented by a 
matrix $K \in \R^{N \times N}, K_{ij} = \kappa(\x_i, \x_j)$.
Diagonalization of $K$ via PCA eventually yields the sought-for 
$d$-dimensional coordinates \citep{schoelkopfetal1998}.
The data are thus still projected into a linear subspace, but one of features 
that are able to capture non-linear intrinsic structures \citep{hametal2003}.

\begin{figure}[H]
  \centering
  \includegraphics[width = \textwidth]{figures/spirals}
  \caption[Schematic functionality of kernel PCA]{Schematic functionality of 
  kernel PCA: the original data (\textit{left}) are observed in two dimensions 
  but clearly intrinsically one-dimensional, where the intrinsic manifold 
  feature is expressed by coloring. The kernel trick creates a non-linear 
  map, visualized here as mapping the intrinsic feature to a third coordinate 
  axis (\textit{middle}). Projecting the data into the subspace of the intrinsic 
  feature yields the desired one-dimensional representation (\textit{right}). 
  \textit{Source:} own representation, using a subset of \texttt{mlbench}'s 
  noise-free \texttt{spirals} data. Note that this is but a schematic depiction 
  where the mid and right representation have not been created by an actual 
  implementation of kernel PCA.}
  \label{fig:spirals}
\end{figure}

\textbf{Particular achievements.} The interesting part is now how manifold 
learning methods arrive at a matrix representation of the data.
All approaches considered here are variations of kernel PCA and might
as such be characterized by their different kernel functions.
For a deeper understanding it is, however, more illustrative to outline their 
respective intuitions and emphasize algorithmic similarities\footnote{
For an explicit interpretation of kernels in LLE, Laplacian eigenmaps and H-LLE 
see for example \citet{bengioetal2004} and \citet{weinbergeretal2004}.
}.
These revolve around two desiderata of manifold learning that shall first be 
made clear. 

\begin{itemize}
  \item[] \textbf{Non-linearity.} As extensively discussed above, $\X$ must be 
  assumed to lie on a non-linear manifold $\mani \subset \RD$, which is 
  precisely why kernel-PCA-based methods apply the kernel trick\footnote{
  By contrast, standard PCA, and the related method of \textit{multi-dimensional 
  scaling (MDS)}, may be viewed as special instances of kernel PCA with linear 
  kernels \citep{hametal2003}.
  }.
  Graph-based methods implicitly define their respective kernels via non-linear 
  functionals that seek to capture the intrinsic manifold structure.
  To this end, they construct a discretized approximation of the manifold 
  surface with a finite neighborhood graph and use this to obtain a matrix of 
  non-linear features \citep{sauletal2006}.
  \item[] \textbf{Locality.} A second important property of $\mani$ is its 
  potential non-convexity, which may be imagined as the manifold containing 
  "holes" \citep{sauletal2006}.
  Non-convexity occurs when $\mani$ is isometric to a non-convex subset of 
  Euclidean space \citep{donohogrimes2003}. 
  Intuitively, such behavior requires careful tracing of the manifold surface.
  Local graph-based methods therefore focus on solely local manifold properties, 
  and, in doing so, produce sparse matrix representations \citep{cayton2005}.
  They are frequently contrasted to \textit{Isomap}, one of the 
  earliest and most prominent examples of global manifold learning.
  Isomap retains pairwise distances between points on the manifold surface as 
  measured along graph edges via geodesic curves\footnote{
  It is thus a non-linear variant of MDS which uses standard Euclidean distances 
  \citep{tenenbaumdesilvalangford2000}.
  } \citep{tenenbaumdesilvalangford2000}.
  While it yields good results in many applications, Isomap does not 
  sufficiently account for the curvature of highly non-convex manifolds.
  In order to avoid this drawback, local methods therefore narrow down isometry 
  to only hold between neighboring points and relax the parameter space 
  condition to open, connected subspaces \citep{donohogrimes2003}.
\end{itemize}

\textbf{Algorithmic structure.} Despite rather different geometrical intuitions, 
which are discussed in chapter \ref{lgb-tech}, their common link to kernel PCA 
leads to an interesting algorithmic similarity of Laplacian eigenmaps, LLE and 
H-LLE.
They all operationalize local-graph based manifold learning as follows 
\citep{bengioetal2004}:

\begin{tight_enumerate}
  \item Approximate the manifold structure by a sparse neighborhood graph 
  $\mathcal{G}$.
  \item Estimate a functional on $\mani$, associated with the intrinsic 
  structure and directly related to a data-dependent kernel, from $\mathcal{G}$.
  \item Perform an eigenanalysis of the estimated matrix and learn $\Y$ from 
  coordinates of the $d$ principal (top or bottom) eigenvectors.
\end{tight_enumerate}

The following chapter will provide some details on the central concepts employed 
in this operationalization.

% ------------------------------------------------------------------------------

\subsection{Concepts in Operationalization}
\label{concepts-op}

% ------------------------------------------------------------------------------

\subsubsection{Neighborhood Graphs}
\label{neighbor-graph}

Approximating the intrinsic structure of $\mani$ by a graph representation 
requires the determination of \textit{neighborhoods}.
A neighborhood of $\x \in \X$ is a subset of $\X$ containing another, open 
subset of $\X$ of which $\x$ is an element.
Members of the neighborhood are called neighbors of $\x$.
In metric spaces neighborhoods are defined via distances and therefore 
translate to open balls around each point \citep{waldmann2014}.
This distance-based construction locally applies to manifolds as a direct 
consequence of their local isometry to the Euclidean observation space 
\citep{mafu2011}.
There are two principal ways to build a neighborhood around $\x \in \X$, 
both of which usually employ the squared Euclidean norm\footnote{
In principle, alternative metrics are equally applicable, for instance such 
that measure angles \citep{belkinniyogi2004}.
} $\| \cdot \|^2$.
Let $\mathcal{N}: \X \rightarrow \X^{\ell}, \x \mapsto \mathcal{N} (\x)$ be a 
constructor that assigns a set of neighbors to $\x$.
The first possibility is to restrict the size of the neighborhood to the $k$ 
points with the smallest distance to $\x$, such that
$\ell = k$ and $\mathcal{N}_k(\x) = \{\x_j \in \X: 
\| \x - \x_j \|^2 \leq \gamma\}$, with $\gamma \in \R$ being the $k$-th instance 
of ordered pairwise distances.
Alternatively, the neighborhood may be constructed by collecting all points that
have a maximum distance of $\epsilon \in \R$ to $\x$, yielding 
$\mathcal{N}_{\epsilon} (\x) = 
\{\x_j \in \X: \| \x - \x_j \|^2 \leq \epsilon\}$ and 
$\ell = |\mathcal{N}_{\epsilon} (\x)|$ \citep{heetal2005}.
Both $k$ and $\epsilon$ are hyperparameters that must be specified up-front.
Their choice reflects beliefs about the topological structure of $\mani$ -- 
smaller neighborhoods corresponding to a higher degree of non-linearity -- and 
may affect performance rather strongly \citep{sudderth2002}.

\begin{minipage}[b]{0.5\textwidth}
  $\mani$ can now be characterized by a \textit{neighborhood 
  graph} $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, still assuming it is 
  sampled well by $\X$. 
  Inputs $\x \in \X$ form vertices $\mathcal{V}$ and edges $\mathcal{E}$ 
  indicate neighborhood relations \citep{belkinniyogi2001}.
  Each vertex is connected to its $k$ nearest neighbors or all points 
  within $\epsilon$-radius, depending on the neighborhood definition.
  It is easy to see that $k$-neighborhoods are an asymmetric notion; for one 
  point to be among another's $k$ nearest neighbors the reverse need not be 
  true.
  $k$-neighborhoods therefore lead to directed graphs.
  Conversely, the $\epsilon$-distance boundary holds in both directions and 
  produces undirected graphs \citep{heetal2005}.
  A fictional example for a directed graph is given by figure 
  \ref{fig:neighbor-graph}, showing $k$-neighborhoods for seven data points 
  ($k = 2$).
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 50 60 30, clip, % left bottom right top
      width = 0.9\textwidth]{figures/neighborhood-graph}
    \caption[Exemplary neighborhood graph]{Exemplary neighborhood graph for 
    seven fictional data points where outgoing arrows point to members of the 
    vertex's respective $k$-neighborhood, $k = 2$. \textit{Source:} own 
    representation.}
    \label{fig:neighbor-graph}
  \end{figure}
\end{minipage}

% ------------------------------------------------------------------------------

\subsubsection{Local Tangent Spaces}
\label{tangent-spaces}

Both Laplacian eigenmaps and H-LLE construct their manifold functional as a 
differential operator with respect to \textit{local tangent spaces}.
LLE can be shown to approximate the Laplacian-Beltrami operator used in 
Laplacian eigenmaps under certain assumptions and thus also bears an implicit 
relation to these tangent spaces (\citet{belkinniyogi2001}, 
\citet{donohogrimes2003}).
The local tangent space $T_m(\mani)$ at a point $m \in \mani$ is spanned by 
vectors tangent to $\mani$ at $m$. 
Smoothness of $\mani$ thereby ensures $T_m(\mani)$ is well-defined.

\begin{minipage}[b]{0.5\textwidth}
  As $\mani$ is embedded in $\RD$, its tangent spaces are again Euclidean and of 
  dimension $d$, that is, $d$-dimensional hyperplanes as depicted in figure 
  \ref{fig:sphere-tangent}.
  They may be viewed as linear approximations to the manifold surface 
  \citep{sudderth2002}.
  If $m \in \mani$ is identified with the origin of $T_m(\mani)$, then every 
  point in the neighborhood of $m$ has a unique closest point 
  $m^{\prime} \in T_m(\mani)$, with a smooth mapping $m \mapsto m^{\prime}$.
  The tangent space therefore inherits a (non-unique) orthonormal coordinate 
  system obtained from endowing $T_m(\mani)$ with the inner product from $\RD$ 
  \citep{donohogrimes2003}.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 70 60 30, clip, % left bottom right top
      width = 0.8\textwidth]{figures/sphere-tangent}
    \caption[Tangent hyperplane for two-dimensional unit sphere]{Tangent 
    hyperplane for exemplary point on the two-dimensional unit sphere manifold, 
    embedded in $\R^3$. \textit{Source:} own representation.}
    \label{fig:sphere-tangent}
  \end{figure}
\end{minipage}

% ------------------------------------------------------------------------------

\subsubsection{Eigenanalysis}
\label{eigenanalysis}

Eventually, all spectral manifold learning methods boil down to finding 
eigenvectors of some matrix associated with the intrinsic manifold structure.

% ------------------------------------------------------------------------------

\subsection{Unsupervised Techniques}
\label{lgb-tech}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps}
\label{laplace}

\begin{itemize}
  \item Notion of locality
  \item Laplacian eigenmaps
\end{itemize}

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

\begin{itemize}
  \item Notion of local linearity
  \item Approximation of graph Laplacian
\end{itemize}

% TODO Regularized version???

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (H-LLE)}
\label{hlle}

\begin{itemize}
  \item Hessian instead of Laplacian (eigenmaps)
  \item Hessian instead of LS fit (LLE)
\end{itemize}