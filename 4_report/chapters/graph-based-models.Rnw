\subsection{Principles of Locally Linear Graph-Based Manifold Learning}
\label{lgb-principles}

% ------------------------------------------------------------------------------

\subsubsection{General Concept}
\label{lgb-concept}

% As stated above, many methods of manifold learning rely on spectral techniques:
% they construct a special matrix from the observed data and use its eigenvectors 
% to compute the sought-after low-dimensional coordinates $Y$ (spectral 
% decomposition will be covered in somewhat more detail in chapter 
% \ref{eigenwert}.
% The nature of this matrix depends on the type of intrinsic structure the
% mapping shall preserve.
% \textit{Principal component analysis (PCA)} and \textit{multi-dimensional 
% scaling (MDS)} both retain global covariance structure
% 
% Linear methods, such as \textit{principal component analysis (PCA)} and 
% \textit{multi-dimensional scaling (MDS)}, implicitly assume the data to lie on a
% linear subspace of $\RD$.
% Both PCA and MDS achieve dimensionality reduction through representation of the 
% data along the axes of maximum variance.

Local graph-based techniques of manifold learning arise from varying geometric 
intuitions and computational approaches.
Interestingly, they still share a common structure that allows for 
interpretation in a framework applicable to general spectral methods.
First, find a matrix representation for the $D$-dimensional data.
Second, perform a spectral decomposition of this matrix such that the data can 
be expressed via coordinates on the principal (top or bottom) eigenvectors
\citep{sauletal2006}.
Precisely how the matrix is constructed determines the kind of intrinsic 
structure that can be learned and preserved \citep{cayton2005}.
\\

\textbf{Non-linearity}. Early methods of manifold learning attempted to preserve 
linear structures observed in $\RD$.
The two most prominent of these are probably \textit{principal component 
analysis (PCA)} and \textit{multi-dimensional scaling (MDS)}.
Both eventually arrive at the same result; while PCA retains the global 
covariance structure, decomposing the covariance matrix, MDS keeps global 
pairwise Euclidean distances by decomposition of the Gram matrix 
\citep{cayton2005}.
The central drawback of this approach is the confinement to learning linear 
mappings.
Graph-based methods acknowledge the fact that, in general, data will rather lie 
on non-linear manifolds.
They model the intrinsic structure through neighborhood relations, 
mapping points that lie close on the manifold surface to nearby locations.
Neighborhoods are characterized via weighted graphs which are then coerced 
to numerical matrix representations.
In effect, these graphs are a discretized approximation of the underlying 
manifold $\mani$, still assuming $\mani$ is sampled well by $\bm{X}$ 
\citep{sauletal2006}.
\textit{ISOMAP} is one such graph-based technique that extends the approach of 
MDS to geodesic distances, meaning "nearness" between points is expressed by 
distance on the manifold surface rather than in the ambient Euclidean space 
\citep{tenenbaumdesilvalangford2000}.
\\

\textbf{Local isometry}. ISOMAP relies on a central assumption\footnote{
In fact, there is a second assumption of convex parameter spaces that is omitted 
here, see \citet{tenenbaumdesilvalangford2000}.
} that turns out 
to be too restrictive for many settings, namely global isometry: for an 
arbitrary pair of points on $\mani$, their geodesic distance is preserved 
in the mapping to $\Rd$ \citep{donohogrimes2003}.
This assumption is violated when $\mani$ is geodesically non-convex, i.e., it is 
not isometric to a convex subset of Euclidean space\footnote{
Intuitively, this can be imagined as the manifold containing "holes".
} \citep{sauletal2006}.
Local graph-based methods relax the isometry assumption to a local one.
Rather than for arbitrary pairs of points, it need only hold for neighboring 
ones \citep{donohogrimes2003}.
Where global solutions are sometimes too coarse, local methods thus allow for 
tracing non-convex behavior.
In doing so, they produce sparse matrix representations \citep{cayton2005}.
A prominent example of local graph-based techniques is given by 
\textit{Laplacian eigenmaps} \citep{belkinniyogi2001}.
Laplacian eigenmaps finds the eigenvectors of the graph Laplacian, achieving 
locality by employment of the heat kernel.
This approach will be studied in some more detail in chapter \ref{laplace}.
\\

\textcolor{red}{Maybe skip this!! Might not hold at all}
\textbf{Local linearity}. An further desideratum in learning Riemannian 
manifolds is the preservation of local linearity.
This is achieved by \textit{locally linear embedding (LLE)}.
As will be explained in chapter \ref{lle}, LLE \citep{roweissaul2000} 
basically performs a discrete approximation of the graph Laplacian by minimizing 
the error of linearly reconstructing points from their neighbors 
\citep{donohogrimes2003}.
A later proposition by \citet{donohogrimes2003}, \textit{Hessian LLE (H-LLE)}, 
may be viewed as an algorithmic variant of LLE and a conceptual variant of 
Laplacian eigenmaps using the Hessian en lieu of the Laplacian.

Summing up the above, local graph-based manifold learning may be schematized as
follows:

\begin{tight_enumerate}
  \item Compute neighborhoods of input data.
  \item Construct a sparse weighted graph from these neighborhood relations.
  \item Condense the graph information in a matrix.
  \item Learn an embedding from the eigenvectors of this matrix.
\end{tight_enumerate}

Crucially, the complex manifold learning problem is decomposed into a sequence 
of tractable optimization steps \citep{sauletal2006}.
The following chapters explain these steps in some more detail. 

% ------------------------------------------------------------------------------

\subsubsection{Neighborhood Graphs}
\label{neighbor-graph}

Approximating the intrinsic structure of $\mani$ by a graph representation 
requires the determination of a \textit{neighborhood} around point.
A neighborhood of $\bm{x} \in \bm{X}$ is but a subset of $\bm{X}$ containing
another, open subset of $\bm{X}$ of which $\bm{x}$ is an element.
Members of the neighborhood are called neighbors of $\bm{x}$.
In metric spaces neighborhoods are defined via distances and therefore 
translate to open balls around each point \citep{waldmann2014}.
This distance-based construction now locally applies to manifolds as a direct 
consequence of their local isometry to the Euclidean observation space 
\citep{mafu2011}.
There are two principal ways to build a neighborhood around $\bm{x} \in \bm{X}$, 
both of which usually employ squared Euclidean distances\footnote{
In principle, alternative metrics are equally applicable, for instance such 
that measure angles \citep{belkinniyogi2004}.
}, denoted by $\| \cdot \|^2$.
Let $\mathcal{N}: \bm{X} \rightarrow \bm{X}^{\ell}, \bm{x} \mapsto \mathcal{N}
(\bm{x})$ be a constructor that assigns a set of neighbors to $\bm{x}$.
The first possibility is to restrict the size of the neighborhood to the $k$ 
points with the smallest distance to $\bm{x}$, such that
$\ell = k$ and $\mathcal{N}_k(\bm{x}) = \{\bm{x}_j \in \bm{X}: 
\| \bm{x} - \bm{x}_j \|^2 \leq
\gamma\}$, with $\gamma \in \R$ being the $k$-th instance of ordered pairwise 
distances.
Alternatively, the neighborhood may be constructed by collecting all points that
have a maximum distance of $\epsilon \in \R$ to $\bm{x}$, yielding 
$\mathcal{N}_{\epsilon} (\bm{x}) = 
\{\bm{x}_j \in \bm{X}: \| \bm{x} - \bm{x}_j \|^2 \leq \epsilon\}$ and 
$\ell = |\mathcal{N}_{\epsilon} (\bm{x})|$ \citep{heetal2005}.

\vspace{0.5cm}

\begin{minipage}[b]{0.5\textwidth}
  The thus defined neighborhoods can now be described by a \textit{neighborhood 
  graph} $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where input points form 
  vertices and edges indicate neighborhood relations \citep{belkinniyogi2001}.
  Each vertex is connected to its $k$ nearest neighbors or all points 
  within $\epsilon$-radius, depending on the neighborhood definition.
  It is easy to see that $k$-neighborhoods are an asymmetric notion; for one 
  point to be among another's $k$ nearest neighbors the reverse need not be 
  true.
  Building upon $k$-neighborhoods therefore leads to directed graphs 
  \citep{heetal2005}.
  An example for such a directed graph is given by figure 
  \ref{fig:neighbor-graph}, showing 2-neighborhoods for seven fictional data 
  points.
  Conversely, the $\epsilon$-distance boundary holds in both directions and 
  produces undirected graphs \citep{heetal2005}.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 50 60 30, clip, % left bottom right top
      width = 0.9\textwidth]{figures/neighborhood-graph}
    \caption[Exemplary neighborhood graph]{Exemplary neighborhood graph for 
    seven fictional data points where outgoing arrows point to members of the 
    vertex's respective $k$-neighborhood, $k = 2$. \textit{Source:} own 
    representation.}
    \label{fig:neighbor-graph}
  \end{figure}
\end{minipage}

% ------------------------------------------------------------------------------

\subsubsection{Matrix Representation of Neighborhood Graphs}
\label{matrix-repr}

In order for neighborhood graphs\footnote{
Graphs are assumed to be connected, i.e., no vertex is left without at least one 
edge linking it to another vertex.
$k$-neighborhoods tend to yield connected graphs more often than 
$\epsilon$-neighborhoods.
If the connectedness assumption does not hold globally, each connected sub-graph 
must be considered separately \citep{belkinniyogi2001}.
} to translate into a low-dimensional embedding, the information they hold must 
be converted to a numerical representation.
This is achieved by assigning weights to graph edges and constructing a matrix 
from these \citep{belkinniyogi2001}.

% ------------------------------------------------------------------------------

\subsubsection{Solving Eigenwertproblems}
\label{eigenwert}

\begin{itemize}
  \item Eigenvectors, eigenvalues
  \item Spectral decomposition
\end{itemize}

% ------------------------------------------------------------------------------

\subsection{Techniques of Local Graph-Based Manifold Learning}
\label{lgb-tech}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps}
\label{laplace}

\begin{itemize}
  \item Notion of locality
  \item Laplacian eigenmaps
\end{itemize}

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

\begin{itemize}
  \item Notion of local linearity
  \item Approximation of graph Laplacian
\end{itemize}

% TODO Regularized version???

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (H-LLE)}
\label{hlle}

\begin{itemize}
  \item Hessian instead of Laplacian (eigenmaps)
  \item Hessian instead of LS fit (LLE)
\end{itemize}