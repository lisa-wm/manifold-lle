\subsection{Principles of Local Graph-Based Manifold Learning}
\label{lgb-principles}

% ------------------------------------------------------------------------------

\subsubsection{General Concept}
\label{lgb-concept}

After the goal of manifold learning has been formalized, it shall now be laid 
out how LLE -- as the conceptual parent of SS-LLE -- approaches the problem. 
This chapter will first sketch the fundamental idea of local graph-based 
techniques to provide some background.
Methods subsumed under this term arise from a variety of geometric intuitions 
and computational implementations but share a common structure that allows for 
interpretation in a framework generally applicable to spectral methods 
\citep{bengioetal2003}.

It must be noted that, while the algorithmic similarity is obvious, it is less 
clear whether the established theoretical connection holds as well.
\citet{belkinniyogi2001} derived their argumentation for using the graph 
Laplacian in Laplacian eigenmaps from its analogy to the Laplace-Beltrami 
operator on manifolds.
In subsequent studies they confirmed the legitimacy of this approach by 
providing proofs of convergence \citep{belkinniyogi2008}.
It has been widely assumed that LLE may be cast into this framework considering 
the inscribed eigenvalue problem as an approximation to the graph Laplacian 
(\citet{belkinniyogi2003}, \citet{donohogrimes2003}).
H-LLE neatly fits the scheme as it bears strong algorithmic resemblance to 
LLE, hence the name, and can be viewed as a conceptual variant of Laplacian 
eigenmaps, substituting the Laplacian for the Hessian \citep{donohogrimes2003}.
Recent studies, however, indicate that the asymptotic convergence of LLE to the 
Laplace-Beltrami operator might be subject to additional assumptions that might 
not always hold \citep{wuwu2018}.
Still, the three methods have at least a triangular relationship where H-LLE 
borrows from the theory behind Laplacian eigenmaps and employs an algorithm 
strongly reminiscent of LLE \citep{donohogrimes2003}.
For the sake of a more integrated picture, the following explanations will 
therefore be made in a shared framework \citep{bengioetal2003} with an emphasis 
on the algorithmic commonalities.
\\

To begin with, all spectral methods of manifold learning perform two central 
tasks: first, find a matrix representation for the intrinsic data structure; 
second, perform a spectral decomposition of this matrix such that the data can 
be expressed via coordinates on the principal eigenvectors
\citep{sauletal2006}.
Precisely how the matrix is constructed determines the kind of intrinsic 
structure that can be learned and preserved \citep{cayton2005}.
Local-graph based methods find theirs in a way that allows for two general 
desiderata in manifold learning. 
\\

\textbf{Non-linearity}. Early methods of manifold learning attempted to preserve 
linear structures observed in $\RD$.
The two most prominent of these are probably \textit{principal component 
analysis (PCA)} and \textit{multi-dimensional scaling (MDS)}.
Both eventually arrive at the same result; while PCA retains the global 
covariance structure, decomposing the covariance matrix, MDS keeps global 
pairwise Euclidean distances by decomposition of the Gram matrix 
\citep{cayton2005}.
The central drawback of these approaches is the confinement to learning linear 
mappings.

Graph-based methods acknowledge the fact that, in general, data will lie 
on non-linear manifolds.
The techniques studied here (explicitly or implicitly) revolve around 
functionals that capture the intrinsic manifold structure and whose top or 
bottom eigenvectors span the $d$-dimensional embedding space.
In order to retrieve the sought-after coordinates, the manifold is approximated 
by a discretized graph representation\footnote{
Graphs are assumed to be connected, i.e., no vertex is left without at least 
one edge linking it to another vertex.
If the connectedness assumption does not hold globally, each connected 
sub-graph must be considered separately \citep{belkinniyogi2001}.
} of the finite data $\X$, which may be imagined as a skeletal model of $\mani$.
The functional, defined with respect to the local tangent spaces of $\mani$, is 
then estimated from the neighborhood graph.
Eigenanalysis of the resulting matrix eventually yields the low-dimensional 
representation (\citet{belkinniyogi2001}, \citet{donohogrimes2003}).

\textit{Isomap} is one of the earliest graph-based techniques.
Like MDS, Isomap preserves pairwise distances between points, but does so for 
distances on the manifold surface (approximated by non-linear geodesics along 
graph edges) rather than in the ambient Euclidean space 
\citep{tenenbaumdesilvalangford2000}.
\\

\textbf{Locality}. Isomap relies on two central assumptions that turn out 
to be too restrictive for some settings.
The first is global isometry: for an arbitrary pair of points on $\mani$, their 
geodesic distance is preserved in the mapping to $\Rd$ \citep{donohogrimes2003}.
The second assumption states that the parameter space, i.e., the subset of 
$\Rd$ to which $\mani$ is locally homeomorphic, must be convex.
\citet{donohogrimes2003} show that many parameter spaces only obey the 
weaker conditions of being open and connected.
Isomap provides a global solution that proves too coarse when $\mani$ is 
geodesically non-convex, i.e., when it is not isometric to a convex subset of 
Euclidean space\footnote{
Intuitively, this can be imagined as the manifold containing "holes".
} \citep{sauletal2006}.
Local graph-based methods focus solely on preserving local properties to avoid 
this drawback and are thus able to trace highly non-convex manifold structures.
In doing so, they produce sparse matrix representations \citep{cayton2005}.
\\

Laplacian eigenmaps, LLE and H-LLE all belong to the family 
of local graph-based techniques.
Summing up the above, their functionality may be schematized as follows:

\begin{tight_enumerate}
  \item Approximate the manifold structure by a sparse neighborhood graph 
  $\mathcal{G}$.
  \item Construct a functional on $\mani$ that is associated with the 
  intrinsic data structure and estimate the functional from $\mathcal{G}$.
  \item Learn the $\Y$ by eigenanalysis of the estimated matrix.
\end{tight_enumerate}

With this, the manifold learning problem is decomposed into a sequence of 
tractable optimization steps \citep{sauletal2006}.
The central concepts employed in all algorithms are briefly explained in the 
subsequent chapters.

% ------------------------------------------------------------------------------

\subsubsection{Neighborhood Graphs}
\label{neighbor-graph}

Approximating the intrinsic structure of $\mani$ by a graph representation 
requires the determination of \textit{neighborhoods}.
A neighborhood of $\x \in \X$ is a subset of $\X$ containing another, open 
subset of $\X$ of which $\x$ is an element.
Members of the neighborhood are called neighbors of $\x$.
In metric spaces neighborhoods are defined via distances and therefore 
translate to open balls around each point \citep{waldmann2014}.
This distance-based construction locally applies to manifolds as a direct 
consequence of their local isometry to the Euclidean observation space 
\citep{mafu2011}.
There are two principal ways to build a neighborhood around $\x \in \X$, 
both of which usually employ the squared Euclidean norm\footnote{
In principle, alternative metrics are equally applicable, for instance such 
that measure angles \citep{belkinniyogi2004}.
} $\| \cdot \|^2$.
Let $\mathcal{N}: \X \rightarrow \X^{\ell}, \x \mapsto \mathcal{N} (\x)$ be a 
constructor that assigns a set of neighbors to $\x$.
The first possibility is to restrict the size of the neighborhood to the $k$ 
points with the smallest distance to $\x$, such that
$\ell = k$ and $\mathcal{N}_k(\x) = \{\x_j \in \X: 
\| \x - \x_j \|^2 \leq \gamma\}$, with $\gamma \in \R$ being the $k$-th instance 
of ordered pairwise distances.
Alternatively, the neighborhood may be constructed by collecting all points that
have a maximum distance of $\epsilon \in \R$ to $\x$, yielding 
$\mathcal{N}_{\epsilon} (\x) = 
\{\x_j \in \X: \| \x - \x_j \|^2 \leq \epsilon\}$ and 
$\ell = |\mathcal{N}_{\epsilon} (\x)|$ \citep{heetal2005}.
Both $k$ and $\epsilon$ are hyperparameters that must be specified up-front.
Their choice reflects beliefs about the topological structure of $\mani$ -- 
smaller neighborhoods corresponding to a higher degree of non-linearity -- and 
may affect performance rather strongly \citep{sudderth2002}.

\begin{minipage}[b]{0.5\textwidth}
  $\mani$ can now be characterized by a \textit{neighborhood 
  graph} $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, still assuming it is 
  sampled well by $\X$. 
  Inputs $\x \in \X$ form vertices $\mathcal{V}$ and edges $\mathcal{E}$ 
  indicate neighborhood relations \citep{belkinniyogi2001}.
  Each vertex is connected to its $k$ nearest neighbors or all points 
  within $\epsilon$-radius, depending on the neighborhood definition.
  It is easy to see that $k$-neighborhoods are an asymmetric notion; for one 
  point to be among another's $k$ nearest neighbors the reverse need not be 
  true.
  $k$-neighborhoods therefore lead to directed graphs.
  Conversely, the $\epsilon$-distance boundary holds in both directions and 
  produces undirected graphs \citep{heetal2005}.
  A fictional example for a directed graph is given by figure 
  \ref{fig:neighbor-graph}, showing $k$-neighborhoods for seven data points 
  ($k = 2$).
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 50 60 30, clip, % left bottom right top
      width = 0.9\textwidth]{figures/neighborhood-graph}
    \caption[Exemplary neighborhood graph]{Exemplary neighborhood graph for 
    seven fictional data points where outgoing arrows point to members of the 
    vertex's respective $k$-neighborhood, $k = 2$. \textit{Source:} own 
    representation.}
    \label{fig:neighbor-graph}
  \end{figure}
\end{minipage}

% ------------------------------------------------------------------------------

\subsubsection{Local Tangent Spaces}
\label{eigenwert}

Both Laplacian eigenmaps and H-LLE employ \textit{local tangent spaces} to 
construct their manifold functional, and as it may under certain assumptions 
also be regarded as an approximation to the Laplace-Beltrami operator, so does 
LLE (if only implicitly). 
The tangent space $T_m(\mani)$ at some point $m \in \mani$ is spanned by vectors 
tangent to $\mani$ at $m$. 
Smoothness of $\mani$ ensures well-definedness of $T_m(\mani)$.

\begin{minipage}[b]{0.5\textwidth}
  As $\mani$ is embedded in $\RD$, its tangent spaces are again Euclidean and of 
  dimension $d$, that is, $d$-dimensional hyperplanes.
  They may therefore be viewed as linear approximations to the manifold surface 
  \citep{sudderth2002}.
  If $m \in \mani$ is identified with the origin of $T_m(\mani)$, then every 
  point in the neighborhood of $m$ has a unique closest point 
  $m^{\prime} \in T_m(\mani)$, with a smooth mapping $m \mapsto m^{\prime}$.
  The tangent space therefore inherits a (non-unique) orthonormal coordinate 
  system obtained from endowing $T_m(\mani)$ with the inner product from $\RD$ 
  \citep{donohogrimes2003}.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 70 60 30, clip, % left bottom right top
      width = 0.8\textwidth]{figures/sphere-tangent}
    \caption[Tangent hyperplane for two-dimensional unit sphere]{Tangent 
    hyperplane for exemplary point on the two-dimensional unit sphere, embedded 
    in $\R^3$. \textit{Source:} own representation.}
    \label{fig:sphere-tangent}
  \end{figure}
\end{minipage}

% ------------------------------------------------------------------------------

\subsubsection{Eigenanalysis}
\label{eigenwert}

Laplacian eigenmaps, LLE and H-LLE each h

% ------------------------------------------------------------------------------

\subsection{Unsupervised Techniques of Local Graph-Based Manifold \\Learning}
\label{lgb-tech}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps}
\label{laplace}

\begin{itemize}
  \item Notion of locality
  \item Laplacian eigenmaps
\end{itemize}

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

\begin{itemize}
  \item Notion of local linearity
  \item Approximation of graph Laplacian
\end{itemize}

% TODO Regularized version???

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (H-LLE)}
\label{hlle}

\begin{itemize}
  \item Hessian instead of Laplacian (eigenmaps)
  \item Hessian instead of LS fit (LLE)
\end{itemize}