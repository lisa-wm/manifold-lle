Machine learning problems increasingly employ data of high dimensionality. 
While a large amount of samples is beneficial to learning, high-dimensional 
feature spaces, such as in speech recognition or gene processing, pose serious 
obstacles to the performance and convergence of most algorithms 
\citep{cayton2005}. 

Three aspects strike as particularly problematic: computational operations, 
interpretation of results, and geometric idiosyncrasies.
Computational cost must be considered but is becoming less of an issue with 
technological evolution \citep{leistetal2009}.
By contrast, the demand for explainable results (for reasons of, say, safety or
ethics) is rather intensified by the advance of complex technology. 
Alas, interpretation in more than a few dimensions is virtually inaccessible to 
humans \citep{doshivelezkim2017}. 
The geometric aspect is often addressed as \textit{curse of dimensionality}, a 
term subsuming various phenomena of high-dimensional spaces. 
It is generally not straightforward to infer properties of objects in these 
spaces as geometric intuition developed in lower dimensions can be 
misleading.
Crucially, the exponential increase of spatial volume induces sparsity. 
Consequences of this behavior are, among others, a sharp incline in the number 
of points required to sample the feature space and a loss in meaningfulness of 
distances. 
Many learners, however, rely on these 
concepts\footnote{For instance, consider support vector machines and
$k$-nearest neighbors, both of which rely on distances, or tuning, which often
requires extensive sampling of the hyperparameter space.} and see 
their functionality deteriorate \citep{verleysenfrancois2005}. 

These challenges make the case for \textit{dimensionality reduction}, that is, 
the endeavor of compressing problem dimensionality to a manageable size. 
Far from undue simplification, dimensionality reduction is justified by the 
belief that the latent data-generating process is indeed of much lower dimension 
than is observed.
Consider, for example, image data showing objects in different poses.
Such data are typically stored in high-dimensional pixel representations, yet it 
is reasonable to suppose that variation in these images is in fact caused by a 
small number of latent features.
More formally, the data are assumed to lie on a $d$-dimensional 
\textit{manifold} embedded in the $D$-dimensional observation space, with 
$d \ll D$ \citep{cayton2005}.

A crucial property of $d$-manifolds, i.e., the $d$-dimensional generalization of
a curved surface, embedded in $\RD$, is their local topological equivalence to 
$\Rd$ \citep{mafu2011}.
% This locally Euclidean behavior is exemplified by a sphere embedded in $\R^3$: 
% although the sphere as a whole is entirely non-linear, on sufficiently small 
% patches of its surface it behaves just like a flat plane in $\R^2$.
It is precisely this fact that allows manifold coordinates to be mapped to 
$\Rd$ in a reduction of dimensionality\footnote{
The most intuitive example of this is probably the representation 
of the Earth, which is a two-dimensional manifold enclosed in three-dimensional 
space, on two-dimensional maps.}.
The goal is now to learn this mapping in an unsupervised manner 
\citep{cayton2005}.
Mapping manifold coordinates to $\Rd$ is in general not equivalent to simple 
projection onto the $d$-dimensional coordinate hyperplanes.
Instead, models must learn the intrinsic neighborhood structure of the manifold
to establish a notion of "nearness" between points.
Standard distance metrics do not apply (globally) as points on general manifolds 
are connected by curved paths rather than straight lines \citep{mafu2011}.

Various approaches have been proposed to retrieve points' manifold coordinates.
A taxonomy may, for example, be found in \citet{vandermaatenetal2009}. 
Many rely on spectral techniques, trying to find a matrix representation of the 
data whose principal eigenvectors are used to span a $d$-dimensional subspace.
% Drawing from the nature of the eigenvalue problem they solve, spectral methods 
% are also referred to as \textit{convex}.
One group of spectral methods attempts to retain global isometry by mapping 
pairwise distances to $\Rd$.
Among them, some are based on Euclidean distances and thus confined to 
learning linear embeddings (such as \textit{principal component analysis (PCA)} 
or \textit{multi-dimensional scaling (MDS)}).
Since linearity is a strong assumption that will not hold for general manifolds, 
non-linear techniques are more widely applicable \citep{vandermaatenetal2009}.
For example, \textit{ISOMAP} achieves non-linearity by applying geodesic 
distances \citep{tenenbaumdesilvalangford2000}.
Research indicates, however, that for non-convex manifolds it is more effective 
to preserve local structures only.
Otherwise, solutions are prone to shortcuts, i.e., placing points close in $\RD$
next to each other when they lie in fact on quite different parts of the 
manifold \citep{belkinniyogi2003}.
In order to avoid such miscalculations, sparse techniques focus on merely 
local neighborhood structures, modeled through weighted graph representations.
The information from these graphs is then condensed into a sparse matrix. 
Eventually, the principal eigenvectors of this matrix yield the desired 
low-dimensional coordinates \citep{vandermaatenetal2009}.

One such local graph-based technique is \textit{Laplacian eigenmaps} 
\citep{belkinniyogi2001}, a method in whose general framework other techniques 
may be interpreted \citep{belkinniyogi2003}.
It employs the graph Laplacian and does well in preserving locality, yet is less 
adept at determining local linearity \citep{dissross2008} 
\textcolor{red}{find better source!!}.
This shortcoming is mitigated by \textit{locally linear embedding (LLE)}, 
proposed by \citet{roweissaul2000}, and its variants.
LLE is based on the idea that the embedded manifold may be approximated by 
locally linear neighborhoods in $\RD$.
Since weights resulting from linear reconstruction are believed to reflect 
the intrinsic geometry of the manifold, they are topological properties and as 
such invariant to rotations, rescalings, and translations.
By consequence, these same weights should also reconstruct points in $d$ 
dimensions.
LLE thus maps vicinity structures to the $d$-dimensional subspace and finds the 
coordinates that preserve them best \citep{roweissaul2000}.
% This requires solving the least-squares problem of minimizing reconstruction 
% error and then the sparse eigenvalue problem of minimizing embedding cost. 
% Convexity of both sub-problems ensures globality of local optima.
A later proposition by \citet{donohogrimes2003}, \textit{Hessian LLE (H-LLE)}, 
may be viewed as an algorithmic variant of LLE and a conceptual variant of 
Laplacian eigenmaps using the Hessian en lieu of the Laplacian.

These approaches have been shown to successfully retrieve manifold structures in
different applications (\textcolor{red}{evidence!!!}).
However, their fully unsupervised functionality offers a drawback: they may fail
to find a low-dimensional embedding that has an actual reflection in the 
real-life setting.
Such situations might require the specification of some pre-labeled instances.
Also, it may simply be the case that manual analysis of a subset of the data 
is available at low cost \citep{yangetal2006}.
When prior knowledge is at hand it is only natural to use it.
Therefore, \citet{yangetal2006} proposed \textit{semi-supervised locally linear 
embedding (SS-LLE)}, an extension to LLE that is able to harvest prior 
information.
\\

Indeed, the presented results indicate considerable success of their technique.
It is the aim of this report to (1) reproduce these results, thereby creating
an open-source implementation of SS-LLE, and (2) to apply SS-LLE to further 
manifold learning tasks for a more thorough assessment of its performance. 
The rest of the report is organized as follows: chapter \ref{math} 
provides a mathematical framework where fundamental concepts are briefly 
introduced; chapter \ref{lgb-mani-learn} explains the idea of local graph-based
manifold learning; chapter \ref{sslle} presents SS-LLE in detail; chapter
\ref{experiment} discusses the results of the conducted experiments; and chapter
\ref{concl} draws final conclusions.







