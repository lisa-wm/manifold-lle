Machine learning problems increasingly employ data of high dimensionality. 
While a large amount of samples is beneficial to learning, high-dimensional 
feature spaces, such as in speech recognition or gene processing, pose serious 
obstacles to the performance and convergence of most algorithms 
\citep{cayton2005}. 

Three aspects strike as particularly problematic: computational operations, 
interpretation of results, and geometric idiosyncrasies.
Computational cost must be considered but is becoming less of an issue with 
technological evolution \citep{leistetal2009}.
By contrast, the demand for explainable results (for reasons of, say, safety or
ethics) is rather intensified by the advance of complex technology. 
Alas, interpretation in more than a few dimensions is virtually inaccessible to 
humans \citep{doshivelezkim2017}. 
The geometric aspect is often addressed as \textit{curse of dimensionality}, a 
term subsuming various phenomena of high-dimensional spaces. 
It is generally not straightforward to infer properties of objects in these 
spaces as geometric intuition developed in two or three dimensions can be 
misleading.
Crucially, the exponential increase of spatial volume induces sparsity. 
Consequences of this behavior are, among others, a sharp incline in the number 
of points required to sample the feature space and a loss in meaningfulness of 
distances. 
Many learners, however, rely on these 
concepts\footnote{For instance, consider support vector machines and
$k$-nearest neighbors, both of which rely on distances, or tuning, which often
requires extensive sampling of the hyperparameter space.} and see 
their functionality deteriorate \citep{verleysenfrancois2005}. 

These challenges make the case for \textit{dimensionality reduction}, that is, 
the endeavor of compressing problem dimensionality to a manageable size. 
Far from undue simplification, dimensionality reduction is justified by the 
idea that the latent data-generating process is indeed of much lower dimension 
than is observed.
Consider, for example, image data showing objects in different poses.
Such data are typically stored in high-dimensional pixel representations.
It is, however, reasonable to suppose that variation in these images is in fact 
caused by a small number of latent features.
More formally, the data are assumed to lie on a $d$-dimensional 
\textit{manifold} embedded in the $D$-dimensional observation space, with 
$d \ll D$ \citep{cayton2005}.

A crucial property of $d$-manifolds, i.e., the $d$-dimensional generalization of
a curved surface, embedded in $\RD$, is their local topological equivalence to 
$\Rd$ \citep{mafu2011}.
% This locally Euclidean behavior is exemplified by a sphere embedded in $\R^3$: 
% although the sphere as a whole is entirely non-linear, on sufficiently small 
% patches of its surface it behaves just like a flat plane in $\R^2$.
It is precisely this fact that allows manifold coordinates to be mapped to 
$\Rd$ in a reduction of dimensionality\footnote{
The most intuitive example of this is probably the representation 
of the Earth, which is a two-dimensional manifold enclosed in three-dimensional 
space, on two-dimensional maps.}.
The goal is now to learn this mapping in an unsupervised manner 
\citep{cayton2005}.
Mapping manifold coordinates to $\Rd$ is in general not equivalent to simple 
projection onto the $d$-dimensional coordinate hyperplanes.
Instead, models must learn the intrinsic neighborhood structure of the manifold
to establish a notion of "nearness" between points.
Standard distance metrics do not apply here as points on general manifolds are 
connected by curved paths rather than straight lines \citep{mafu2011}.

Various approaches have been proposed to retrieve points' manifold coordinates.
A taxonomy may, for example, be found in \citet{vandermaatenetal2009}. 
Many rely on spectral techniques, trying to find a matrix representation of the 
data whose principal eigenvectors are used to span a $d$-dimensional subspace.
% Drawing from the nature of the eigenvalue problem they solve, spectral methods 
% are also referred to as \textit{convex}.
One group of spectral methods attempts to retain global isometry by mapping 
pairwise distances to $\Rd$.
Among them, some are based on Euclidean distances and thus confined to 
learning linear embeddings (such as \textit{principal component analysis (PCA)} 
or \textit{multi-dimensional scaling (MDS)}).
Since linearity is a strong assumption that will not hold for general manifolds, 
non-linear techniques are more widely applicable.
For example, \textit{ISOMAP} achieves non-linearity by applying geodesic 
distances \citep{vandermaatenetal2009}.
Research indicates, however, that for non-convex manifolds it is more effective 
to preserve local structures only.
Otherwise, solutions are prone to shortcuts, i.e., placing points close in $\RD$
next to each other when they lie in fact on quite different parts of the 
manifold \citep{belkinniyogi2003}.
In order to avoid such miscalculations, sparse techniques focus on merely 
local neighborhood structures, modeled through graph representations.
The information from these graphs is then condensed into a sparse matrix 
\citep{vandermaatenetal2009}.
\textit{Laplacian eigenmaps} employ the graph Laplacian to this end 
\citep{belkinniyogi2003}.
While Laplacian eigenmaps do well in preserving locality, they are less adept
at determining local linearity, a property that comes with manifolds' being 
locally Euclidean \citep{dissross2008} \textcolor{red}{find better source!!}.

This shortcoming is mitigated by locally linear methods.
One such technique is \textit{locally linear embedding (LLE)}, proposed by
\citet{roweissaul2000}.
While it originated from a different perspective, it can be shown to be a
variant of Laplacian eigenmaps \citep{belkinniyogi2003}. 
LLE is based on the idea that neighborhoods on the manifold are locally linear
and so points can be linearly reconstructed from their neighbors.
Since the corresponding weights are believed to reflect the intrinsic geometry 
of the  manifold, they are topological properties and as such invariant to 
rotations, rescalings, and translations.
By consequence, the same weights that reconstruct an individual point in $D$
dimensions should do so in $d$ dimensions.
LLE thus maps vicinity structures to the $d$-dimensional subspace and finds the 
coordinates that preserve them best.
This requires solving the least-squares problem of minimizing reconstruction 
error and then the sparse eigenvalue problem of minimizing embedding cost. 
Convexity of both sub-problems ensures globality of local optima 
\citep{roweissaul2000}.

This approach has been shown to successfully retrieve manifold structures in
different applications (\textcolor{red}{evidence!!!}).
However, its fully unsupervised functionality offers a drawback: it may fail
to find a low-dimensional embedding that has an actual reflection in the 
real-life setting.
Such situations might require the specification of some pre-labeled instances.
Also, it may simply be the case that manual analysis of a subset of the data 
is available at low cost \citep{yangetal2006}.
When prior knowledge is at hand it is only natural to use it.
Therefore, \citet{yangetal2006} proposed \textit{semi-supervised locally linear 
embedding (SS-LLE)}, an extension to LLE that is able to harvest prior 
information.

Indeed, the presented results indicate considerable success of their technique.
It is the aim of this report to (1) reproduce these results, thereby creating
an open-source implementation of SS-LLE, and (2) to apply SS-LLE to further 
manifold learning tasks for a more thorough assessment of its performance. 
The rest of the report is organized as follows: chapter \ref{math} 
provides a mathematical framework where fundamental concepts are briefly 
introduced; chapter \ref{lgb-mani-learn} explains the idea of local graph-based
manifold learning; chapter \ref{sslle} presents SS-LLE in detail; chapter
\ref{experiment} discusses the results of the conducted experiments; and chapter
\ref{concl} draws final conclusions.







