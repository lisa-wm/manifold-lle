Machine learning problems increasingly employ data of high dimensionality. 
While a large amount of samples is beneficial to learning, high-dimensional 
feature spaces, such as in speech recognition or gene processing, pose serious 
obstacles to the performance and convergence of most algorithms 
\citep{cayton2005}. 
\\

\textbf{High dimensionality.}
Three aspects strike as particularly problematic: computational operations, 
interpretation of results, and geometric idiosyncrasies.
Computational cost must be considered but is becoming less of an issue with 
technological evolution \citep{leistetal2009}.
By contrast, the demand for explainable results (for reasons of, say, safety or
ethics) is rather intensified by the advance of complex technology. 
Alas, interpretation in more than a few dimensions is virtually inaccessible to 
humans \citep{doshivelezkim2017}. 
The geometric aspect is often addressed as \textit{curse of dimensionality}, a 
term subsuming various phenomena of high-dimensional spaces. 
It is generally not straightforward to infer properties of objects in these 
spaces as geometric intuition developed in lower dimensions can be 
misleading.
Crucially, the exponential increase of spatial volume induces sparsity. 
Consequences of this behavior are, among others, a sharp incline in the number 
of points required to sample the feature space and a loss in meaningfulness of 
distances. 
Many learners, however, rely on these 
concepts\footnote{For instance, consider support vector machines and
$k$-nearest neighbors, both of which rely on distances, or tuning, which often
requires extensive sampling of the hyperparameter space.} and see 
their functionality deteriorate \citep{verleysenfrancois2005}. 
\\

\textbf{Manifold learning.}
These challenges make the case for \textit{dimensionality reduction}, that is, 
the endeavor of compressing problem dimensionality to a manageable size. 
Far from undue simplification, dimensionality reduction is justified by the 
belief that the latent data-generating process is indeed of much lower dimension 
than is observed.
Consider, for example, image data showing objects in different poses.
Such data are typically stored in high-dimensional pixel representations, yet it 
is reasonable to suppose that variation in these images is in fact caused by a 
small number of latent features.
More formally, the data are assumed to lie on a $d$-dimensional 
\textit{manifold} embedded in the $D$-dimensional observation space, with 
$d \ll D$ \citep{cayton2005}.
A crucial property of $d$-manifolds, i.e., the $d$-dimensional generalization of
a curved surface, embedded in $\RD$, is their local topological equivalence to 
$\Rd$ \citep{mafu2011}.
% This locally Euclidean behavior is exemplified by a sphere embedded in $\R^3$: 
% although the sphere as a whole is entirely non-linear, on sufficiently small 
% patches of its surface it behaves just like a flat plane in $\R^2$.
It is precisely this fact that allows manifold coordinates to be mapped to 
$\Rd$ in a reduction of dimensionality\footnote{
The most intuitive example of this is probably the representation 
of the Earth, which is a two-dimensional manifold enclosed in three-dimensional 
space, on two-dimensional maps.}.
The goal is now to learn this mapping in an unsupervised manner 
\citep{cayton2005}.
Mapping manifold coordinates to $\Rd$ is in general not equivalent to simple 
projection onto the $d$-dimensional coordinate hyperplanes.
Instead, models must learn the intrinsic neighborhood structure of the manifold
to establish a notion of "nearness" between points.
Standard distance metrics do not apply (globally) as points on general manifolds 
are connected by curved paths rather than straight lines \citep{mafu2011}.
\\

\textcolor{red}{Adapt to structure of chapter 3}

\textbf{Local graph-based techniques.}
Various approaches have been proposed to retrieve points' manifold coordinates.
A taxonomy may, for example, be found in \citet{vandermaatenetal2009}. 
Many rely on spectral techniques, trying to find a matrix representation of the 
data whose principal eigenvectors are used to span a $d$-dimensional subspace.
% Drawing from the nature of the eigenvalue problem they solve, spectral methods 
% are also referred to as \textit{convex}.
One group of spectral methods attempts to retain global isometry by mapping 
pairwise distances to $\Rd$.
Among them, some are based on Euclidean distances and thus confined to 
learning linear embeddings (such as \textit{principal component analysis (PCA)} 
or \textit{multi-dimensional scaling (MDS)}).
Since linearity is a strong assumption that will not hold for general manifolds, 
non-linear techniques are more widely applicable \citep{vandermaatenetal2009}.
For example, \textit{Isomap} achieves non-linearity by applying geodesic 
distances in the MDS setup \citep{tenenbaumdesilvalangford2000}.
Research indicates, however, that for non-convex manifolds it is more effective 
to preserve local structures only.
Otherwise, solutions are prone to shortcuts, i.e., placing points close in $\RD$
next to each other when they lie in fact on quite different parts of the 
manifold \citep{belkinniyogi2003}.
In order to avoid such miscalculations, sparse techniques focus on merely 
local neighborhood structures, modeled through weighted graph representations.
The information from these graphs is then condensed into a sparse matrix. 
Eventually, the principal eigenvectors of this matrix yield the desired 
low-dimensional coordinates \citep{vandermaatenetal2009}.
\\

\textbf{Locally linear embedding.}
One such local graph-based technique is \textit{locally linear embedding (LLE)}, 
the unsupervised algorithm SSLLE builds upon \citep{roweissaul2000}.
LLE is based on the idea that the embedded manifold may be approximated by 
locally linear neighborhoods in $\RD$.
Weights for the resulting graph are obtained by linear reconstruction of points 
from their neighbors. 
As these weights are believed to reflect the intrinsic geometry of the manifold, 
they are topological properties and should as such also reconstruct points in 
$d$ dimensions.
LLE thus maps vicinity structures to the $d$-dimensional subspace and finds the 
Euclidean coordinates that preserve them best by means of spectral decomposition \citep{roweissaul2000}.

Much of the theoretical foundation for LLE has been discussed only in later 
work.
In particular, \citet{belkinniyogi2001} proposed \textit{Laplacian eigenmaps 
(LEM)}, a method which employs the graph Laplacian, and provided evidence for 
the fact that, under certain assumptions, LLE may be generalized to the same 
framework \citep{belkinniyogi2003}.
A later proposition by \citet{donohogrimes2003}, \textit{Hessian LLE (HLLE)}, 
may be viewed as an algorithmic variant of LLE and a conceptual variant of 
LEM (using the Hessian en lieu of the Laplacian).
The theoretical link between LLE and LEM, centered around the Laplace-Beltrami 
operator, has recently been found to hold less generally than previously assumed \citep{wuwu2018}. 
It still appears beneficial to interpret all methods in this common framework 
also found by \citet{bengioetal2003}; a more thorough study of convergence 
guarantees is left to future research.
\\

\textbf{Semi-supervised extension.}
The above approaches have been shown to successfully retrieve manifold 
structures in different applications \citep{wuwu2018}.
However, their fully unsupervised functionality offers a drawback: they may fail
to find a low-dimensional embedding that has an actual reflection in the 
real-life setting.
Such situations might require the specification of some pre-labeled instances.
Also, it may simply be the case that some observations already come with labels, 
or that annotation of a subset of the data is available at low cost 
\citep{yangetal2006}.
When prior knowledge is at hand it is only natural to use it.
Therefore, \citet{yangetal2006} proposed \textit{semi-supervised locally linear 
embedding (SSLLE)}, an extension to LLE that is able to harvest prior 
information\footnote{
It should be noted that an alternative proposal for SSLLE has been made by 
\citet{zhangchau2009} some time after but without reference to 
\citet{yangetal2006} and building upon a rather different approach, namely 
the fully supervised LLE model that \citet{deridderduin2002} had designed for 
classification tasks.
Here, solely the work by \citet{yangetal2006} is examined.
}.
\\

\textbf{Outline.}
Indeed, the presented results indicate considerable success of their technique.
It is the aim of this report to (1) reproduce these results, thereby creating
an open-source implementation of SSLLE, and (2) to apply SSLLE to further 
manifold learning tasks for a more thorough assessment of its performance. 
The rest of the report is organized as follows: chapter \ref{math} 
provides a mathematical framework where fundamental concepts are briefly 
introduced; chapter \ref{lgb-mani-learn} explains the idea of local 
graph-based manifold learning; chapter \ref{sslle} presents SSLLE in detail; 
chapter \ref{experiment} discusses the results of the conducted experiments; and 
chapter \ref{concl} draws final conclusions.