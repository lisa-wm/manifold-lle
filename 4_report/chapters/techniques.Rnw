\subsection{Unsupervised Techniques}
\label{unsupervised}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps (LEM)}
\label{laplace}

The reason for LEM to appear in this report alongside the LLE family is its 
underlying theory both providing a foundation for LLE \citep{belkinniyogi2003}, 
which was originally proposed lacking such, and closely related to the 
theoretical concepts in HLLE \citep{donohogrimes2003}.

LEM are centered around the preservation of locality, i.e., mapping nearby 
inputs to nearby outputs.
Locality is enforced via the \textit{Laplace-Beltrami operator} defined on 
smooth, compact manifolds and operationalized by means of the \textit{graph Laplacian} acting as a discrete approximator \citep{belkinniyogi2003}.
This idea is best understood recalling that the similarity of outputs for 
similar inputs is essentially a notion of smoothness and can thus be controlled 
by a size constraint on the gradient of the mapping function.
\\

\textbf{Laplace-Beltrami operator.}
Consider the twice differentiable function $f: \mani \rightarrow \R$ mapping 
two points $\pv, \qv \in \mani$ to $f(\pv)$ and $f(\qv)$, respectively. 
On $\mani$ they are connected by a length-parametrized curve $c(t)$.
Denote the geodesic distance between $\pv$ and $\qv$ by $\ell$, such that
$\pv = c(0)$ and $\qv = c(\ell)$.

\begin{minipage}[b]{0.65\textwidth}
  Gradients of $f$ with respect to $\pv$ are defined in the local tangent space
  $T_{\pv}(\mani)$ spanned by vectors tangent to $\mani$ at $\pv$.
  As $\mani$ is embedded in $\RD$, its tangent spaces are again Euclidean and of 
  dimension $d$, i.e., $d$-dimensional hyperplanes as depicted in figure 
  \ref{fig:sphere-tangent} \citep{sudderth2002}.
  If $\pv$ is identified with the origin of $T_{\pv}(\mani)$, the tangent space
  inherits an orthonormal coordinate system obtained from endowing
  $T_{\pv}(\mani)$ with the inner product from $\Rd$ \citep{donohogrimes2003}.
  With this, the distance $|f(\pv) - f(\qv)|$ of mappings can be expressed as
  the length of the integral
  $\int_0^{\ell} \langle \nabla f(c(t)), c^{\prime}(t) \rangle dt$.
  In other words, the geodesic curve connecting $\pv$ and $\qv$ is projected onto
  $T_{\pv}(\mani)$, and the length of this projection depends on the gradient of 
  $f$ and the curve velocity.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.3\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 90 70 60 30, clip, % left bottom right top
      width = 0.8\textwidth]{figures/sphere-tangent}
    \caption[Tangent hyperplane for two-dimensional unit sphere]{Tangent 
    hyperplane for an exemplary point on the two-dimensional unit sphere 
    manifold, embedded in $\R^3$. \textit{Source:} own representation.}
    \label{fig:sphere-tangent}
  \end{figure}
\end{minipage}

Exploiting the Schwartz equality and relations proved in 
\citet{belkinniyogi2008}, it
can be shown that 
$|f(\pv) - f(\qv)| \leq \| \nabla f(\pv) \| \cdot \| \pv - \qv\| + o$.
Acknowledging the fact that the distance between $\pv$ and $\qv$ is a datum,
$\| \nabla f \|$ controls how far apart points are mapped on the real line.
Consequently, the goal is to find a mapping that, on average, preserves 
locality by minimizing $\int_{\mani}\| \nabla f \|^2$.
This is just equal to minimizing $\int_{\mani} \mathcal{L}(f)f$ with 
$\mathcal{L}$ the Laplace-Beltrami operator.
The optimal embedding map is then given by the $d$ principal eigenfunctions of
$\mathcal{L}$ after removing the bottom one which would map $\mani$ but to a 
single point \citep{belkinniyogi2003}.
\\

\textbf{Graph Laplacian.}
Now the same reasoning can be applied to the neighborhood graph approximation of
$\mani$.
$\mathcal{L}$ is replaced by the graph Laplacian 
$\bm{L} \in \R^{N \times N}$ derived from edge weights 
$\bm{W} = (w)_{ij} \in \R^{N \times N}$.
In LEM, these weights are computed using a heat kernel, such that 
$w_{ij} = \exp(\frac{\|x_i - x_J \|^2}{t})$, $t \in \R$, for any two points 
$x_i, x_j \in \RD$ if they connected by a graph edge and 0 otherwise.
Clearly, edges between closer points receive larger weights.
Let $\bm{D} = (d)_{ij} \in \R^{N \times N}$ be the diagonal matrix of row sums 
of $\bm{W}$ \citep{belkinniyogi2003}.
Then, $\bm{L} = \bm{D} - \bm{W}$.
Translating the idea of smoothness from above leads to a generalized 
eigenvalue problem as stated in equation \ref{eq-gevproblem} \citep{kurras2016}.
To arrive at this, recall the desideratum of mapping nearby inputs to nearby 
outputs.
As the heat kernel assigns larger weights to points with small distance, this 
requirement may be put in terms of minimizing weighted output distances: 
% \begin{equation*}
% \begin{split}
% \argmin_{\Y} \frac{1}{2} \sum_{i, j} (\Y_i - \Y_j)^2 w_{ij} 
% &= \argmin_{\Y} \frac{1}{2} \sum_{i, j} (\Y_i^2 + \Y_j^2 - 2\Y_i\Y_j) w_{ij} \\
% &= \argmin_{\Y} \frac{1}{2} (\sum_i \Y_i^2 d_{ii} + \sum_j \Y_j^2 d_{jj} 
% - 2 \sum_{ij}\Y_i\Y_j w_{ij}) \\
% &= \argmin_{\Y} \Y^T \bm{L} \Y.
% \end{split}
% \end{equation*}
\begin{equation*}
\begin{split}
\min_{\Y} \sum_{i, j} (\Y_i - \Y_j)^2 w_{ij} 
&= \min_{\Y} \sum_{i, j} \| \Y_i - \Y_j \|^2 w_{ij} \\
&= \min_{\Y} \text{\textit{trace}}(\Y^T \bm{L} \Y), \quad \text{s.t.} 
  \quad \Y^T \bm{D} \Y^T = \bm{I},
\end{split}
\end{equation*}

the additional constraint removing an arbitrary scaling factor.
As explained in section \ref{eigenanalysis}, the solution to this generalized 
eigenproblem is given by the eigenvectors of $\bm{L}$, the $d$ bottom ones 
(again, after removing the constant vector) yielding the desired low-dimensional 
embedding \citep{belkinniyogi2003}.

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

\begin{itemize}
  \item Notion of local linearity
  \item Approximation of graph Laplacian
\end{itemize}

% TODO Regularized version --> look into de ridder

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (HLLE)}
\label{hlle}

\begin{itemize}
  \item Hessian instead of Laplacian (eigenmaps)
  \item Hessian instead of LS fit (LLE)
\end{itemize}

% ------------------------------------------------------------------------------

\subsection{Semi-Supervised Locally Linear Embedding (SSLLE)}
\label{sslle}

% ------------------------------------------------------------------------------

\subsubsection{Employment of Prior Information}
\label{prior-info}

% Read belkinniyogi2004 paper on this

\begin{itemize}
  \item Why use labels in the first place?
  \item How will that help?
  \item Exact vs inexact knowledge
\end{itemize}

% ------------------------------------------------------------------------------

\subsubsection{Finding Prior Points}
\label{prior-points}

Prior points: take minmax approach from sparse MDS (Sparse multidimensional 
scaling using landmark points Vin de Silva and Joshua B. Tenenbaum 2004). 
Easy and deterministic after choosing seed value.
Instead Euclidean distances, though, take geodesics as estimated in isomap.

can we view the prior info as some kind of active learning? like we choose 
some points to label in a hopefully cleverish way and then hand them to you 
(e.g., to look at some pictures instead of all droelf thousand)

% ------------------------------------------------------------------------------

\subsubsection{SSLLE Algorithm}
\label{algo-sslle}

\begin{itemize}
  \item What is different wrt standard LLE?
\end{itemize}

% ------------------------------------------------------------------------------

\subsection{Particular Challenges}
\label{challenges}

A number of computational and design-related challenges arise from this 
procedure that must be faced in implementation.
\\

\textbf{Choice of intrinsic dimensionality.} 
Until now, it has been assumed, rather implicitly, that the intrinsic dimension 
$d$ of the data is a known parameter.
This is obviously not always the case in practical applications.
Some methods offer the advantage of estimating $d$ in a built-in fashion. 
PCA, MDS and Isomap, for instance, typically show an indicative gap in their 
eigenvalue spectrum, distinctly pointing out the dimensions with the largest 
share of variability \citep{sauletal2006}.
For LLE, LEM and HLLE, no such tell-tale gap exists.
While \citet{shasaul2005} have indeed drawn a mathematical relation between the 
respective eigenspectra in LLE and LEM and intrinsic data dimensionality, they 
immediately discarded this finding for practical applications due to large 
computational overhead and lack of reliability in finite-sample situations.
There have been various other proposals to tackle the problem of dimensionality 
estimation (for an extensive discussion, see for example 
\citet{disswissel2017}).
However, as the focus of this report lies on a semi-supervised method of 
manifold learning, it is mainly concerned with situations where prior knowledge 
of coordinates, and of $d$ in particular, is actually available.
\\

\textbf{Choice of neighborhood size.} Choosing the size of neighborhoods for 
graph approximation does pose a challenge.
It is a standard hyperparameter optimization problem in which a trade-off 
between locality and overall approximation must be balanced.
If neighborhoods are too small, the model will not be able to learn the global 
manifold structure; with overly large neighborhoods, it will forgo the 
advantages of locality and non-linearity and essentially behave like PCA 
\citep{deridderduin2002}.

\textcolor{red}{Describe applied approach}
\\

\textbf{Robustness of eigendecomposition.} Mainly problem in LLE (?)
\\

\textbf{Computational cost.} Text
\\

\begin{itemize}
  \item Number of neighbors (diss grilli (referenced in lle manual) discusses 
  regression model, ghojogh propose different things)
  \item Intrinsic dimensionality
  \item Singularity of gram matrix (LLE-specific?!)
  \item Large data (landmarks)
\end{itemize}

\textcolor{red}{Comment on difficulty of finding neighbors in high dimensions}
\\
\textcolor{red}{What about using RF proximities for neighbor search? 
Unsupervised RF works with simulating new data from the estimated dist of the 
present ones, see e.g. https://horvath.genetics.ucla.edu/html/RFclustering/RFclustering/RandomForestHorvath.pdf, https://arxiv.org/pdf/2004.02121.pdf}