\subsection{Overview}
\label{lgml_overview}

In the following, it shall be laid out how the manifold learning problem is 
approached by LLE as the conceptual parent of SSLLE 
(the incorporation of prior information is a rather different matter; aside from 
this, the functionalities of SSLLE and LLE are identical). 
Much of the theoretical foundation for LLE has been discussed only in later 
work.
In order to provide a more integrated background, explanations will therefore be 
given in a broader context.
LEM, in particular, provides much of the mathematical framework the original 
proposal of LLE is lacking, and HLLE emerges as a combination of both ideas.
All three may be viewed as instances of LGML.
\\

\textbf{Taxonomy.}
LGML arises from a variety of geometric intuitions and computational 
implementations. 
Nonetheless, methods share common structures that allow for interpretation in a 
more abstract framework\footnote{
It should be noted that such a framework might be established from several 
angles; after all, the different approaches attempt to solve the same problem 
and can thus be translated into one another in various ways.
} (\citet{bengioetal2003}, \citet{bengioetal2004}).
Figure \ref{fig_models_overview} depicts a schematic overview on the models 
studied here.
All of these belong to the realm of \textit{spectral} models.
The non-spectral group includes, among others, techniques based on neural 
networks and is not discussed here \citep{vandermaatenetal2009}.

\begin{figure}[H]
  \centering
  \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
    width = \textwidth]{figures/models_overview_new}
  \caption[Overview on selected methods of manifold learning]{
  \raggedright
  Overview on selected methods of manifold learning. 
  \textit{Source:} own representation, inspired by a similar example in 
  \citet{vandermaatenetal2009} and re-interpreted with the findings in 
  \citet{bengioetal2004}.}
  \label{fig_models_overview}
\end{figure}

\textbf{Intuition.}
As indicated by figure \ref{fig_models_overview}, LGML may be viewed in the 
light of \textit{kernel principal component analysis (KPCA)}. %see \ref{KPCA}).
KPCA was actually proposed earlier and only later shown to link the other 
concepts by a unified idea (\citet{hametal2003}.
It provides a useful general intuition to manifold learning and subsumes the 
other methods in a way that is beneficial to the important task of out-of-sample 
extension \citep{bengioetal2004}.
KPCA builds upon two fundamental concepts in machine learning: it 
performs \textit{principal component analysis (PCA)} on data transformed by the
\textit{kernel trick}.
First, features of interest are extracted from the data by kernelization, 
exploiting the fact that dot products of arbitrary feature maps, whose 
computation might be prohibitively expensive, may be stated equivalently by 
application of a kernel function.
The kernelized data then form a matrix representation taken to capture the 
intrinsic data structure and therefore understood as an approximation to the 
latent manifold properties.
Second, PCA finds the principal axes along which these intrinsic properties 
vary.
To this end, eigenanalysis is performed on the representation matrix, yielding 
the desired reduction in dimensionality through preserving the most relevant 
latent dimensions \citep{schoelkopfetal1998}.
In theory KPCA is able to capture arbitrary data structures.

% ------------------------------------------------------------------------------

\newpage
\subsection{Concept}
\label{lgml_concept}

If KPCA sounds like a powerful concept, the crux of course lies in finding an 
appropriate kernel function.
Methods using linear kernels, such as standard PCA, suffer from the confinement 
to linear embedding spaces \citep{vandermaatenetal2009}.
If $\X$ lies on a non-linear manifold, as must be generally assumed, 
kernelization is best performed with non-linear feature maps 
\citep{schoelkopfetal1998}.
There is no obvious way to arrive at such a mapping.
\textit{Graph-based} models therefore approach the problem from an alternative 
angle.
In fact, they do not perform kernelization explicitly\footnote{
Explicit kernels may still be derived for all methods but as their illustrative 
ability is rather limited, this is not covered here.
For the kernel perspective see for example \citet{bengioetal2004} and 
\citet{weinbergeretal2004}.
}, but build on a different intuition. 
\\

\textbf{Idea.} 
All LGML methods fundamentally rely on graph approximations of the 
manifold surface.
These graphs are discretized models of the manifold and as such, in principle, 
able to reflect any structure exhibited by the data.
Distances may then be measured along the approximated manifold surface rather 
than in the ambient Euclidean space, effectively enabling non-linearity 
\citep{sauletal2006}.
A second desideratum in manifold learning is the ability to handle manifolds 
having locally varying properties possibly at odds with their global structure.
Such behavior often also entails non-convexity, meaning $\mani$ is not isometric 
to a convex subset of Euclidean  space \citep{donohogrimes2003}. 
Intuitively, this requires careful tracing of the manifold surface to 
avoid coarse mappings of the global structure at the expense of local 
congruence.
LGML methods therefore focus on local properties
% \footnote{
% As opposed to, for example, \textit{Isomap} 
% \citep{tenenbaumdesilvalangford2000}, one of the earliest and most prominent 
% examples of global manifold learning.
% Isomap's central assumptions are global isometry and convexity of the parameter 
% space \citep{tenenbaumdesilvalangford2000}.
% While it yields good results in many applications, Isomap does not sufficiently 
% account for the curvature of strongly non-convex manifolds.
% In order to avoid this drawback, local methods limit isometry to only hold 
% between neighboring points and relax the parameter space 
% condition to open, connected subspaces \citep{donohogrimes2003}.
% } 
\citep{cayton2005}.
\\

\textbf{Local neighborhoods.} 
Graph approximations are constructed from neighborhood relations in the 
observation space.
Neighborhoods are typically taken to be $k$-neighborhoods, i.e., based on a 
fixed number $k \in \N$ of neighbors.
It is equally possible to restrict neighborhoods to a maximum 
distance of $\epsilon > 0$ to the centroid.
However, $k$-neighborhoods are often more easily specified due to the inherent 
scale invariance of $k$, and have attracted rather more attention in general research\footnote{
However, \citet{tenenbaumdesilvalangford2000} note that, when local 
dimensionality is not constant across the observed data, 
$\epsilon$-neighborhoods might provide more reliable results.
} \citep{heetal2005}. 
For a formal definition of $k$- and $\epsilon$-neighborhoods, see section
\ref{neighborhoods} of the appendix.
Both notions usually rely on Euclidean distance.
In the end, any vicinity condition is admissible so long as it serves to 
faithfully characterize the manifold surface in a computationally affordable 
manner \citep{roweissaul2000}.
For the remainder of this report, neighborhoods will be understood as
$k$-neighborhoods.
More important in LGML is neighborhood size.
It encodes beliefs about the topological structure of $\mani$: smaller 
neighborhoods correspond to a higher degree of non-linearity, emphasizing local 
properties more strongly, and vice versa \citep{sudderth2002}.
Chapter \ref{challenges} will discuss how the trade-off is addressed in 
practice.
\\

\begin{minipage}[b]{0.7\textwidth}
  \textbf{Graph construction.}
  $\mani$ may then be approximated by a \textit{neighborhood graph} 
  $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, always assuming it is 
  sampled well by $\X$. 
  Observations form vertices $\mathcal{V}$ and edges $\mathcal{E}$ 
  indicate neighborhood relations \citep{belkinniyogi2001}.
  Each vertex is connected to its $k$ nearest neighbors (or all points 
  within $\epsilon$-radius).
  It is easy to see that $k$-neighborhoods are an asymmetric notion and 
  therefore lead to directed graphs.
  Conversely, the $\epsilon$-distance boundary holds in both directions and 
  produces undirected graphs \citep{heetal2005}.
  Figure \ref{fig_neighbor_graph} shows how a neighborhood graph may be used to 
  approximate the S-curve manifold.
  It was built using $k$-neighborhoods with $k = 3$.
  Note that neighborhood construction solely relies on the observed data, not
  requiring any information about the intrinsic structure.
  For a densely sampled set of points, the graph representation should yield a 
  fairly good approximation of the manifold surface.
\end{minipage}
\begin{minipage}[b]{0.05\textwidth}
  \phantom{xxx}
\end{minipage}
\begin{minipage}[b]{0.25\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[trim = 250 170 200 180, clip, % left bottom right top
      width = 0.8\textwidth]{figures/s_curve_connected}
    \caption[S-curve neighborhood graph]{
    \raggedright
    $k$-neighborhood graph for 300 points sampled from the S-curve with $k = 3$.
    \textit{Source:} own representation.}
    \label{fig_neighbor_graph}
  \end{figure}
\end{minipage}

\vspace{0.4cm}

\textbf{Eigenanalysis.}
Eventually, spectral manifold learning boils down to eigenanalysis 
of a matrix derived from the graph approximation.
This matrix representation is obtained by application of some graph functional.
Precisely how the functional is constructed defines the core of each LGML 
method.
A common trait of all LGML methods is the sparsity of their respective matrix as 
a direct consequence of local emphasis, speeding up the eigendecomposition
\citep{sauletal2006}.
The resulting $d$ principal (top or bottom) eigenvectors -- as determined by the 
associated eigenvalues -- span a subspace into which the data may be projected 
under minimal loss of information, preserving as much variability as possible 
along the axes of intrinsic structure (for a formal definition of eigenanalysis 
and generalized eigenvalue problems, see section \ref{eigenanalysis}).
The nature of different graph functionals and resulting matrix representations 
across methods will be discussed in the subsequent chapters.