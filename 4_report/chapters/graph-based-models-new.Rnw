\subsection{Overview and Conceptual Framework}
\label{principles-overview}

After the goal of manifold learning has been formalized, it shall now be laid 
out how the problem is approached by LLE as the conceptual parent of SS-LLE 
The incorporation of prior information is a rather different matter that will be 
addressed in chapter \ref{sslle}. 
Much of the theoretical foundation for LLE has been discussed only in later 
work.
In order to provide a more integrated background, explanations will therefore be 
given in a broader context of local graph-based manifold learning which also 
comprises Laplacian eigenmaps and H-LLE.
The particular relationship of the three methods shall be made clear along the 
way.

Local graph-based manifold learning generally arises from a variety 
of geometric intuitions and computational implementations.
Nonetheless, methods share common structures that allow for interpretation in a 
more abstract framework (\citet{bengioetal2003}, \citet{bengioetal2004}).
It should be noted that such a framework might be established from several 
perspectives; after all, the different methods attempt to solve the same problem 
and can thus often be translated into one another.
\\

Figure \ref{fig:models-overview} depicts a schematic overview on the models 
studied here, representing the specific perspective taken within this report.
All of these belong to the realm of \textit{spectral} models.
The non-spectral group includes, for instance, techniques based on neural 
networks and is not discussed here \citep{vandermaatenetal2009}.

\begin{figure}[H]
    \centering
    \includegraphics[trim = 0 0 0 0, clip, % left bottom right top
      width = 0.65\textwidth]{figures/models-overview}
    \caption[Overview on selected manifold learning models]{A schematic overview 
    on selected methods of manifold learning (the list is by no means extensive 
    and could arguably be ordered in several alternative ways). 
    \textit{Source:} own representation, inspired by a similar example given in 
    \citet{vandermaatenetal2009} and re-interpreted with the findings in 
    \citet{bengioetal2004}.}
    \label{fig:models-overview}
  \end{figure}

As visually indicated in figure \ref{fig:models-overview} , this report will 
sketch the idea behind local graph-based manifold learning in the light of 
\textit{kernel principal component analysis (kernel PCA)}.
Kernel PCA was actually proposed first and later shown to link the others by a 
unified idea (\citet{hametal2003},
It makes for an appealing framework that, firstly, provides a useful general 
intuition to manifold learning, and, secondly, subsumes the other methods in a 
way that proves beneficial for out-of-sample extension \citep{bengioetal2004}.
\\

\textbf{Kernel PCA.} Kernel PCA builds upon two fundamental concepts 
in machine learning: it performs \textit{principal component analysis (PCA)} on 
data transformed by the \textit{kernel trick}.
In principle, it consists of two subsequent steps.
First, features of interest are extracted from the data by kernelization. 
These are taken to capture the intrinsic data structure and may therefore be 
understood as an approximation to the latent manifold properties.
In the end, they constitute a matrix representation.
Second, \textit{principal component analyis (PCA)} finds the principal axes 
along which these intrinsic properties vary, yielding the desired reduction in 
dimensionality by preserving the most relevant latent dimensions \citep{schoelkopfetal1998}.

\begin{itemize}

  \item[] \textbf{Kernelization.} By kernelization, i.e., mapping the data to a 
  space $\mathcal{F}$ of arbitrarily high dimension, features may be obtained 
  that relate to the input in a possibly non-trivial way\footnote{
  Support vector machines use the kernel trick to achieve linear separability. 
  An intuitive example may be given by data observed in two classes that form 
  concentric circles in $\R^2$. 
  While such data are not linearly separable in two dimensions, they are in three: 
  mapping the classes to different heights enables separation by a horizontal 
  hyperplane.
  This example also hints at the idea of (spectral) clustering to which kernel 
  PCA is indeed intimately related \citep{bengioetal2004}.
  }.
  Crucially, the feature map $\phi: \RD \rightarrow \mathcal{F}$ need not be 
  computed explicitly (this might prove prohibitively expensive).
  Kernelization instead solely relies on inner products $\langle \phi(\x_i), 
  \phi(\x_j) \rangle$ of the transformed inputs.
  Employing Mercer's theorem of functional analysis, these inner products may 
  be interpreted as performed by a continuous kernel 
  % Remove domain and co-domain of kappa, might be wrong, needs the Hilbert 
  % space after all, right
  $\kappa(\x_i, \x_j)$ in some space with Hilbert property.
  Appropriate choice of $\kappa$ then allows for the data to be represented by a 
  matrix $K \in \R^{N \times N}, K_{ij} = \kappa(\x_i, \x_j)$.
  This matrix is the numerical representation derived with respect to the 
  latent data properties. \citep{schoelkopfetal1998}.
  Precisely how it is computed depends on the choice for the kernel function 
  $\kappa$ and gives rise to different techniques \citep{hametal2003}.
  
  \item[] \textbf{PCA.} PCA is a quite powerful technique by itself.
  It finds the directions of maximum variance through eigenanalysis of the 
  empirical covariance matrix, yielding the most important axes of inter-feature 
  relations that coincide with the principal eigenvectors of the covariance 
  matrix (for comments on eigenanalysis see chapter \ref{eigenanalysis} of the 
  appendix).
  The data are projected into the linear subspace spanned by these $d$ 
  eigenvectors, thereby mapping the observations to a coordinate system given 
  precisely by those linear feature combinations that represent the strongest 
  (co)variability.
  PCA thus performs an orthogonal input transformation that allows for linear
  dimensionality at minimal loss of information \citep{cayton2005}.
  In kernel PCA, this eigenanalysis is implicitly performed in the feature space 
  $\mathcal{F}$.
  Algorithmically, it boils down to diagonalizing the kernel matrix $K$ 
  \citep{schoelkopfetal1998}.
 
\end{itemize}

Figure \ref{fig:spirals} attempts to visualize the idea of kernel PCA.
The original data (\textit{left}) are observed in two dimensions but clearly 
intrinsically one-dimensional, where the latent manifold feature is expressed by 
coloring. 
The kernel trick creates a non-linear map, visualized here as a projection of 
the intrinsic feature to a third coordinate axis (\textit{middle}). 
Coercing the data to this dimension as the sole axis of variation yields the 
desired one-dimensional representation (\textit{right}). 

\begin{figure}[H]
  \centering
  \includegraphics[width = \textwidth]{figures/spirals}
  \caption[Schematic idea of kernel PCA]{Schematic idea of kernel PCA: from data
  observed in two dimensions, but clearly of intrinsic dimensionality one 
  (\textit{left}), create a mapping to a higher-dimensional feature space 
  (\textit{middle}), reduction of which to its principal axes yields the desired 
  one-dimensional representation (\textit{right}).
  \textit{Source:} own representation, using a subset of \texttt{mlbench}'s 
  noise-free \texttt{spirals} data. Note that this is but a schematic depiction 
  where the mid and right representation have not been created by an actual 
  implementation of kernel PCA.}
  \label{fig:spirals}
\end{figure}

% ------------------------------------------------------------------------------

\subsection{Non-Linearity and Locality}
\label{nonlin-local}

\textcolor{red}{SOURCES!!!}

If kernel PCA sounds like a powerful concept, the crux of course lies in 
finding an appropriate kernel function.
The nature of the feature map applied to the input data determines the kind of 
mapping that may be learned and serves to distinguish the various techniques.
As foreshadowed in figure \ref{fig:models-overview}, spectral methods decompose 
into groups using \textit{linear} and \textit{non-linear} kernels, respectively.
This distinction now directly translates to the feature map $\phi$.
Linear methods suffer from the considerable drawback of being confined to 
finding linear subspaces \citep{vandermaatenetal2009}.
PCA in its standard form can be interpreted as kernel PCA by identifying the 
kernel function with the covariance function.
It thus returns the subspace of greatest variability in the original input 
features \citep{hametal2003}.
The closely related \textit{multi-dimensional scaling (MDS)} approach yields the 
same result, albeit from a different intuition \citep{sauletal2006}.
\\

\textbf{Non-linearity.} As extensively discussed above, $\X$ must often be 
assumed to lie on a non-linear manifold $\mani \subset \RD$, which is precisely 
why kernelization is usually performed such that the resulting feature space is 
related to the input space in a non-linear way.
Conceivably, there is no obvious way to arrive at such a mapping.
\textit{Graph-based} models therefore approach the problem from an alternative 
angle.
In fact, they do not even perform kernelization explicitly: they transform the 
data in a way that can be shown to correspond to applying a kernel function, 
but the fundamental intuition is a different one. 
The key idea in graph-based learning is to approximate the manifold by a 
discretized graph representation.
Obviously, such a graph, which may be intuitively imagined as a skeletal model 
of the manifold surface, may take on highly non-linear forms.
The graph properties -- but an approximation of the latent manifold properties 
-- are described by functionals that vary across methods.
Eigenanalysis of the associated matrix representation then leads to precisely 
the sought-for low-dimensional subspace coordinates.
\\

\textbf{Locality.} The second desideratum in general manifold learning is the 
ability to trace the surface of potentially highly non-linear manifolds with 
sufficiently local focus.
Otherwise, solutions will fail to account for the numerous non-convexities such 
complicated subsets of $\RD$ may exhibit. 

\textcolor{red}{SOURCES AND FINISH!!!}
 
% ------------------------------------------------------------------------------

\subsection{Unsupervised Techniques}
\label{techniques}

% ------------------------------------------------------------------------------

\subsubsection{Laplacian Eigenmaps}
\label{laplace}

\begin{itemize}
  \item Notion of locality
  \item Laplacian eigenmaps
\end{itemize}

% ------------------------------------------------------------------------------

\subsubsection{Locally Linear Embedding (LLE)}
\label{lle}

\begin{itemize}
  \item Notion of local linearity
  \item Approximation of graph Laplacian
\end{itemize}

% TODO Regularized version???

% ------------------------------------------------------------------------------

\subsubsection{Hessian Locally Linear Embedding (H-LLE)}
\label{hlle}

\begin{itemize}
  \item Hessian instead of Laplacian (eigenmaps)
  \item Hessian instead of LS fit (LLE)
\end{itemize}