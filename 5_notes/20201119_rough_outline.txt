++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
ROUGH OUTLINE - DRAFT
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

1 Intro blabla (motivation of dimensionality reduction)

2 Non-linear dimensionality reduction
  2.1 Goal of (non-linear) dimensionality reduction
      - We want to learn the coordinates of the original points in the d-dimensional space (Euclidean, again!)
      - How we construct the mapping from m-dim to d-dim coordinates depends on the type of projection
      - Projection can happen in a linear (flatten the Swiss roll acc to direction of highest variance) or nonlinear (unroll it) manner
  2.2 Embedded manifolds
      - Topologies
      - Manifolds
      - Dimensionality of manifolds, maps, charts
      - Local homeomorphism to Euclidean space
  2.3 Methods of non-linear dimensionality reduction
     - Rough overview on methods (only to list what is out there)
       - Linear
         - PCA
         - MDS
       - Non-linear
         - Spectral graph-based --> central idea: capture intrinsic notion of neighborhood on manifold that shall be preserved in projection
         - Kernel-based
         - NN-based
         - Semi-definite programming
  2.4 Evaluating embedded manifolds

3 Locally Linear Embedding
  3.1 Conceptual framework (neighborhood graph - least squares/Hessian/ - sparse eigenvalue problem)
      3.1.1 Locally linear neighborhoods
            - k-/epsilon-neighborhoods
            - Neighborhood graphs
      3.1.2 Spectral graph theory
            - Manifolds & graphs
            - Laplace operators (Laplace-Beltrami)
      3.1.3 Laplacian eigenmaps 
            - Neighborhood-preserving projection via generalized eigenvalue problem
            - Convergence to Laplace-Beltrami
  3.2 Original LLE
      - Functionality
      - Pro's & con's
  3.3 Shortcomings
      - Limitations (manifolds with holes, finding d, scalability, distance preservation, ...)
      - Justification of existence for methods XY
  3.4 Extensions
      3.4.1 Hessian LLE
      3.4.2 Semi-supervised LLE
  
4 Application of LLE methods to toy data
  4.1 Data sets
  4.2 Implementation 
      - PCA as benchmark???
  4.3 Results & discussion (incl computational aspects?)

5 Conclusion


