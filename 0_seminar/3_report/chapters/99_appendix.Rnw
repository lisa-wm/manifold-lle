\subsection{Formal Definition of Topological Concepts}
\label{topology}

This section contains definitions of the main geometric concepts considered 
above.
Obviously, the list is by no means extensive; manifold theory is
presented much more in detail (and mathematical rigor) in, for example, 
\citet{mccleary2006} or \citet{waldmann2014}.
\\

\textbf{Topological spaces.} A \textit{topological space} is constituted by a 
set $T$ equipped with a \textit{topology} $\topo$. 
A topology is a general way of describing relations between elements in $T$.
Consider a function $\topo: T \rightarrow 2^T, t \mapsto \topo(t)$, which 
assigns to $t \in T$ a set of subsets of $T$ called \textit{neighborhoods}.
For $\topo$ to be a topology\footnote{
Alternative definitions employ open subsets of $T$, see for example 
\citet{waldmann2014}.
}
on $T$, the following properties must hold \citep{brown2006}:
\begin{tight_enumerate}
  \item[(T1)] If $\topo$ is a neighborhood of $t$, then $t \in \topo$.
  \item[(T2)] If $\topo$ is a subset of $T$ containing a neighborhood of $t$, 
  then $\topo$ is a neighborhood of $t$.
  \item[(T3)] The intersection of two neighborhoods of $t$ is again a 
  neighborhood of $t$.
  \item[(T4)] Any neighborhood $\topo$ of $t$ contains a neighborhood 
  $\topo^{\prime}$ of $t$ such that $\topo$ is a neighborhood of each element in 
  $\topo^{\prime}$.
\end{tight_enumerate}

Note that, in this general definition, neighborhoods are based on an abstract
notion of "nearness". 
Learning the structure of a topological space effectively boils down to learning 
neighborhood relations.
In Euclidean topological space these are directly based on distance: 
neighborhoods around $t$ are constructed by $\epsilon$-balls containing all 
elements within a Euclidean distance of $\epsilon$ from $t$. 
The resulting topology is also called the \textit{metric topology} 
\citep{mccleary2006}.

Topological spaces in general are not accessible via distances (or angles, for 
that matter) known from Euclidean spaces.
The ultimate goal in manifold learning therefore is the interpretation of the 
data in a space that is again Euclidean, albeit of lower dimensionality, where 
such concepts are meaningful.
\\

\textbf{Homeomorphisms.} Consider two topological spaces $(S, \topo_S)$, 
$(T, \topo_T)$ (denoted by the respective shorthands $S$, $T$ from here) and a 
mapping function $f: S \rightarrow T$. 
If $f$ is bijective and continuous and $f^{-^1}: T \rightarrow S$ is also 
continuous, $f$ is called a \textit{homeomorphism} \citep{brown2006}.
Topological spaces for which such a mapping exists are \textit{homeomorphic} to
each other. 
Any properties of $S$ that $T$ shares when it is homeomorphic to $S$ are 
referred to as topological properties. 
Two homeomorphic spaces are thus topologically equivalent \citep{mccleary2006}.
\\

If there exists a non-negative integer $d$ such that for every $s$ in a 
topological space $S$ a local neighborhood $U \ni s$, $U \subset S$, is 
homeomorphic to an open subset of $\Rd$ (sometimes called \textit{parameter 
space}), $S$ is \textit{locally 
Euclidean}\footnote{
For locally Euclidean topological spaces it is thus meaningful to speak of
elements as points.
} \citep{mafu2011}.
In other words, there is a homeomorphism $f: U \rightarrow \Rd$ for every 
element in $S$.
The neighborhoods $U$ are also referred to as  \textit{coordinate patches} and 
the associated maps $f$ are called \textit{coordinate charts} 
\citep{cayton2005}.
In local neighborhoods $S$ then behaves like $\Rd$ \citep{mafu2011}.

\newpage

\textbf{Manifolds.} \textit{Manifolds} are now precisely such locally Euclidean
topological spaces, with some additional properties.
For a topological space $\mani$ to be a $d$-dimensional manifold\footnote{
$\mani$ is again a shorthand, omitting the explicit notation of the 
corresponding topology. 
} (also: $d$-manifold) it must meet 
the following conditions \citep{waldmann2014}:

\begin{tight_enumerate}
  \item[(M1)] $\mani$ is Hausdorff.
  \item[(M2)] $\mani$ is second-countable.
  \item[(M3)] $\mani$ is locally homeomorphic to $\Rd$.
\end{tight_enumerate}

The Hausdorff condition is a separation property and ensures that for any two 
distinct points from $\mani$ disjoint neighborhoods can be found 
\citep{brown2006}.
Second-countability restricts the manifold's size via the number of open sets 
it may possess \citep{waldmann2014}.
\\

\textbf{Embeddings.} Recall that the data are observed in $\RD$ but taken to 
lie on $\mani$, locally homeomorphic to $\Rd$.
This implies the assumption $\mani \subset \RD$ and $\mani$ is said to be 
\textit{embedded} in the ambient $D$-dimensional Euclidean space 
\citep{cayton2005}.
The associated \textit{embedding} is but a map $f: \mani \rightarrow \RD$ 
whose restriction to $\mani$ is a homeomorphism \citep{brown2006}, or, more 
specifically, the canonical inclusion map identifying points on the manifold 
as particular points of $\RD$ \citep{waldmann2014}.
It can be shown that $K = 2d + 1$ is sufficient to create an embedding 
\citep{mafu2011}.
\\

\textbf{Geodesics.} 
In order to enable the construction of a meaningful distance metric, manifolds 
must fulfill two additional properties: \textit{smoothness}\footnote{
The smoothness property is based on differentiability of coordinate charts and 
ensures that concepts of curvature, length and angle remain meaningful 
\citep{mafu2011}.
A detailed derivation may be found, for example, in \citet{mukherjee2015}.
} and \textit{connectedness}\footnote{
Connectedness means that no separation $\{ U, V\}$ of a manifold 
$\mani$ exists with open, non-empty and disjoint $U, V \subset \mani$, 
$\mani = U \cup V$.
This may be loosely put as paths linking arbitrary pairs of manifold points 
\citep{mccleary2006}.
} \citep{mafu2011}.
For smooth, connected manifolds, \textit{geodesic distance} is the length of the 
shortest curve (\textit{geodesic}) on $\mani$ between two points on $\mani$.
A curve $c$ in $\mani$ is a smooth mapping from an open interval $\Lambda 
\subset \R$ into $\mani$.
$c$ is parametrized by a point $\lambda \in \Lambda$, such that

\begin{equation}
  c(\lambda) = (c_1(\lambda), ..., c_d(\lambda))^T
\end{equation} 

is a curve in $\Rd$ (all 
$c_j, j = 1, ..., d$ having a sufficient number of continuous derivatives).
Component-wise differentiation with respect to $\lambda$ yields
\textit{velocity} in $\lambda$: 

\begin{equation}
  c^{\prime}(\lambda) = (c_1^{\prime}(\lambda), ..., c_d^{\prime}(\lambda))^T.
\end{equation} 

The \textit{speed} of $c$ is given by $\| c^{\prime}(\lambda) \|^2_2$, where
$\| \cdot \|^2$ denotes the square norm.
Distance along this curve is measured by the arc-length
$$L(c) = \int_{\pv}^{\qv} \twonorm{c^{\prime}(\lambda)}  d\lambda.$$
Eventually, geodesic distance can be derived as the length of the shortest such
curve, out of the set $\mathcal{C}(\pv, \qv)$ of differentiable curves in 
$\mani$ that connect $\pv$ and $\qv$ \citep{mafu2011}:

\begin{equation}
  d^{\mani}(\pv, \qv) = \inf_{c \in \mathcal{C}(\pv, \qv)} L(c).
  \label{eq-geodesic}
\end{equation}

% ------------------------------------------------------------------------------

% \newpage
% \subsection{Kernel PCA}
% \label{kpca}
% 
% In this section, the concept of kPCA \citep{schoelkopfetal1998} is laid out in
% some more detail.
% \\
% 
% \textbf{Kernelization.} By kernelization, mapping the data to a
% space $\mathcal{F}$ of arbitrarily high dimension, features may be obtained
% that relate to the input in a non-trivial way.
% Crucially, the feature map $\phi: \RD \rightarrow \mathcal{F}$ need not be
% computed explicitly, which might prove prohibitively expensive.
% Kernelization instead solely relies on inner products $\langle \phi(\x_i),
% \phi(\x_j) \rangle$ of the transformed inputs.
% Employing Mercer's theorem of functional analysis, these inner products may
% be interpreted as performed by a continuous kernel
% $\kappa(\x_i, \x_j)$ in some space with Hilbert property.
% Appropriate choice of $\kappa$ then allows for the data to be represented by a
% matrix $\K \in \R^{N \times N}, \K_{ij} = \kappa(\x_i, \x_j)$.
% This matrix is the numerical data representation derived with respect to their
% latent properties. \citep{schoelkopfetal1998}.
% Precisely how it is computed depends on the choice of the kernel function
% $\kappa$ and gives rise to different techniques \citep{hametal2003}.
% \\
% 
% \textbf{PCA.} PCA is a quite powerful technique by itself.
% It finds the directions of maximum variance through eigenanalysis of the
% empirical covariance matrix, yielding the most important axes of inter-feature
% relations that coincide with the principal eigenvectors of the covariance
% matrix.
% The data are projected into the linear subspace spanned by these $d$
% eigenvectors, thereby mapping the observations to a coordinate system given
% by the linear feature combinations that represent the strongest
% (co)variability.
% Note that for this transformation to actually rotate the coordinate system
% about the origin, the data must be mean-centered beforehand.
% PCA thus performs an orthogonal input transformation that allows for
% dimensionality reduction at minimal information loss \citep{cayton2005}.
% In kernel PCA, this eigenanalysis is implicitly performed in the feature space
% $\mathcal{F}$.
% Algorithmically, it boils down to diagonalizing the kernel matrix $\K$
% \citep{schoelkopfetal1998}.
% \\
% 
% Figure \ref{fig-spirals} visualizes the idea of kernel PCA.
% The original data (\textit{left}) are observed in two dimensions but clearly
% intrinsically one-dimensional, where the non-linear manifold feature is
% expressed by coloring.
% The kernel trick creates a feature map, visualized here as a projection of
% the intrinsic feature to a third coordinate axis (\textit{middle}).
% Coercing the data to this dimension as the sole axis of variation yields the
% desired one-dimensional representation (\textit{right}).
% 
% \begin{figure}[H]
%  \centering
%  \begin{subfigure}[c]{0.29\textwidth}
%    \centering
%    \includegraphics[%trim = 50 0 0 0, clip, % left bottom right top
%    width = \textwidth]{figures/spirals_2d}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[c]{0.4\textwidth}
%    \centering
%    \includegraphics[%trim = 250 50 100 50, clip, % left bottom right top
%    width = \textwidth]{figures/spirals_3d}
%  \end{subfigure}
%  \begin{subfigure}[c]{0.29\textwidth}
%    \centering
%    \includegraphics[%trim = 0 0 0 300, clip, % left bottom right top
%    width = \textwidth]{figures/spirals_1d}
%  \end{subfigure}
%   \caption[Schematic idea of kernel PCA]{Schematic idea of kernel PCA: from data
%   observed in two dimensions, but clearly of intrinsic dimensionality one
%   (\textit{left}), create a mapping to a higher-dimensional feature space
%   (\textit{middle}), reduction of which to its principal axes yields the desired
%   one-dimensional representation (\textit{right}).
%   \textit{Source:} own representation, using a subset of \texttt{mlbench}'s
%   noise-free \texttt{spirals} data. Note that this is but a schematic depiction
%   where the mid and right representation have not been created by an actual
%   implementation of kernel PCA.}
%   \label{fig-spirals}
% \end{figure}

% ------------------------------------------------------------------------------

\newpage
\subsection{Formal Definition of Eigenanalysis and Generalized Eigenvalue 
Problems}
\label{eigenanalysis}

\textbf{Eigenvectors and eigenvalues.} Formally, eigenanalysis is the 
decomposition of a square matrix into pairs of \textit{eigenvectors} and 
\textit{eigenvalues}.
Let $\bm{A} \in \R^{N \times N}$ be a square matrix and $\lambda \in \R$ a 
scalar value. 
$\lambda$ is said to be an eigenvalue to $\bm{A}$ if there exists 
$\bm{v} \in \R^N \setminus \{\bm{0}\}$ such that 
$\bm{A} \bm{v} = \lambda \bm{v}$.
Then, $\bm{v}$ is the eigenvector corresponding to the eigenvalue $\lambda$, and 
their tuple is also called an \textit{eigenpair}.
\\

\textbf{Null spaces.} A closely related notion is that of the 
\textit{null space}, consisting of the vectors that map $\bm{A}$ to $\bm{0}$ 
upon multiplication from the right: 
$\{\bm{v} \in \R^N: \bm{A} \bm{v} = \bm{0}\}$.
It can be easily seen that the null space consists of those eigenvectors of 
$\bm{A}$ that are associated with an eigenvalue of zero, and the zero vector 
itself. 
For a specific eigenvalue $\lambda$ of $\bm{A}$, the null space of 
$\lambda \I - \bm{A}$ 
(with $\I$ the $N$-dimensional identity matrix) constitutes the 
\textit{eigenspace} of $\bm{A}$ \citep{boermmehl2012}.
\\

\textbf{Generalized eigenvalue problems.} Eigendecomposition of a matrix 
$\bm{A}$ can be framed as the solution of a generalized eigenvalue problem.
Generalized eigenvalue problems are posed subject to a constraint on a second, 
also symmetric matrix $\bm{B} \in \R^{N \times N}$.
As the standard eigenvalue problem results immediately from 
$\bm{B} = \I$, the generalized form subsumes both cases.
It is given by 

\begin{equation}
  \bm{A} \bm{V} = \bm{B} \bm{V} \bm{\Lambda},
  \label{eq_gevproblem}
\end{equation}

where $\bm{V} = \begin{bmatrix} \bm{v}_1 & \bm{v}_2 & \dots & \bm{v}_N 
\end{bmatrix} \in \R^{N \times N}$
is the matrix of eigenvectors of $\bm{A}$, and \\ 
$\bm{\Lambda} = \text{\textit{diag}}(\lambda_1, \lambda_2, \dots, \lambda_N) 
\in \R^{N \times N}$ 
is the diagonal matrix of the associated eigenvalues with \\
$\lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_N$.
The generalized eigenvalue problem may be stated equivalently as 

\begin{equation}
  \min_{\bm{V}} \text{\textit{trace}}(\bm{V}^T \bm{A} \bm{V}), \quad \text{s.t.} 
  \quad \bm{V}^T \bm{B} \bm{V} = \I,
  \label{eq_gevproblem_max}
\end{equation}

and translated to the first form by means of a Lagrangian multiplier \citep{ghojoghetal2019}.

% ------------------------------------------------------------------------------

\newpage
\subsection{Formal Definition of $k$- and $\epsilon$-Neighborhoods}
\label{neighborhoods}

A neighborhood of $\x \in \X$ is a subset of $\X$ containing another, open 
subset of $\X$ of which $\x$ is an element.
Members of the neighborhood are called neighbors of $\x$.
In metric spaces neighborhoods are defined via distances and therefore 
translate to open balls around each point \citep{waldmann2014}.
This distance-based construction locally applies to manifolds as a direct 
consequence of their local isometry to the Euclidean observation space 
\citep{mafu2011}.
There are two principal ways to build a neighborhood around $\x \in \X$, 
both of which usually employ the squared Euclidean norm\footnote{
In principle, alternative metrics are applicable, for instance such 
that measure angles \citep{belkinniyogi2004}.
} $\| \cdot \|^2$.
Let $\mathcal{N}: \X \rightarrow \X^{\ell}, \x \mapsto \mathcal{N} (\x)$ be a 
constructor that assigns a set of neighbors to $\x$.
The first possibility is to restrict the size of the neighborhood to the 
$k \in \N$ points\footnote{
In presence of ties in pairwise distances $k$ may vary across the data, but with 
zero probability in continuous feature spaces.
} with the smallest distance to $\x$, such that
$\ell = k$ and 

\begin{equation}
  \mathcal{N}_k(\x) = \{\x_j \in \X: \| \x - \x_j \|^2 \leq d_{(k)}\},
  \label{eq-kneighborhood}
\end{equation}

with $d_{(k)} \in \R$ being the $k$-th instance of ordered pairwise distances 
between $\x$ and all other points.
Alternatively, the neighborhood may be constructed by collecting all points that
have a distance of less than $\epsilon \in \R^+$ to $\x$, yielding 

\begin{equation}
  \mathcal{N}_{\epsilon} (\x) = \{\x_j \in \X: \| \x - \x_j \|^2 < \epsilon\}
  \label{eq-epsneighborhood}
\end{equation}

and $\ell = |\mathcal{N}_{\epsilon} (\x)|$ \citep{heetal2005}.

% ------------------------------------------------------------------------------

% \newpage
\subsection{Optimal Choice of $k$ in SSLLE Implementation}
\label{choice_k}

The tuning procedure for choosing the optimal neighborhood size in the SSLLE 
implementation builds upon a proposal by \citet{kouroptevaetal2001}.
% However, rather than following the authors in evaluating embedding performance 
% by residual variance, an alternative criterion, suggested by 
% \citet{kraemeretal2019} and explained in chapter \ref{eval}, is used.
Its key idea is to select $k$ at minimum computational expense, most of which is 
caused by embedding cost minimization.
First, the less costly reconstruction weight computation is carried out for a 
range of candidate values.
The maximum number $k_{\text{max}} \in \N$ to try must still be specified as a 
hyperparameter, but with arbitrary resources, there is no limit besides the 
practical $k_{\text{max}} \leq N - 1$.
This yields an empirical distribution of reconstruction errors over the given 
range of $1, 2, \dots, k_{\text{max}}$.
The most promising candidates for $k$ emerge as the local minima\footnote{
While weight reconstruction for a fixed $k$ is a convex optimization problem, 
finding the minimum error for a range of $k$ values is not.
} of the distribution, i.e., whenever the error is lower for a certain $k$ 
than for the immediate successor and predecessor values.
For the selected subset, the actual (expensive) embeddings are calculated and 
evaluated\footnote{
Again, this is a non-convex, blackbox problem.
} \citep{kouroptevaetal2001}.

However, in contrast to what \citet{kouroptevaetal2001} proposed, embeddings are 
not evaluated on residual variance.
The area under the $R_{NX}$ curve \citep{kraemeretal2019}, which is considered 
a more reliable measure, is used instead.
Details on $R_{NX}$ and the corresponding AUC may be found in section 
\ref{auc_rnx}.

% ------------------------------------------------------------------------------

\newpage
\subsection{Area under the $R_{NX}$ Curve}
\label{auc_rnx}

The area under the $R_{NX}$ curve, $\text{AUC}(R_{NX})$, has been chosen as 
evaluation criterion to assess embedding quality.
It is based on the \textit{co-ranking matrix} of high-dimensional and 
low-dimensional coordinates and measures the degree of neighborhood preservation 
during the embedding \citep{kraemeretal2019}.
\\

\textbf{Co-ranking matrix.}
The co-ranking matrix $\bm{Q} = (q)_{\ell m} \in \R^{N \times N}$ compares 
neighborhood relations in observation and embedding space.
Consider the rank distance matrices $(r)_{ij}^{\text{obs}}, 
(r)_{ij}^{\text{emb}} \in \R^{N \times N}$, stating for the element in the 
$i$-th row and $j$-th column that the $j$-th observation ranks $r_{ij}$ 
among the nearest neighbors of the $i$-th observation in the respective space 
\citep{lueksetal2011}. 
Any suitable distance metric is admissible; here, squared Euclidean distances 
are used.
The co-ranking matrix then counts for each pair of ranks $(\ell, m)$, $\ell, m 
\in \setN$, for how many pairs of observations $(i,j)$, $i, j \in \setN$, the 
rank in the embedding space distance matrix equals $\ell$ and the rank in the 
observation space distance matrix equals $m$:

\begin{equation}
  q_{\ell m} = \big \rvert \{ (i, j): 
  r_{ij}^{\text{emb}} = \ell \land r_{ij}^{\text{obs}} = m \} \big \rvert.
  \label{eq_coranking}
\end{equation}

Clearly, an ideal embedding would preserve all ranks and only have positive 
entries on the diagonal, i.e., where $\ell = m$.
If, by contrast, most non-zero entries are located on the upper triangle, the 
embedding has torn apart points that lie close in the observation space, 
and, vice versa, if it is mostly the lower triangle containing non-zero entries, 
faraway points have been collapsed together \citep{lueksetal2011}.
\\

\textbf{Co-ranking-based metrics.}
Based on $\bm{Q}$, various metrics may be derived.
The $Q_{NX}$ criterion counts the (normalized) number of points that remain in 
the $k$-neighborhoods they form part of in $\RD$, depending on the choice of 
neighborhood size:

\begin{equation}
  Q_{NX}(k) = \dfrac{1}{kN} \sum_{\ell = 1}^k \sum_{m = 1}^k q_{\ell m}.
  \label{eq_qnx}
\end{equation}

Adjusting for random embeddings and again normalizing to a zero-one range leads
to the $R_{NX}$ criterion \citep{kraemeretal2019}:

\begin{equation}
  R_{NX}(k) = \dfrac{(N - 1) Q_{NX}(k) - k}{N - 1 - k}.
  \label{eq_rnx}
\end{equation}

\textbf{Area under the $R_{NX}$ curve.}
Plotting $R_{NX}$ versus the number $k$ of neighbors yields the sought-for 
$R_{NX}$ curve, the area under which serves as a parameter-free measure of 
embedding quality \citep{kraemeretal2019}:

\begin{equation}
  \text{AUC}(R_{NX}) = \dfrac{\sum_{k = 1}^{N - 2} R_{NX}(k)}{
  \sum_{k = 1}^{N - 2} 1 / k} \in [0, 1].
  \label{eq_aucrnx}
\end{equation}

As is common with AUC metrics, the maximum value of 1 corresponds to the optimal 
value, whereas 0 marks an entirely random embedding \citep{kraemeretal2019}.

% ------------------------------------------------------------------------------

\newpage
\subsection{Generation of Synthetic Manifolds}
\label{synthetic_data}

This section documents how the synthetic manifolds considered in the report may 
be generated.
\\

\textbf{S-curve.} 
Construct S-curve as:
\begin{tight_enumerate}
  \item Sample $\bm{u}_1, \bm{u}_2 \sim U(0, 1)$ $\mathit{ iid}$ with 
  $\rvert \bm{u}_1 \rvert = \rvert \bm{u}_2 \rvert = N$.
  \item Compute $\bm{t} = 3 \pi (\bm{u}_1 - 0.5)$ and $\bm{s} = 2 \bm{u}_2$.
  \item $\X_{\text{scurve}} = 
    \begin{bmatrix} \x_1 & \x_2 & \x_3 \end{bmatrix} =
    \begin{bmatrix} \sin{\bm{t}} & \bm{s} & \mathit{sgn}(\bm{t}) 
    (\cos{\bm{t}} - 1) \end{bmatrix}$. 
\end{tight_enumerate}

\textbf{Swiss roll.}
Construct Swiss roll as:
\begin{tight_enumerate}
  \item Sample $\bm{u}_1, \bm{u}_2 \sim U(0, 1)$ $\mathit{ iid}$ with 
  $\rvert \bm{u}_1 \rvert = \rvert \bm{u}_2 \rvert = N$.
  \item Compute $\bm{t} = 1.5 \pi (1 + 2\bm{u}_1)$ and $\bm{s} = 21 \bm{u}_2$.
  \item $\X_{\text{swiss}} = 
    \begin{bmatrix} \x_1 & \x_2 & \x_3 \end{bmatrix} =
    \begin{bmatrix} \bm{t} \cos{\bm{t}} & \bm{s} & 
    \bm{t} \sin{\bm{t}} \end{bmatrix}$. 
\end{tight_enumerate}

\textbf{Incomplete tire.}
Construct incomplete tire as:
\begin{tight_enumerate}
  \item Sample $\bm{u}_1, \bm{u}_2 \sim U(0, 1)$ $\mathit{ iid}$ with 
  $\rvert \bm{u}_1 \rvert = \rvert \bm{u}_2 \rvert = N$.
  \item Compute $\bm{t} = \frac{5 \pi}{3} \bm{u}_1$ and 
  $\bm{s} = \frac{5 \pi}{3} \bm{u}_2$.
  \item $\X_{\text{tire}} =
  \begin{bmatrix} \x_1 & \x_2 & \x_3 \end{bmatrix} = 
  \begin{bmatrix} (3 + \cos{\bm{s}})
  \cos{\bm{t}} & (3 + \cos{\bm{s}}) \sin{\bm{t}} & \sin{\bm{s}} \end{bmatrix}$.
\end{tight_enumerate}

\textbf{World data.}
The world data are available in the public repository referenced in the 
Electronic Appendix as a CSV file.
The original code to synthesize them may be found 
\href{https://github.com/NikolayOskolkov}{\underline{here}}.

% ------------------------------------------------------------------------------

\subsection{Qualitative Embedding Results of Sensitivity Analysis}
\label{qual_results}

The following figures are full-page versions of those shown in chapter 
\ref{results}.

\newpage

\begin{turn}{90}
  \begin{minipage}{1.5\textwidth}
    \begin{figure}[H]
      \includegraphics[width = \textwidth]
      {figures/sensitivity_landmarks_qual_swiss_roll}
      \caption[]
      {Qualitative results for Swiss roll data, evaluating method of landmark 
      coverage versus number of prior points.
      \textit{Source:} own representation.}
    \end{figure}
  \end{minipage}
\end{turn}

\begin{turn}{90}
  \begin{minipage}{1.5\textwidth}
    \begin{figure}[H]
      \includegraphics[width = \textwidth]
      {figures/sensitivity_landmarks_qual_incomplete_tire}
      \caption[]
      {Qualitative results for incomplete tire data, evaluating method of 
      landmark coverage versus number of prior points.
      \textit{Source:} own representation.}
    \end{figure}
  \end{minipage}
\end{turn}

\begin{turn}{90}
  \begin{minipage}{1.5\textwidth}
    \begin{figure}[H]
      \includegraphics[width = \textwidth]
      {figures/sensitivity_noise_qual_swiss_roll}
      \caption[]
      {Qualitative results for Swiss roll data, evaluating noise level versus 
      number of prior points.
      \textit{Source:} own representation.}
    \end{figure}
  \end{minipage}
\end{turn}

\begin{turn}{90}
  \begin{minipage}{1.5\textwidth}
    \begin{figure}[H]
      \includegraphics[width = \textwidth]
      {figures/sensitivity_noise_qual_incomplete_tire}
      \caption[]
      {Qualitative results for incomplete tire data, evaluating noise level 
      versus number of prior points.
      \textit{Source:} own representation.}
    \end{figure}
  \end{minipage}
\end{turn}
